<?xml version="1.0" encoding="utf-8"?>
<search>
  <entry>
    <title><![CDATA[对抗样本生成系列：FGSM和DeepFool]]></title>
    <url>%2F%E5%AF%B9%E6%8A%97%E6%A0%B7%E6%9C%AC%E7%94%9F%E6%88%90%E7%B3%BB%E5%88%97%EF%BC%9AFGSM%E5%92%8CDeepfool.html%2F</url>
    <content type="text"><![CDATA[摘要：近些年来，深度学习技术在海量数据以及强大计算能力的驱动下取得了长足的发展，特别是在语音识别、计算机视觉、自然语言处理等领域，深度学习以其强大的网络表达能力刷新了一项又一项记录，各种各样基于深度学习的产品和服务也逐渐在产业界落地应用。正因为深度学习技术蕴含着巨大的商业价值，其背后潜在的安全问题更值得我们去深究。最近的研究表明，深度学习面临安全和隐私等多方面的威胁。该系列主要讨论深度学习领域最为热门的安全问题–对抗样本。本文讨论的FGSM和DeepFool是较为的早期对抗样本的生成算法，除此之外还会对DeepFool衍生出的Universal Perturbation进行解读。 文章概览 FGSM 算法概述 代码实现 DeepFool 算法概述 代码实现 Universal Perturbation FGSM算法概述&emsp;&emsp;在机器学习领域，对抗样本的问题始终存在。特别是在入侵检测、垃圾邮件识别等传统的安全应用场景，对抗样本的生成和识别一直是攻击者和防御者博弈的战场。2013年，Szegedy等人首次提出针对深度学习场景下的对抗样本生成算法–BFGS，作者认为，深度神经网络所具有的强大的非线性表达能力和模型的过拟合是可能产生对抗性样本原因之一。2014年，Goodfellow等人（包括Szegedy）对该问题进行了更深层次的研究，它们认为高维空间下深度神经网络的线性线性行为是导致该问题的根本原因。并根据该解释，设计出一种快速有效的生成对抗性样例的算法–FGSM。以下是对论文的解读。 &emsp;&emsp;对于线性模型$f(x)=w^Tx+b$来说，如果我们对输入$ x $添加一些扰动，使得$ \tilde{x}=x+\eta $。为了确保添加的扰动是极小的或者说是无法感知的，我们要求$ \left|\eta\right|_\infty&lt;\epsilon $。所以我们可以得到加噪后的模型输出为$$f(\tilde{x})=w^Tx+w^T\eta+b$$为了尽可能增大添加的噪声对输出结果的影响，令$\eta=\epsilon sign(w)​$（sign()函数的定义)。如果$w​$是一个$n​$维的向量，每一维的均值为$m​$，则$w^T\eta=\epsilon nm​$。虽然$\epsilon​$的值很小，但是当$w​$的维度$n​$很大时，$\epsilon nm​$将是一个很大的值，这将会给模型的预测带来很大的影响。因此，高维特征和线性行为可以成为对抗性样本存在的一种解释。 &emsp;&emsp;上面的解释是基于线性模型而言的，而深度神经网络作为一种高度非线性模型，为什么会存在对抗性样例呢？深度神经网络的非线性单元赋予了其强大的表达能力，但是非线性单元的存在会降低学习的效率。为了提高学习效率，需要对非线性单元进行改进，通常的做法是通过降低其非线性来实现。从早期的sigmoid到tanh再到ReLU，我们会发现这些非线性单元的线性行为在不断增强，这也导致了深度神经网络中的线性能力的增强，这在一定程度上解释了深度神经网络中对抗性样例存在的原因。 &emsp;&emsp;根据这种解释，作者提出了生成对抗性样例的算法FGSM。我们将深度神经网络模型看作是一个线性模型，即类似于$f(x)=w^Tx+b$。在线性模型中，$w$为$f(x)$关于$x$的导数，而在深度神经网络模型中我们可以将$w$视为代价函数关于输入$x​$的导数，即$$w=\nabla_{x}J(\theta,x,y)$$ $$\eta=\epsilon sign(\nabla_{x}J(\theta,x,y))$$ &emsp;&emsp;在文章的后半部分主要介绍了针对对抗性样本的防御机制，即通过对抗性训练来提高模型的鲁棒性，这些内容会在之后的工作中讨论。 代码实现&emsp;&emsp;为了更好的理解算法的实现过程，以下给出本文相关的代码段。这些代码段给出的定义，对于后面的DeepFool和Universal Perturbation算法的解读依赖有效。所有的代码基于python实现，使用的深度学习框架为pytorch，更加完整的算法实现，参加我的github. 网络模型 1234567891011class Net(nn.Module): def __init__(self): super(Net, self).__init__() self.fc1 = nn.Linear(28*28, 300) self.fc2 = nn.Linear(300, 100) self.fc3 = nn.Linear(100, 10) def forward(self, x): x = F.relu(self.fc1(x)) x = F.relu(self.fc2(x)) x = self.fc3(x) return x 数据集（这里只给出测试集的数据定义） 12345# 定义数据转换格式mnist_transform = transforms.Compose([transforms.ToTensor(), transforms.Lambda(lambda x : x.resize_(28*28))])# 导入数据，定义数据接口testdata = torchvision.datasets.MNIST(root="./mnist", train=False, download=True, transform=mnist_transform)testloader = torch.utils.data.DataLoader(testdata, batch_size=256, shuffle=True, num_workers=0) 代价函数 1loss_function = nn.CrossEntropyLoss() &emsp;&emsp;FGSM算法的实现较为简单，其核心在于利用代价函数求解已知样本的梯度值。我们假设模型已经训练完成，首先我们加载已训练完成的深度网络模型 1net = torch.load('mnist_net_all.pkl') # 加载模型 然后我们选择一个测试样本，针对该测试样本生成其对应的对抗性样例。其原始图片格式如图所示。 123index = 100 # 选择测试样本image = Variable(testdata[index][0].resize_(1,784), requires_grad=True) # requires_grad存储梯度值label = torch.tensor([testdata[index][1]]) 接着将测试样本作为网络模型的输入，通过前向传播的过程计算其损失，然后利用其损失进行反向传播（即求导）。 123outputs = net(image) # 前向传播loss = loss_function(outputs, label) # 计算损失loss.backward() # 反向传播 在深度学习框架中，反向传播的过程对用户来说是透明的。计算结束后，其梯度值存储在image.data.grad中，添加扰动的过程如下 1234# FGSM添加扰动epsilon = 0.1 # 扰动程度x_grad = torch.sign(image.grad.data)x_adversarial = torch.clamp(image.data + epsilon * x_grad, 0, 1) 添加扰动后，得到对抗性样本如下所示 实验过程中我们发现，深度网络模型对于原始的图片能够正确的分类，而对于扰动之后的的样本不能正确分类（分类结果为2）。 DeepFool算法概述&emsp;&emsp;FGSM算法能够快速简单的生成对抗性样例，但是它没有对原始样本扰动的范围进行界定（扰动程度$\epsilon$是人为指定的），我们希望通过最小程度的扰动来获得良好性能的对抗性样例。2016年，Seyed等人提出的DeepFool算法很好的解决了这一问题。文章的核心思想是希望找到一种对抗性扰动的方法来作为对不同分类器对对抗性扰动鲁棒性评估的标准。简单来说就是，现在我需要两个相同任务的分类器A、B针对同一个样本生成各自的对抗性样例。对于分类器A而言，其生成对抗性样例所需要添加的最小扰动为$a$；对于分类器B而言，其生成对抗性样例所需要添加的最小扰动为$b$；通过对$a$、$b$的大小进行比较，我们就可以对这两个分类器对对抗性样例的鲁棒性进行评估。由于FGSM产生扰动是人为界定的，所以它不能作为评估的依据。DeepFool可以生成十分接近最小扰动的对抗性样例，因此它可以作为衡量分类器鲁棒性的标准。 &emsp;&emsp;DeepFool源于对分类问题的思考。对于如图所示的线性二分类问题，令$f(x)=w^Tx+b$，其中$\mathscr{F}={x : f(x)=0}$。此时$x_0$位于直线的下方，即$f(x_0)&gt;0$。现在我们希望对$x_0$添加扰动$r$，使得分类器$f(x_0+r)&lt;0$。那么如何添加扰动才能使得扰动的程度最小呢？这个问题可以转化为求点到直线之间的距离，我们通过$x_0$做直线$\mathscr{F}$的垂线，与$\mathscr{F}$相交于$p$（投影点），则$p$与$x_0$之间的距离即为$x_0$到分类边界$\mathscr{F}$的最短距离。所以当我们沿着分类边界法线方向对$x_0​$进行扰动，可以保证扰动的程度最小$$r(x_0):={\arg\min_k}{|{r}|_{2}}=-{\frac{f(x_0)}{|{w}|_{2}^{2}}{w}}$$添加扰动之后将$x_0$映射到分类边界的投影点$p$，即$p=x_0+r(x_0)​$。 &emsp;&emsp;同样，对于非线性的二分类问题（分类边界为曲线或者曲面），我们也需要计算从目标样本点到分类边界的最短距离。这个计算过程较为复杂，一般采用垂直逼近法来逐步的逼近$x_0​$在分类边界上的投影点，所以在论文中算法1会有一个迭代过程。值得注意的是，我们得到最终的$\sum_i{r_i}​$表示的是从$x_0​$到分类边界投影点的距离向量，满足$f(x_0+\sum_i{r_i})=0​$。如果要使分类结果改变，需要再添加一些极小的扰动，如$f(x_0+(1+\eta)\sum_i{r_i})&lt;0​$（文中$\eta​$取0.02）。 &emsp;&emsp;对于多分类任务，思路也大致相同。在线性多分类任务中，需要注意的大致有两点：首先，对于多分类任务的分类边界要重新进行定义。我们令$f(x)​$为分类器，$\hat{k}(x)=\mathop{\arg\max}_{k}{f_k(x)}​$表示其对应的分类结果（一共有$k​$个类别）。分类边界定义为$$\mathscr{F}_k={x : f_k(x)-f_{\hat{k}(x_0)}(x)=0}$$其次，由于分类边界有多个，我们需要以此求出$x_0​$到每个分类边界的距离（类似于进行多次二分类的计算过程），然后进行比较，选择其中最短距离向量作为最终的扰动。$$\hat{l}(x_0)=\mathop{\arg\min_{k \neq \hat{k}_{x_0} }}{\frac{|f_k(x_0)-f_{\hat{k}(x_0)}(x_0)|}{|{w_k -w_{\hat{k}_{x_0}}}|_2}}$$之后通过计算$\boldsymbol{r}(x_0)​$得到$x_0​$在分类边界上的投影。 &emsp;&emsp;在非线性多分类任务中，与线性多分类的区别在于其分类边界是不确定的，所以我们需要采用类似于非线性二分类任务中的方法来逼近分类边界。获得分类边界之后的计算与线性多分类的过程类似。此后，重复该过程多次，即可获得最终$x_0​$在分类边界的投影。 代码实现&emsp;&emsp;以下讨论多分类情况下DeepFool算法的代码实现，模型结构以及数据集等与上述一致，不再重复。首先，我们选择一个测试样本，生成该样本的对抗性样例。 1234net = torch.load('mnist_net_all.pkl') # 加载模型index = 100 # 选择测试样本image = Variable(testdata[index][0].resize_(1,784), requires_grad=True)label = torch.tensor([testdata[index][1]]) 接下来，通过前向传播的过程，我们获得该样本对每一类别可能的取值情况，并将其从大到小排列起来，列表$I$对应其索引值，$label$即$\hat{k}_{x_0}$。 1234567f_image = net.forward(image).data.numpy().flatten() # f_image: [-1.1256416 , -1.0344085 , 2.0596995 , -2.0181773 , -0.24274658, -0.53373957, 6.6361637 , -2.250309 , 0.06580263, -3.0854702 ]I = (np.array(f_image)).flatten().argsort()[::-1]# I: [6, 2, 8, 4, 5, 1, 0, 3, 7, 9]label = I[0] # 该样本的标签为6 然后，我们定义一些需要用到的变量 123456789101112input_shape = image.data.numpy().shape # 获取原始样本的维度pert_image = copy.deepcopy(image) # 深度复制原始样本w = np.zeros(input_shape) r_tot = np.zeros(input_shape)loop_i = 0 max_iter = 50 # 最多迭代次数overshoot = 0.02 x = Variable(pert_image, requires_grad=True)fs = net.forward(x)fs_list = [fs[0][I[k]] for k in range(len(I))] # 每个类别的取值情况，及其对应的梯度值k_i = label 下面是算法实现的核心部分，参考论文中的伪代码，其中orig_grad表示$\nabla f_{\hat{k}_{x_0}}(x_i)$，cur_grad表示$\nabla f_k(x_i)$，fs[0][I[k]]表示$f_k(x_i)$，fs[0][I[0]]表示$f_{\hat{k}_{x_0}}(x_i)$。通过内部的for循环可以获得x到各分类边界的距离；在外部的while循环中，我们利用内部循环获得的所有边界距离中的最小值对x进行更新。重复这一过程，直到$x​$的分类标签发生变化。 1234567891011121314151617181920212223242526while k_i == label and loop_i &lt; max_iter: pert = np.inf fs[0][I[0]].backward(retain_graph=True) orig_grad = x.grad.data.numpy().copy() for k in range(len(I)): zero_gradients(x) fs[0][I[k]].backward(retain_graph=True) cur_grad = x.grad.data.numpy().copy() w_k = cur_grad - orig_grad f_k = (fs[0][I[k]] - fs[0][I[0]]).data.numpy() pert_k = abs(f_k) / np.linalg.norm(w_k.flatten()) if pert_k &lt; pert: # 获得最小的分类边界距离向量 pert = pert_k w = w_k r_i = (pert + 1e-4) * w / np.linalg.norm(w) r_tot = np.float32(r_tot + r_i) # 累积扰动 pert_image = image + (1+overshoot)*torch.from_numpy(r_tot) # 添加扰动 x = Variable(pert_image, requires_grad=True) fs = net.forward(x) k_i = np.argmax(fs.data.numpy().flatten()) # 扰动后的分类标签 loop_i += 1r_tot = (1+overshoot)*r_tot # 最终累积的扰动 对原始图片，添加扰动获得如下图片（分类器将其错误分类为2，有些奇怪的地方在于其扰动的程度要大FGSM，算法应该没有问题，一直没找到原因）。 Universal Perturbation&emsp;&emsp;前面介绍的FGSM和DeepFool算法，它们都是针对单个样本生成其对抗性样例，也就是说每个对抗性样例的扰动程度都不同。那么是否能找到一种通用性的扰动边界，能够为不同的样本生成对抗性样例。在DeepFool工作的基础上，Seyed 等人提出了Universal Perturbation，该算法为寻找通用性扰动边界提供了可能。以下简单介绍一下论文的核心思想： 从数据集中随机选取部分测试样本作为生成通用性扰动边界的范例，通过这些测试样本生成的通用性扰动适用于整个数据集。 算法的计算过程：输入第一个样本后，通过DeepFool算法找到该样本的最小扰动距离向量，将其累积到通用扰动向量$v$中（对$v$的扰动程度会有限制和调整，${|v|_p&lt;\xi}$）；当输入第二个样本之后，对其添加$v$的扰动之后，然后再通过DeepFool计算扰动后的样本的最小扰动距离向量，将其累积到通用扰动向量$v$中（对$v$的扰动程度会有限制和调整,${|v|_p&lt;\xi}$）。重复这一过程，直到最后一个测试样本。然后，我们使用通用扰动向量$v$，对原始的测试样本进行扰动，测试其生成对抗性样例的成功率。如果小于预先设置的阈值$1-\delta$，则跳出循环返回结果。否则，重复上述过程。 通用性扰动对不同的网络模型依然有效。也就是说，利用网络模型A生成的通用性扰动，同样适用于生成网络模型B的对抗性样例（A、B是不同类型的网络架构）。]]></content>
      <categories>
        <category>机器学习安全</category>
      </categories>
      <tags>
        <tag>深度学习</tag>
        <tag>对抗样本</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[校园网环境下服务器双网卡配置]]></title>
    <url>%2F%E6%A0%A1%E5%9B%AD%E7%BD%91%E7%8E%AF%E5%A2%83%E4%B8%8B%E6%9C%8D%E5%8A%A1%E5%99%A8%E5%8F%8C%E7%BD%91%E5%8D%A1%E9%85%8D%E7%BD%AE.html%2F</url>
    <content type="text"><![CDATA[摘要：最近一段时间在忙着写论文、看论文，博客一直没有更新了。这几天实验室添了两台交换机和五台服务器，这对于我这个爱折腾的人来说，确实是个大喜事，老师也把实验室设备的管理工作全权交予我来负责。忙活了几天时间，装好了系统，建立了一个小型的Hadoop集群和私有云。这其中我觉得最有意思的是关于实验室网络环境的配置，所以在这里做一些分享。 文章概览 网络环境简介 双网卡配置 Linux密码 网络环境简介&emsp;&emsp;一开始，实验室的服务器都没有分配校园网固定IP，为了对服务器进行安装配置，我用了一台闲置的交换机和一个二手路由器组建了一个简单的网络环境（服务器—&gt;交换机—&gt;路由器）。但是存在的问题是，服务器只能在实验室的网络环境才能访问，无法通过校园网访问。一些内网映射的工具又不太稳定，使用起来很不方便，最关键的是路由器成为了整个网络的瓶颈（几十块钱的路由器）。后来通过学校网络中心分配了几个固定的校园网IP，但是校园网IP不能直接连接外网，需要拨号才能联网。五台服务器也就意味着需要五个账号才能使所有的服务器同时连接外网，我也没有这么多账号。仔细考虑了一下我们当前的需求和配置： 所有的服务器能通过校园网直接访问 所有的服务器能同时连接外网 宽带账号只有一个，路由器一个，交换机一个 综合这些因素，配置服务器双网卡可能是一个好的选择。每台服务器都有多个网口，将每台服务器同时连接在校园网和路由器两个网络中就能满足这些需求。通过给连接在校园网的网口配置校园网固定IP，可以保证校园网对服务器的访问；通过路由器拨号上网，将服务器的另一个网口连接到路由器上，即可保证服务器对外网的访问。 双网卡配置&emsp;&emsp;配置过程需要注意以下几点： 再给两个网卡配置静态IP的时候，不需要GATEWAY字段 删除系统默认的网关（在 /etc/sysconfig/network文件中修改） 使用ip route命令添加默认网关 （ip route add default via 网关地址 dev 网卡1）。一般我们将路由器的网关地址作为服务器的默认网关，这样能保证服务器可以访问外网。 添加默认路由（ip route add 校园网网段 via 校园网静态ip的网关地址 dev 网卡2）。为了在校园网内部可以对服务器进行访问，我们需要添加一条默认的路由。对于校园网网段的访问，不通过路由器，而是通过校园网静态ip的网关地址。 Linux密码&emsp;&emsp;之前一个同学在实验室主机上装过centos，可能太长时间没用，所以root密码忘了。找回LInux密码的过程挺有意思，一般情况下，通过单用户模式可以对root密码进行修改，但是系统设置了grub密码（防止修改用户密码）。至于grub密码就更不记得了，于是我们又想通过救援模式去修改grub密码，然后再修改root密码。参考了很多教程，我们发现修改不了grub密码。就在准备放弃的时候，我发现救援模式下可以直接访问/etc/shadow文件，我试着将文件中root用户记录删除，竟然修改成功了。重新开机之后，奇迹发生了，可以使用root用户直接登录了。这里给出参考的博客链接，希望对大家有所帮助。]]></content>
      <categories>
        <category>网络安全</category>
      </categories>
      <tags>
        <tag>Linux</tag>
        <tag>校园网</tag>
        <tag>服务器</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[tor匿名网络]]></title>
    <url>%2FTor%E5%8C%BF%E5%90%8D%E7%BD%91%E7%BB%9C.html%2F</url>
    <content type="text"><![CDATA[摘要： 前段时间看到一篇报道，暗网最大的托管商遭到黑客攻击，6500+网站被删。根据知道创宇平台的暗网雷达显示，一夜之间活跃在暗网中的网站从12000+下降到5000+，几乎遭到团灭，这一事件让暗网再次进入人们的视野。一直以来，暗网被赋予了很多传奇的色彩，本文将带你一步步揭开暗网的神秘面纱。 文章概览 基本概念 深网 暗网 黑暗网络 tor匿名网络 tor基本概述 tor网络结构 tor路由技术 tor匿名服务 tor客户端代理 基本概念&emsp;&emsp;在介绍暗网之前，我们先来了解一下经常会混淆的三个概念“深网”(Deep web)、“暗网”(Dark web) 和“黑暗网络”(Darknet) 。 深网&emsp;&emsp;深网是指服务器上可通过标准的网络浏览器和连接方法访问的页面和服务，但主流搜索引擎不会收录这些页面和服务。搜索引擎之所以不会收录深网，通常是因为网站或服务的配置错误、拒绝爬虫爬取信息、需要付费查看、需要注册查看或其他内容访问限制。据不完全统计，互联网世界中只有4％是对公众开放，剩下的96％的网站和数据则隐藏在深网中。 暗网&emsp;&emsp;暗网是深网中相对较小的一部分，与被故意隐藏的 Web 服务和页面有关。仅使用标准浏览器无法直接访问这些服务和页面，必须依靠使用覆盖网络 (Overlay Network)；而这种网络需要特定访问权限、代理配置、专用软件或特殊网络协议。 黑暗网络&emsp;&emsp;黑暗网络是在网络层访问受限的框架，例如 tor 或 I2P，私有 VPN 也属于这个类别。通过这些框架的网络流量会被屏蔽。当进行数据传输时，系统只会显示您连接的黑暗网络以及您传输了多少数据，而不一定会显示您访问的网站或所涉及数据的内容。与之相反的是，直接与明网（Clean Net）或与未加密的表网服务和深网服务交互。在这种情况下，您与所请求资源之间的互联网服务提供商和网络运营商可以看到您传输的流量内容。 tor匿名网络基本概述&emsp;&emsp;经过上面的描述，我们对暗网整体的框架有个大致的了解。暗网不同于普通的互联网络，它拥有特殊的运作方式和网络协议，tor 是目前世界范围内是最大的暗网。tor又名洋葱网络，最初是由美国军方的情报机构开发并且提供财务支持的。洋葱网络使用特殊的路由转发技术，即洋葱路由技术。洋葱路由技术利用 P2P 网络,把网络流量随机地通过 P2P 的节点进行转发，这样可以掩盖源地址与目标地址的路径，使得在 Internet 上难以确定使用者的身份和地址。这就类似于你给某人送一封匿名信，你不是自己去送或者通过邮局的邮差去送，而是在大街上随便找几个不认识的人让他帮你送，这样收信人就很难往回追踪找到你。 &emsp;&emsp;洋葱路由项目最初进展缓慢,废弃了其中的几个版本。直到 2002 年才由麻省理工的两位毕业生 Roger Dingledine 和 Nick Mathewson 以及原项目组成员 PaulSyverson 一起开发出一个新版的洋葱路由，也就是Tor。“洋葱路由”的最初目的并不是保护大众的隐私，它的目的是帮助政府情报人员隐藏身份，网上活动不被敌对国家监控。后来为了混淆系统的流量，而不是让系统仅仅拥有来自美国安全部门网络的流量，他们又让系统中加入来自其它网络的流量。于是，tor 的普通版本被推广给了普通大众，让普通大众也能使用 tor 来保护自己的隐私。这样就可以把政府情报人员的流量与社会上各行各业用户的复杂流量混在一起，能让流量分析更加困难，提供更强地隐私保护。 &emsp;&emsp;目前全球范围内，tor网络由超过一万七千个中继节点组成，每个中继节点都是由全球志愿者免费提供，这些中继节点大部分分布在欧洲和北美。暗网除了能给我们提供匿名的网络服务之外，在按暗网中也存在各种类型的网站，这些网站只有通过暗网才能够访问。暗网中的网站大部分都是提供一些“特殊的服务”，包括枪支、毒品、网络攻击等等。 网络结构Tor 匿名网络结构主要由目录服务器、洋葱代理、洋葱路由器这三部分组成。 目录服务器：它负责收集和更新网络中所有可运行的中继节点信息，为客户端提供节点公钥等建立链路所需的必要信息。目前，绝大多数的目录服务器在欧美地区,只有极少数分布在亚洲。 洋葱代理：它是 tor 用户的客户端代理程序，负责下载目录服务器中的路由信息，选择节点和建立路径，并对通信信息进行加解密。 洋葱路由器（又称中继节点）：它是构建匿名通信链路的基础，负责转发 tor 客户端和网络服务器的通信信息，是实现匿名通信的关键。目前整个 tor 网络中有几千个路由节点分布在世界各地，这些中继节点又分为三种类型: Entry/Guard 中继节点──这是 tor 网络的入口节点。这些中继节点运行一段时间后，如果被证明是稳定的，并具有高带宽，就会被选来作为 Guard 中继节点。 Middle 中继节点──Middle 中继节点是位于中间节点位置上的洋葱路由器，充当流量从 Guard 中继节点传输到 Exit 中继节点的桥梁，这可以避免 Guard 中继节点和 Exit 中继节点探查到彼此的位置。 Exit 中继节点──位于出口节点位置上的洋葱路由器，负责将 Tor 网络内的流量转发到网络外部的互联网中去。每个出口节点都有一个相关的出口政策，该政策规定该节点能通过哪个端口转发何种协议的流量来防止滥用 tor 网络。 &emsp;&emsp;tor 网络主要依赖于这些中继节点转发用户流量，tor 通过随机选取遍布于全球由志愿者运行的三个中继节点，然后分别与选择的入口节点、中间节点、出口节点协商会话密钥。用这些协商的密钥将通信数据先进行多层加密，然后再将加密的数据在三个洋葱路由器组成的通信链路上传送。数据每经过一个洋葱路由器就像是剥去一层洋葱皮一样解密去掉一个加密层，以此得到下一跳路由信息，然后将数据继续发往下一个洋葱路由器，不断重复此过程,直到数据送达目的地。这种转发方式能防止那些知道数据发送端以及接收端的中间人窃听数据内容。 tor路由技术以下步骤讲述了Alice在使用tor与Bob的服务器进行通讯时，tor是如何工作的。 Alice开启tor客户端代理，获取来自tor 目录服务器（Dave）中的tor节点（或中继的列表）以及它们的公钥。 Alice 选择三个节点建立通信链路。Alice 得到入口中继节点的 IP 地址和身份摘要，和入口节点协商一个只用于两者之间通信的短暂会话密钥。成功建立一跳的链路以后，Alice 使用同样地方法要求入口节点拓展链路到中间节点，得到了 Alice与中间节点的短暂会话密钥。Alice 重复此过程直至建立一条含有三跳的通信链路并且获得三个她与三个节点独一无二的短暂会话密钥。整个链路建立过程中的所有连接都是加密的。然后Alice 向 Bob 发送服务请求，并且将请求内容使用三个会话密钥按由远到近的顺序依次加密。入口节点接收到解密消息后会使用会话密钥将其解密一层，并将仍然加密的信息转发给中间节点。直至到达出口节点后，才将信息解密为明文。图中的虚线表示出口节点与 Bob 之间的连接是未加密的，出口节点将原始数据发送给 Bob。Bob 回复请求内容，并且每个节点会以相反的加密顺序加密一层,最终送还给 Alice。 如果 Alice 与 Bob 通信时间较长，Alice 每隔几分钟会重新选择三个节点建立新的通信链路以防攻击者窃听。如果 Alice 想要访问另一服务器 Jane，她也会重新选择中继节点建立新的通信链路。 tor匿名服务&emsp;&emsp;之前提到暗网中也存在很多网站，这些网站在提供相应的服务时（暗网中大部分网络服务都是违法的）也希望是匿名的，即用户无法追踪到关于该网站的相关信息。下面主要讲解用户Alice和匿名服务器Bob交互的具体过程。 服务器Bob与tor网络中的一些中继节点连接，请求这些中继节点作为匿名服务的接入点，并将Bob的公钥发送给这些中继节点。注意Bob和中继节点之间的连接也是匿名的，这些中继节点无法获取关于Bob的相关位置信息。 Bob将之前建立的接入点的相关信息和自己的公钥组装成描述符，并用自己的私钥该描述符进行签名，然后将其发送到目录服务器。通过Bob的公钥可以生成一个16位的字符串，记为XYZ。当客户端请求XYZ.onion时就可以找到对应Bob的描述符。 Alice通过某些渠道获得了tor域名XYZ.onion，Alice想要访问该服务器。她通过tor客户端访问XYZ.onion，此时可以从目录服务器中获取对应服务器的描述符，通过描述符可以知道Bob服务器的接入点和公钥。与此同时，Alice会提前建立另外一条私密临时会话点，用于下一步与Bob交互。 当确认描述符存在且临时通道准备好之后，Alice用描述符中的公钥加密一条信息，包括临时会话点和会话秘钥，将加密后的信息发送给描述符中的接入点，之后接入点会将加密信息发送到对应的服务器（即Bob的服务器）。 Bob收到加密信息后将其解密，获取临时会话点和会话秘钥。然后和临时会话点建立匿名连接，连接成功后，临时会话点会通知Alice。之后Alice和Bob可以通过临时会话点进行通信，注意通信过程中建立的连接都是匿名的。 tor客户端代理&emsp;&emsp;如果只是希望匿名浏览web网页，我们可以通过tor浏览器来实现。tor浏览器是基于火狐浏览器改造而来，可以方便的帮助我们连接到tor，实现网络匿名。如果想要在更多应用中实现网络匿名，我们可以安装tor客户端代理。以下配置过程基于ubuntu，且默认已安装shadowsocks。 安装polipo&emsp;&emsp;polipo是轻量级的跨平台代理服务器，可以实现http和socks代理，polipo本地服务端口为8123。 1sudo apt-get install polipo 安装完成后，修改配置文件/etc/polipo/config 12socksParentProcy = "localhost:9050" #tor服务本地端口为9050socksProxyType = socks5 安装tor1sudo apt-get install tor 安装完成后，修改配置文件/etc/tor/torrc，添加以下内容。 1SOCKS5Proxy 127.0.0.1:1080 #shadowsocks本地服务端口为1080 测试&emsp;&emsp;完成上述操作后，开启polipo、tor和shadowsocks。如果我们希望在chrome浏览器中实现网络匿名，可以通过添加tor代理来实现。 1代理协议：socks5 代理服务器：127.0.0.1 代理端口：9050 如果我们希望在命令行中实现网络匿名，可以通过polipo代理来 1http_proxy=http://localhost:8123 "需要执行的操作“]]></content>
      <categories>
        <category>网络安全</category>
      </categories>
      <tags>
        <tag>洋葱路由</tag>
        <tag>计算机网络</tag>
        <tag>暗网</tag>
        <tag>代理</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Hadoop分布式集群搭建]]></title>
    <url>%2FHadoop%E5%88%86%E5%B8%83%E5%BC%8F%E9%9B%86%E7%BE%A4%E6%90%AD%E5%BB%BA.html%2F</url>
    <content type="text"><![CDATA[摘要：Hadoop，是一个分布式系统基础架构，由Apache基金会开发。用户可以在不了解分布式底层细节的情况下，开发分布式程序。充分利用集群的威力高速运算和存储。简单地说来，Hadoop是一个可以更容易开发和运行处理大规模数据的软件平台。该平台使用的是面向对象编程语言Java实现的，具有良好的可移植性。本文将介绍Hadoop相关的技术框架以及搭建Hadoop平台的详细过程。 文章概览 Hadoop简介 Hadoop体系结构 HDFS分布式文件系统 MapReduce编程模型 Hadoop平台搭建 Hadoop简介Hadoop体系结构 HDFS分布式文件系统&emsp;&emsp;在正式讨论HDFS分布式文件系统之前，我们首先了解一下什么是文件系统。文件系统实际上可以看作是一个用户与底层数据交互的一个接口，对于底层数据而言它定义了数据的存储和组织方式，同时也提供了存储空间的管理功能；而对于用户而言它使用文件和树形目录的抽象逻辑概念代替了存储设备中块的概念，用户使用文件系统来操作数据不必关心数据实际保存在硬盘（或者光盘）的地址为多少的数据块上，只需要记住这个文件的所属目录和文件名（关于文件系统更详细的介绍参见维基百科和这篇博客)。传统文件系统适用于存储容量小等一些没有特殊要求的应用场景。 &emsp;&emsp;但是随着信息技术的不断发展，人们可以获取的数据成指数倍的增长，单纯通过增加硬盘个数来扩展计算机文件系统的存储容量的方式，在容量大小、容量增长速度、数据备份、数据安全等方面的表现都差强人意。为了满足这些特殊应用场景的需求，分布式文件系统应运而生。分布式文件系统可以有效解决数据的存储和管理难题：将固定于某个地点的某个文件系统，扩展到任意多个地点/多个文件系统，众多的节点组成一个文件系统网络。每个节点可以分布在不同的地点，通过网络进行节点间的通信和数据传输。人们在使用分布式文件系统时，无需关心数据是存储在哪个节点上、或者是从哪个节点从获取的，只需要像使用本地文件系统一样管理和存储文件系统中的数据（我们需要知道的是在分布式文件系统的每个数据结点上，数据的存储方式是建立在传统文件系统的基础上的，所谓分布式文件系统它提供的是数据宏观上的存储和管理方式）。 &emsp;&emsp;HDFS是分布式文件系统的一种，它采用master/slave架构，一个HDFS集群是由一个Namenode和一定数目的Datanodes组成。它可以处理超大规模的数据，并且提供了良好的容错机制，下图是HDFS的基本结构。 NameNode: 可以看作是分布式文件系统中的管理者，主要负责管理文件系统的命名空间、集群配置信息和存储块的复制等，NameNode会将文件系统的Meta-data存储到内存中，这些信息主要包括了文件信息、每一个文件对应的文件块的信息和每一个文件块在DataNode的信息等。 Datanode: DataNode是文件存储的基本单元，他将Block存储在本地文件系统中，保存了Block的meta-data，同时周期性的将所有存在的Block信息发送给NameNode。slave存储实际的数据块，执行数据块的读写。 Client: 文件切分与NameNode的交互，获取文件位置信息；与DataNode交互，读取或者写入数据；管理HDFS；访问HDFS。 HDFS读取数据流程客户端将要读取的文件路径发送给Namenode，Namenode获取文件的元信息（主要是block的存放位置信息）返回给客户端，客户端根据返回的信息找到相应Datanode逐个获取文件的block并在客户端本地进行数据追加合并从而获得整个文件 HDFS写数据流程客户端要向HDFS写数据，首先要跟Namenode通信以确认可以写文件并获得接收文件block的Datanode，然后，客户端按顺序将文件逐个block传递给相应Datanode，并由接收到block的Datanode负责向其他Datanode复制block的副本。 MapReduce编程模型&emsp;&emsp;MapReduce的诞生也是由于对大规模数据处理的需求。在大型的互联网公司，比如说Google、亚马逊等，在他们的平台上每天都会产生大量的数据，单个的处理器不可能在有限的时间内完成计算。根据多线程和并行计算的启发，我们可以将这些计算分布在成百上千的的机器上，这些机器集群就可以看作硬件资源池，将并行的任务拆分，然后交由每一个空闲机器资源去处理，能够极大地提高计算效率。但是由此而来引发的问题是在这个分布式计算系统中应该如何合理的处理并行计算？如何分发数据？如何处理错误？为了避免对这些问题的考虑，我们希望获得这样一个抽象模型，在这个模型中我们只需要关注我们希望执行的任务，而不必关心并行计算、容错、数据分布、负载均衡等复杂的细节，MapReduce就是这样一个抽象模型。 &emsp;&emsp;MapReduce 是一个编程模型，也是一个处理和生成超大数据集的算法模型的相关实现。用户首先创建一 个 Map 函数处理一个基于 key/value pair 的数据集合， 输出中间的基于 key/value pair 的数据集合； 然后再创建 一个 Reduce 函数用来合并所有的具有相同中间 key 值的中间 value 值。MapReduce 架构的程序能够在大量的普通配置的计算机上实现并行化处理。这个系统在运行时只关心： 如何分割输入数据，在大量计算机组成的集群上的调度，集群中计算机的错误处理，管理集群中计算机之间 必要的通信。采用 MapReduce 架构可以使那些没有并行计算和分布式处理系统开发经验的程序员有效利用分布式系统的丰富资源。 执行过程 用户程序首先调用的 MapReduce 库将输入文件分成 M 个数据片度，每个数据片段的大小一般从 16MB 到 64MB(可以通过可选的参数来控制每个数据片段的大小)。然后用户程序在机群中创建大量 的程序副本。 这些程序副本中的有一个特殊的程序–master。副本中其它的程序都是 worker 程序，由 master 分配 任务。有 M 个 Map 任务和 R 个 Reduce 任务将被分配，master 将一个 Map 任务或 Reduce 任务分配 给一个空闲的 worker。 被分配了 map 任务的 worker 程序读取相关的输入数据片段，从输入的数据片段中解析出 key/value pair，然后把 key/value pair 传递给用户自定义的 Map 函数，由 Map 函数生成并输出的中间 key/value pair，并缓存在内存中。 缓存中的 key/value pair 通过分区函数分成 R 个区域，之后周期性的写入到本地磁盘上。缓存的 key/value pair 在本地磁盘上的存储位置将被回传给 master，由 master 负责把这些存储位置再传送给 Reduce worker 当 Reduce worker 程序接收到 master 程序发来的数据存储位置信息后， 使用 RPC 从 Map worker 所在的磁盘上读取这些缓存数据。当 Reduce worker 读取了所有的中间数据后，通过对 key 进行排序 后使得具有相同 key 值的数据聚合在一起。由于许多不同的 key 值会映射到相同的 Reduce 任务上， 因此必须进行排序。如果中间数据太大无法在内存中完成排序，那么就要在外部进行排序。 Reduce worker 程序遍历排序后的中间数据， 对于每一个唯一的中间 key 值， Reduce worker 程序将这 个 key 值和它相关的中间 value 值的集合传递给用户自定义的 Reduce 函数。Reduce 函数的输出被追加到所属分区的输出文件。 当所有的 Map 和 Reduce 任务都完成之后，master 唤醒用户程序。在这个时候，在用户程序里对 MapReduce 调用才返回。 Hadoop平台搭建&emsp;&emsp;Hadoop有三种安装模式：本地模式安装、伪分布模式安装和完全分布式安装。本文主要介绍Hadoop完全分布式安装，真实环境下都是以这种方式部署（1台Master，2台Slave）。 部署环境 Hadoop2.8.5 VMware14 ubuntu16.04 jdk11.01 第一步：安装ubuntu虚拟机的安装教程网上很多，这里不在赘述，注意这里只需要安装一个虚拟机，虚拟机的网络连接方式为NAT。 第二步：在虚拟机中安装jdk我使用的jdk版本是jdk11.01，建议使用jdk1.8。jdk安装完成后需要配置环境变量，jdk的默认安装路径是 1/usr/lib/jvm/jdk-11.01 编辑用户目录下.bashrc文件，在文件末尾添加 1234export JAVA_HOME=/usr/lib/jvm/jdk-11.01 export JRE_HOME=$&#123;JAVA_HOME&#125;/jre export CLASSPATH=.:$&#123;JAVA_HOME&#125;/lib:$&#123;JRE_HOME&#125;/lib export PATH=$&#123;JAVA_HOME&#125;/bin:$PATH 第三步：克隆虚拟机利用VMware的克隆功能，克隆两个虚拟机 第四步：修改hostname文件现在我们得到三个一模一样的虚拟机，我们选取其中一个虚拟机为Master，其余两个虚拟机分别为Slave。修改Master主机的hostname为Master，两个Slave的hostname分别为Slave1和Slave2，hosts文件的路径为 1/etc/hostname 第五步：配置静态IP分别查看三台主机的IP地址，然后修改hosts文件，将三台主机的hostname以及对于的IP添加到hosts文件中，hosts文件路径为，三台主机都要进行同样的操作。 1/etc/hosts 在我的系统中，三台主机的IP如图所示 第六步：建立Hadoop运行账号在三台主机上都要建立一个hadoop用户组，并在用户组中添加名为hduser的用户,具体操作如下。 12345sudo groupadd hadoop #建立hadoop用户组sudo useradd -s /bin/bash -d /home/hduser -m hduser -g hadoop #添加hduser，指定用户目录sudo passwd hduser #修改hduser用户密码sudo adduser hduser sudo #赋予hduser管理员权限su hduser #切换到hduser用户 第七步：配置ssh免密登录ubuntu默认安装了ssh客户端，但没有安装ssh服务器，配置之前先在三台主机中安装ssh-server 1sudo apt-get install 接着在Master主机中执行如下命令 12ssh-keygen -t rsa -P '' -f ~/.ssh/id_rsa #生成ssh公钥和私钥cat ~/.ssh/id_rsa.pub &gt;&gt; ~/.ssh/authorized_keys #添加公钥到已认证的key中 在两台Slave主机中的用户目录下新建.ssh文件夹，然后将Master中的id_rsa.pub文件复制到两台Slave主机的.ssh文件夹下，执行如下命令 1cat ~/.ssh/id_rsa.pub &gt;&gt; ~/.ssh/authorized_keys #添加公钥到已认证的key中 最后验证是否可以通过Master免密登录两台Slave主机。 第八步:下载并解压Hadoop三台主机都要进行该操作，在用户目录下建立名为hadoop2.8.5的目录，将文件解压到该目录下。 第九步：修改配置文件三台主机都要进行该操作 修改hadoop-env.sh文件,添加JAVA_HOME。（~/hadoop/hadoop2.8.5/etc/hadoop/hadoop-env.sh） 修改core-site.cml文件，添加如下内容。（~/hadoop/hadoop2.8.5/etc/hadoop/core-site.cml) 修改hdfs-site.xml文件，添加如下内容。（~/hadoop/hadoop2.8.5/etc/hadoop/hdfs-site.xml） 修改mapred-site.xml.template文件，添加如下内容。（~/hadoop/hadoop2.8.5/etc/hadoop/mapred-site.xml.template） 修改slaves文件，将两台Slave主机名添加进去即可。（~/hadoop/hadoop2.8.5/etc/hadoop/slaves） 修改/etc/profile文件，添加如下内容 123export JAVA_HOME=/usr/lib/jvm/jdk-11.0.1export HADOOP_INSTALL=/home/hduser/hadoop/hadoop-2.8.5export PATH=$PATH:$&#123;HADOOP_INSTALL&#125;/bin:$&#123;HADOOP_INSTALL&#125;/sbin:$&#123;JAVA_HOME&#125;/bin 然后执行 1source /etc/profile 检查hadoop是否安装成功 第十步：格式化namenode，并启动集群在Master主机中执行以下命令 12hadoop namenode -format #格式化namenodestart-all.ssh #启动集群 在ubuntu地址栏中输入http:Master:50070,可以看到Slave结点的相关信息]]></content>
      <categories>
        <category>Linux</category>
      </categories>
      <tags>
        <tag>Linux</tag>
        <tag>Hadoop集群</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[HITB安全峰会之旅.md]]></title>
    <url>%2FHITB%E5%AE%89%E5%85%A8%E5%B3%B0%E4%BC%9A%E4%B9%8B%E6%97%85.html%2F</url>
    <content type="text"><![CDATA[摘要：2018年10月25日，一次偶然的机会在阿里安全响应中心看到了一则新闻-HITB首次走进中国。当时也并没有太在意，因为本身对HITB没有什么印象，在安全领域我关注的会议并不多，比较了解的是BLACKHAT和DEFCON。正好前几天去合肥参加了科大讯飞的全球开发者大会，回来之后其实挺失望的（从学术界到产业界都在极力鼓吹“AI”所蕴含的价值，但是却始终看不到实质性的产品和突破，而且很大程度忽视了“AI”所带来的负面影响），当看到这则新闻的时候有感而发，在留言处写下了一点个人的感受。没想到的是，第二天居然收到通知说我的留言被小编抽中了，作为奖励我也免费获得了一张HITB安全峰会的门票。突如其来的惊喜让我有点措手不及，我赶紧上网查了一下关于HITB峰会的相关信息，这才对HITB有了一些了解。作为欧洲三大顶级安全会议之一，HITB一直拥有良好的口碑和技术氛围，我觉得这对我来说是一次很好的机会。之后，我也获得了导师的支持，于是2018年10月30日，我踏上了这次难忘的HITB安全峰会之旅。]]></content>
      <categories>
        <category>心得体会</category>
      </categories>
      <tags>
        <tag>网络安全</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[维吉尼亚加解密及唯密文破解]]></title>
    <url>%2F%E7%BB%B4%E5%90%89%E5%B0%BC%E4%BA%9A%E5%8A%A0%E8%A7%A3%E5%AF%86%E5%8F%8A%E5%94%AF%E5%AF%86%E6%96%87%E7%A0%B4%E8%A7%A3.html%2F</url>
    <content type="text"><![CDATA[摘要 古典密码体制主要通过字符间的置换和代换来实现，常见的置换密码包括列置换密码和周期置换密码，而常见的代换密码包括单表代换密码和多表代换密码，本文所讨论的维吉尼亚算法是属于多表代换密码的一种。多表代换密码是以一系列代换表依次对明文消息的字母序列进行代换的加密方法，即明文消息中出现的同一个字母，在加密时不是完全被同一固定的字母代换，而是根据其出现的位置次序用不同的字母代换。如果代换表序列是非周期的无限序列，则相应的密码称为非周期多表代换密码，这类密码对每个明文都采用了不同的代换表进行加密，故称为一次一密密码，它是理论上不可破译的密码体制。但实际应用中经常采用的是周期多表代换密码，它通常使用有限个代换表，代换表被重复使用以完成消息的加密。作为多表代换密码的典型代表，维吉尼亚密码算法蕴含着丰富的古典密码设计思想，本文将深入探讨维吉尼亚算法的加解密过程实现，以及利用统计分析的方法进行唯密文攻击。 文章概览 维吉尼亚算法简介 加密算法实现 编码方式 对明文进行处理 加密过程 解密算法实现 唯密文攻击 确定密钥长度 确定密钥 恢复明文 维吉尼亚算法简介加密算法实现&emsp;&emsp;实现加密算法的大致流程是：首先我们需要确定编码方式，本文采用的编码方式是[a-z]对应[0-25]；接着进行加密算法前需要对明文字符串进行处理，删除非字母字符，将大写字符统一转换为小写字母；最后选定密钥对密文中的逐个字符进行加密（即代换操作），生成最后的密文。 编码方式&emsp;&emsp;本文的字母编码方式由列表s确定，s中每个元素的索引即对应该元素的数字编码。 1s = ['a','b','c','d','e','f','g','h','i','j','k','l','m','n','o','p','q','r','s','t','u','v','w','x','y','z'] 对明文进行处理&emsp;&emsp;对明文进行处理的目的是去除明文中非字母的字符，并将大写字母统一转换为小写字母。转换大小写我们可以使用python字符串内置的lower()函数，稍微有点棘手的是前者，因为在这里需要考虑到一些效率的问题还有如何对后续操作进行优化的问题，比如说: 读取文件中的明文时我们可以采用read()，readline()，readlines()这三个函数，那我们到底采用哪一个呢？（这三个函数的对比可以参考这篇博客) 采用不同读取明文的函数，导致读取结果也不尽相同，有列表形式也有字符串形式，到底哪种形式对后续的操作更有好处 去掉读取后的明文中的非字母字符应采用何种方式？（逐个字符判断或者正则表达式） 本文根据文本的实际情况，采用的处理方式如下所示 12345678910def pretreatment(): """ pretreatment函数的主要作用是对明文进行预处理，去除非字母字符和转换大小写 :return: 经过预处理的明文字符串 """ with open("plain.txt","r") as f: wen = f.read() pattern = re.compile('[\n]|\d|\W') plain_1 = re.sub(pattern,'',wen).lower() return plain_1 加密过程&emsp;&emsp;维吉尼亚算法的加密过程比较简单，基本思想是利用密钥循环对明文字符进行代换操作，进行代换前将相应的明文字符和密钥字符转化为对应的数字编码，然后相加对26取余即得到对应的密文字符。 12345678910111213141516171819def encrypt(key): """ encrypt函数的主要作用是进行加密 :param key: 密钥 :return: 密文字符串 """ wen = pretreatment() num_key = key_to_num(key) ciphertext = '' k = 0 for w in wen: if k == len(num_key): k = 0 cipher = change(w,num_key[k]) cipher = num_to_char(cipher) ciphertext = ciphertext + cipher k += 1 wirte_txt(ciphertext,'crypt.txt') return ciphertext 解密算法实现&emsp;&emsp;解密算法是加密算法的逆过程，进行的代换操作是将密文字符的数字编码减去密钥字符的数字编码，如果相减的结果小于0，则令结果加上26，在转换为对应编码的字符。 123456789101112131415161718192021222324252627282930313233def de_change(ch,num): """ de_change函数的作用是根据密文字符和密钥还原明文字符 :param ch: 密文字符 :param num: 密钥编码 :return: 明文字符 """ ch_num = char_to_num(ch) result = ch_num - num if result &lt; 0: result = 26 + result return resultdef decrypt(key): """ decryption函数的主要作用是将密文解密成明文 :param key: 密钥 :return: 明文 """ with open('crypt.txt','r') as f: ciphertext = f.read() num_key = key_to_num(key) wen = '' k = 0 for c in ciphertext: if k == len(num_key): k = 0 w = de_change(c,num_key[k]) w = num_to_char(w) wen = wen + w k += 1 wirte_txt(wen,'result.txt') return wen 唯密文攻击&emsp;&emsp;某种语言中各个字符出现的频率不一样而表现出一定的统计规律，而这种统计规律可能在密文中重现，所以我们可以通过统计分析的手段进行一些推测和验证过程来实现对密文的分析。在英文字母中各个字母出现的频率如下所示， 1234#编码规则s = ['a','b','c','d','e','f','g','h','i','j','k','l','m','n','o','p','q','r','s','t','u','v','w','x','y','z']#字母出现频率frequency = [0.082,0.015,0.028,0.043,0.127,0.022,0.02,0.061,0.07,0.002,0.008,0.04,0.024,0.06,0.075,0.019,0.001,0.06,0.063,0.091,0.028,0.01,0.023,0.001,0.02,0.001] 对于维吉尼亚密码体制来说，我们可以通过统计分析的方法对其密文进行分析，从而获取明文信息。基于维吉尼亚密码体制的唯密文攻击的破解主要包含三个步骤： 确定密钥长度，常用的方法包括卡西斯基测试法和重合指数法，本文将采用后者进行分析 确定密钥，常用的方法是拟重合指数法 根据密文和密钥恢复明文 确定密钥长度&emsp;&emsp;本文采用重合指数法猜解密钥长度，关于重合指数法的具体解释可以参照《现代密码学教程》或者维基百科，本文主要讲解猜解密钥长度的实现过程。 1234567891011121314151617181920212223def guess_len_key(crypt): """ guess_len_key函数的主要作用是通过密文猜解密钥长度 :param crypt: 密文 :return: 密钥长度以及划为的子串 """ l = 1 d = &#123;&#125; while True: print("****************************假设密钥长度 为%s***********************************" % l) sum_index = 0.0 for i in range(len(crypt)): n = i % l if n not in d: d[n] = '' d[n] += crypt[i] sum_index = sum(coincidence_index(d[j]) for j in range(l)) / l if sum_index &gt;= 0.06 and sum_index &lt;= 0.07: break else: l += 1 d = &#123;&#125; return l,d 该算法的主要思想是将密文划分为l个子串，子串存放在字典d中。分别计算l个子串的重合指数，然后计算l个重合指数的平均数，如果该平均数位于[0.06,0.07]这个区间内，则说明密钥长度为l，返回密钥长度以及划分的l个子串；如果得到的平均数不在[0.06,0.07]这个区间内，则l自增，d初始化，进行下一轮猜解。 确定密钥&emsp;&emsp;确定密钥长度大致过程是：利用之前得到的l个子串，对每个子串都进行移位操作。假设现在对第i个子串进行移位操作（子串的每个字符移动相同的位数，最坏情况下对同一个子串需要进行26次移位操作），移动的位数为k,（k在[0-25]区间内，也就对应了[a-z]）。每进行一次移位操作，就对该子串计算一次拟重合指数，如果该拟重合指数位于[0.06,0.07]这个区间内，则说明此时移动的位数对应的s列表中的字符即为该子串的密钥；否则，继续进行下一次移位操作。 1234567891011121314151617181920212223def crack_key(): """ cracker函数的主要作用是破解密钥 :return: 返回密钥 """ with open("crypt.txt","r") as f: crypt = f.read() len_key,d = guess_len_key(crypt) key = '' print("\n-------------------------------------") print("| 经计算可知，密钥长度为%s |" % len_key) print("-------------------------------------\n") for i in range(len_key): substring = d[i] print("当前字串为：",d[i]) for n in range(26): dex = quasi_index(substring, n) print("假设子串移动&#123;&#125;,拟重合指数为&#123;:.4f&#125;".format(s[n],dex)) if dex &gt;= 0.06 and dex &lt;= 0.07: key += s[n] break print("******************************破解的最终密钥为%s*********************************" % key) 恢复明文&emsp;&emsp;恢复明文的过程与解密过程类似，这里不在详述。 系统运行演示加密 解密 猜解密钥长度 猜解密钥]]></content>
      <categories>
        <category>密码学</category>
      </categories>
      <tags>
        <tag>密码学</tag>
        <tag>维吉尼亚</tag>
        <tag>重合指数</tag>
        <tag>唯密文</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[raspberry入门配置]]></title>
    <url>%2Fraspberry%E5%85%A5%E9%97%A8%E9%85%8D%E7%BD%AE.html%2F</url>
    <content type="text"><![CDATA[摘要 最近准备在树莓派上搭建一个智能家居系统，更新系统的过程中不知道什么原因导致系统崩了，我的心顿时凉了半截。查阅了很多资料，没找到解决方法，只能重装系统（基于stretch版本）了。虽然之前的系统也是自己一步步配置的，但是这次重新配置的过程中还是遇到了很多问题，在这里记录一下，希望能给小伙伴们一些启发。 文章概览 重新安装系统 连接树莓派 安装远程桌面服务 更新软件源 raspi-config 配置无线网络 配置静态IP 配置内网映射 安装zsh python环境搭建 重新安装系统&emsp;&emsp;重新安装系统的过程我们需要用到：系统镜像、DiskGenius、Win32DiskImager，DiskGenius的作用是格式化TF卡，Win32DiskImager的作用是将系统镜像写入TF卡，具体的操作过程可以参考这篇博客。 连接树莓派&emsp;&emsp;对于如何连接树莓派，我在之前的博客中详细的讨论过，这里不再赘述，需要提醒大家的是只需要使用某一种方法连接树莓派即可。 安装远程连接服务&emsp;&emsp;ssh连接是通过命令行对树莓派进行远程操作，而远程桌面是直接通过树莓派的GUI界面进行操作，操作简单，交互性好。xrdp是一个开源的远程桌面服务器，支持windows远程桌面连接，但是需要使用tightvncserver作为其基础服务，具体安装操作如下所示。 123sudo apt-get update #更新sudo apt-get install xrdpsudo apt-get install tightvncserver 安装好以上两个服务以后。可以使用windows自带的远程连接工具连接到树莓派。 更新软件源&emsp;&emsp;在更新软件源的时候，大家注意查看自己的系统版本（推荐大家安装最新的系统版本），具体操作参见清华大学开源软件镜像站。 raspi-config&emsp;&emsp;通过远程桌面连接到树莓派之后，系统会提示进行一些初始化配置，包括拓展内存、设置时区、语言等等，具体操作参见这篇博客。 配置无线网络&emsp;&emsp;配置无线网络有两种方式，一个是在图形化界面直接选择连接的ssid，输入密码即可，系统会保存该无线网络的相关信息到/etc/wpa_supplicant/wpa_supplicant.conf文件；另外一种方式是直接修改该配置文件，将无线网络配置信息添加到该文件中。 123456789country=CNctrl_interface=DIR=/var/run/wpa_supplicant GROUP=netdevupdate_config=1network=&#123; ssid="***" psk="***" priority=1 &#125; 配置静态IP&emsp;&emsp;由于每次树莓派连接路由器的时候，路由器会分配不同的IP地址，所以当我们连接树莓派的时候每次都要通过路由器查看树莓派的IP地址，这样比较麻烦，所以，我们需要给路由器指定静态的IP。修改/etc/dhcpcd.conf文件（一定要注意不是修改/etc/network/interfaces文件），在后面添加以下内容： 1234567891011interface eth0 #有线 static ip_address=192.168.0.10/24static routers=192.168.0.1static domain_name_servers=192.168.0.1 interface wlan0 #无线连接 static ip_address=192.168.0.200/24static routers=192.168.0.1static domain_name_servers=192.168.0.1 配置内网映射&emsp;&emsp;如果想要从外网直接访问树莓派，那我们需要将树莓派的内网IP映射到公网当中，这里我们使用的映射工具是花生壳，具体操作参见官方教程。 安装zsh&emsp;&emsp;树莓派基于linux操作系统，其终端shell默认的是bash，而zsh是比bash更加强大的shell，而且更加美观，具体配置参见这篇博客。 python环境搭建&emsp;&emsp;树莓派中内置了两个版本的python，python2.7和python3.5，系统默认版本为python2.7。在进行系统环境配置和相关依赖安装的过程中，一律使用系统默认版本即python2.7（如果切换至python3.5.会出现各种各样的问题）。在程序开发过程中如果需要使用python3.5，可以切换python环境，具体操作参见这篇博客。]]></content>
      <categories>
        <category>树莓派</category>
      </categories>
      <tags>
        <tag>python</tag>
        <tag>树莓派</tag>
        <tag>内网穿透</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[如何连接树莓派]]></title>
    <url>%2F%E5%A6%82%E4%BD%95%E8%BF%9E%E6%8E%A5%E6%A0%91%E8%8E%93%E6%B4%BE.html%2F</url>
    <content type="text"><![CDATA[摘要 早就听说了树莓派的大名，什么智能家居，智能机器人，无人机等等它都不在话下。我最近刚刚入手了一款树莓派3B+，想利用它来开发一个智能家居的控制系统。由于从来没有接触过相关的硬件，在配置过程中确实遇到了不少坑，在这里简单的记录一下，希望能给读者一些启发。本文主要讲解如何连接树莓派的问题。 文章概览 基本配置 系统安装 连接树莓派 有路由器和网线的情况下 有网线没有路由器的情况下 没有路由器没有网线的情况下 基本配置&emsp;&emsp;想要玩树莓派，仅仅买一块主板是不够的，基本的配置包括：一块主板（大概225RMB）、一个读卡器（大概10RMB）、一根电源线（大概8RMB）、一张大于8G的内存卡（我买的是32G的，38RMB）、散热片（大概5RMB）和一个保护外壳（大概20RMB），推荐大家主板和配件分开买，这样比较划算。 系统安装&emsp;&emsp;树莓派的安装过程比较简单，首先我们需要下载准备安装的系统镜像，在树莓派官网可以下载到相关的镜像文件，树莓派支持的系统很多，包括ubuntu、kail、win10等等。我选择的系统是树莓派官方推荐的RSAPBIAN，基于Debian，稳定兼容性好。然后我们需要下载一款工具：win32diskimager，我们需要利用这款工具将树莓派的系统镜像写入内存卡。 &emsp;&emsp;准备工作完成后，我们通过读卡器将内存卡接入电脑，在利用win32diskmager工具将系统镜像写入内存卡（这个过程很简单，这里不再详细解释）。镜像写入完成之后，我们的系统就成功的安装到内存卡中。但是，有一个问题需要我们注意，镜像写入完成后系统会弹出一个对话框，大概的意思是：无法识别内存卡中的数据，是否要将内存卡格式化，这个时候大家一定选择否或者直接关闭对话框。因为系统写入完成之后，我们的主机只能读出内存卡中系统的boot分区（大概只有40多MB），内存卡中的其他分区我们的系统识别不出来，所以才会弹出这个对话框，一旦我们选择格式化，刚才安装的系统会被删除。如果有小伙伴不小心选择了格式化，那就只有重新安装系统了，大家可以参考这篇博客。 &emsp;&emsp;还有一个需要注意的问题是，树莓派默认没有开启ssh服务，所以之后我们如果需要通过ssh连接树莓派时，我们需要在刚刚写入系统的内存卡的boot文件夹下，建立一个文件名为ssh的空文件（无后缀名），这样在之后的操作中我们就能通过ssh连接树莓派。 连接树莓派&emsp;&emsp;连接树莓派最简单的方法就是通过HDMI数据线连接显示屏，这样我们可以直接通过显示屏对树莓派进行操作。那如果我们没有显示屏，我们该如何连接树莓派呢？ 有路由器和网线的情况下&emsp;&emsp;这种情况下连接树莓派也比较简单，我们只需要将网线的一端接入路由器的lan接口，一端接入树莓派的网络接口（树莓派会自动获取IP地址），同时我们的主机也连接在路由器所建立的局域网内，此时我们可以通过路由器查看树莓派的IP地址。获取树莓派IP地址之后，我们通过主机中的xshell或者putty等远程连接工具就可以连接到树莓派。 有网线没有路由器的情况下&emsp;&emsp;没有了路由器，树莓派就不能获取到IP地址，这种情况稍微复杂一点。解决办法是：将网线的两端连接树莓派和主机，然后我们打开主机的网络和共享中心 点击我们已经连接的网络，查看其属性 点击共享按钮，勾选允许其他网络用户通过此计算机的Internet连接来连接（N） 接下来我们回到网络和共享中心，点击未识别的网络那一栏对于的以太网选项，查看其详细信息，可以看到其IP地址，这里的IP为192.168.137.1 然后我们拔掉网线，关掉树莓派，拔出内存卡，将其通过读卡器连接到主机上。我们进入内存卡的boot文件夹，修改comline.txt文件，将 ip = 192.168.137.100 这句话添加到开头。完成上述步骤之后，我们将内存卡插入树莓派，用网线重新连接树莓派和主机，启动树莓派电源。我们通过远程连接工具连接树莓派，此时树莓派的IP地址为刚才设置的192.168.137.100，不出意外的话，我们也能连接上树莓派。 没有路由器没有网线的情况下&emsp;&emsp;要使我们的主机能够远程连接到树莓派上，我们必须满足一个条件：主机和树莓派位于同一个局域网。没有路由器和网线的情况下，我们可以通过手机热点建立一个局域网环境，让树莓派和主机同时连接手机热点。现在需要解决的问题是如何才能让树莓派连接上手机的热点。开机状态下，树莓派的无线模块一直处于工作状态，我们需要将无线热点的相关配置文件写入系统。同样，我们通过读卡器读取内存卡boot文件夹，在文件夹下新建文件名为wpa_supplicant.conf的文件，在文件内写入手机热点的配置信息并保存。 然后重新插入内存卡，开启树莓派，在手机上查看树莓派的IP地址，用远程连接工具进行连接。]]></content>
      <categories>
        <category>树莓派</category>
      </categories>
      <tags>
        <tag>树莓派</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[解释型语言python]]></title>
    <url>%2F%E8%A7%A3%E9%87%8A%E5%9E%8B%E8%AF%AD%E8%A8%80python.html%2F</url>
    <content type="text"><![CDATA[摘要 计算机不能直接理解高级语言，只能直接理解机器语言，所以必须要把高级语言翻译成机器语言，计算机才能执行高级语言编写的程序。由于翻译方式的不同，习惯上我们大致把高级语言分为两类，即编译型语言和解释型语言。对于这两种类型的编程语言，很多人在理解层面上存在盲点，本文将对这两种类型的编程语言进行探讨，帮助读者更好的理解这一问题。 文章概览 编译型语言和解释型语言 基本解释 优缺点 python python解释器 python代码执行过程 编译型语言和解释型语言基本解释&emsp;&emsp;对于编译型语言，我们以C语言为例，C语言在执行过程中，先要将源程序编译为目标文件（机器代码），该目标文件是与平台相关的，也就是说ARM生成的目标文件，不能被用于MIPS的CPU，也不能用于x86的CPU。目标文件经过连接操作就可以生成可执行文件，以后我们想再次运行这段代码时，不必进行编译操作，只需要直接执行生成的可执行文件即可。 &emsp;&emsp;对于解释型语言呢，我们不需要执行编译过程，程序在执行时直接由解释器逐句地对程序进行解释，转换为机器可以执行的代码。但是对于有些解释型语言来说，也需要进行编译操作，比如Java。Java程序在执行过程中先要将源代码编译成字节码文件，然后再由解释器对字节码文件逐句进行解释，所以说Java是一种先编译后解释的语言。（注：Java为了实现跨平台的特性，专门在从高级语言代码转换至机器码过程的中间加入了一层中间层JVM（java虚拟机），Java首先依赖编译器将代码（.java）编译成JVM能识别的字节码文件（.class），然后由JVM解释并执行该字节码，也可结合JIT（just-in-time compilation即时编译）技术，将解释生成的机器码转换为更高效的本地机器码，且该机器码可被缓存，来提高重复执行的效率。) &emsp;&emsp;常见的编译型语言包括：C/C++、Pascal等，常见的编译型语言包括：Java、JavaScript、VBScript、Perl、Ruby、MATLAB 等。 优缺点 编译型语言可以做到一次编译，多次运行，执行效率比较高；而解释型语言在每次执行时都需要解释器进行解释，执行效率较低（但是我们也不能一概而论，一些解释型语言也可以通过解释器的优化来在对程序做出翻译时对整个程序做出优化，从而在效率上超过编译型语言）。 编译型语言的执行依赖于平台，生成的可执行文件不能运行在其他平台，需重新编译，跨平台的性较差；而解释型语言的执行依赖于解释器，各个平台都有相应的解释器，解释器会将程序解释成基于当前机器指令集的机器码并执行，所以解释型语言可以很好的移植到其他平台，具有很好的跨平台性。 编译型语言，在编译阶段即可发现常见的语法或者链接等错误，此机制可在运行前帮助程序员排查出可能潜在的语法、语义和类型转换错误，编译型语言一般都有明确的变量类型检测，也被称作强类型语言，即编译型语言至少能确保所生成的可执行文件肯定是可运行的，至于执行的逻辑不对则属于程序员业务逻辑错误范畴了。而对于解释型语言，代码中的错误必须直到运行阶段方可发现，由此造成的困惑是：往往一段程序看不出问题但却在运行阶段错误连连且需要一个个排查：变量拼写错误、方法不存在等。但也正是基于解释是在运行期执行转化的特性，一般的解释型语言通常都有自己的shell，可以在不确定某些执行结果时立即“动手执行”试一下，这就比每次都需要编译后才能运行并看到结果省去不少时间。 python&emsp;&emsp;通过上面对编译型语言和解释型语言的分析，我们可以得出结论，python是属于解释型语言的一种。python类似于Java，为了效率上的考虑，也提供了编译方式，编译后生成的也是字节码的文件形式，并由Python的的VM（虚拟机）的去执行。不同点在于，Python的编译并非强制执行的操作，确切来说Python的编译是自动的，通常发生在对某个模块（module）的调用过程中，编译成字节码的可以节省加载模块的时间，以此达到提高效率的目的。可见，某些先进的高级语言在对编译和解释方面的拿捏舍去，都采取了一种：两手抓，两手都要硬的态度。 python解释器&emsp;&emsp;由于整个Python语言从规范到解释器都是开源的，所以理论上，只要水平够高，任何人都可以编写Python解释器来执行Python代码（当然难度很大）。事实上，确实存在多种Python解释器 CPython: 这个解释器是用C语言开发的，所以叫CPython。在命令行下运行python就是启动CPython解释器,它是使用最广的Python解释器. IPython是基于CPython之上的一个交互式解释器，也就是说，IPython只是在交互方式上有所增强，但是执行Python代码的功能和CPython是完全一样的。CPython用&gt;&gt;&gt;作为提示符，而IPython用In [序号]:作为提示符。 绝大部分Python代码都可以在PyPy下运行，但是PyPy和CPython有一些是不同的，这就导致相同的Python代码在两种解释器下执行可能会有不同的结果。PyPy是另一个Python解释器，它的目标是执行速度。PyPy采用JIT技术，对Python代码进行动态编译（注意不是解释），所以可以显著提高Python代码的执行速度。 Jython是运行在Java平台上的Python解释器，可以直接把Python代码编译成Java字节码执行。 python代码执行过程&emsp;&emsp;参考这篇博客和这篇博客]]></content>
      <categories>
        <category>python</category>
      </categories>
      <tags>
        <tag>python</tag>
        <tag>解释型语言</tag>
        <tag>编译式语言</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[决策树算法详解与python实现：ID3和CART]]></title>
    <url>%2F%E5%86%B3%E7%AD%96%E6%A0%91.html%2F</url>
    <content type="text"><![CDATA[摘要 决策树是一种基本的分类与回归方法，本文主要讨论用于分类的决策树，决策树模型呈树形结构，在分类问题中，表示基于特征对实例进行分类的过程，它可以认为是if-then规则的集合，也可以认为是定义在特征空间与类空间上的条件概率分布，其主要优点是模型具有可读性，分类速度快。学习时，利用训练数据，根据损失函数最小化原则建立决策树模型。预测时，对新的数据，利用决策树模型进行分类。决策树学习通常包括三个步骤：特征选择、决策树的生成以及决策树的修剪。本文将主要讲解ID3和CART算法的原理和实现细节。 文章概览 ID3算法 特征选择 决策树的生成 ID3算法的缺陷 C4.5算法对ID3算法的改进 CART算法 特征选择 决策树生成 剪枝 ID3算法&emsp;&emsp;ID3算法是由澳大利亚计算机科学家Ross Quinlan提出的，它是构建决策树中一种非常重要的算法。在设计算法的过程中，它首次采用了信息增益准则来进行特征选择，这很大程度上推动了决策树算法的发展。 特征选择&emsp;&emsp;我们可以将决策树看作是if- then规则的集合，使用决策树模型进行预测的过程就相当于对if - then规则进行判断，那我们可以想到如果if -then规则越多，也就是决策树越复杂，那么预测所需要的时间越长，所以为了不断优化决策树的决策过程，我们需要合理的构建决策树，那么如何来选择if - then的决策规则至关重要。 &emsp;&emsp;在ID3算法中，我们通过信息增益作为决策规则。信息增益 = 信息熵 - 条件熵，信息熵代表随机变量的不确定度，条件熵代表在一定条件下，随机变量的复杂度，所以信息增益表示在一定条件下信息复杂度减少的程度。信息增益越大说明该决策规则的区分度越高，在构建决策树时，我们选取信息增益最大的特征作为决策规则。 123456789101112131415161718def emp_entropy(y_data): ''' emp_entropy函数的主要功能是计算数据集的经验熵 :param y_data: 数据集的类别 :return: 返回数据集的经验熵 ''' count = &#123;&#125; emp = 0.0 m = len(y_data) for y in y_data: if y in count: count[y] += 1 else: count[y] = 1 for i in count.keys(): info = (1.0 * count[i] / m) emp = emp + info * math.log(info,2) return emp emp_entropy函数的主要功能是计算数据集合的经验熵，经验熵的计算公式可以参考《统计学习方法》，同样，下面涉及到条件熵、信息增益的计算公式也可参考本书。代码中字典count的主要作用是统计数据集中不同类别出现的次数，emp即是信息增益。 1234567891011121314151617181920212223def emp_cond_entropy(x_data,y_data,feature): ''' emp_cond_entropy函数的主要作用是计算经验条件熵 :param x_data: 数据集 :param y_data: 数据集类别 :param feature: 数据集特征特征 :return: 数据集的经验条件熵 ''' count_y = &#123;&#125; emp_cond = 0.0 m = len(y_data) fea = x_data[:,feature] for i in range(len(fea)): if fea[i] in count_y: count_y[fea[i]].append(y_data[i]) else: count_y.setdefault(fea[i]) count_y[fea[i]] = [] count_y[fea[i]].append(y_data[i]) for e in count_y.keys(): l = len(count_y[e]) emp_cond = emp_cond + (1.0 * l / m) * emp_entropy(count_y[e]) return emp_cond emp_cond_entropy函数的主要作用是计算经验条件熵，fenture表示数据的某一维特征，对于离散性特征（ID3算法不能处理连续型特征）来讲，特征的取值有多个，这里的count_y就是来统计该特征中不同取值的数据分布情况，列表fea表示的即是该数据集中该特征对应的值，emp_cond表示的是将该特征作为决策规则时的条件熵。 123456789101112131415def choose_feature(x_data,y_data): ''' choose_feature函数的主要作用是从数据集中选择信息增益最大的特征 :param x_data: 数据集 :param y_data: 数据集类别 :return: 信息增益最大的特征 ''' n = np.size(x_data,1) count = [] emp = emp_entropy(y_data) for i in range(n): emp_cond = emp_cond_entropy(x_data,y_data,i) count.append(emp - emp_cond) feature = count.index(min(count)) return feature choose_feature函数的主要作用是从数据集中选择信息增益最大的特征，算法的思路就是对数据集进行遍历，计算每一个特征的信息增益，返回信息增益最大的特征。（由于在计算经验熵的过程中没有添加负号，所以我这里取的是负数的最小值，也就是正数的最大值） 决策树的生成&emsp;&emsp;特征树的生成过程其实是一个递归过程，我们首先选择一个特征，作为根结点，根据根结点的不同取值，将数据集分为几个不同的部分，同时将该特征从数据集中删除。然后再对这几个不同的部分进行同样的操作，直到数据集类别相同或者没有特征为止。 1234567891011121314151617181920212223def create_tree(x_data,y_data,feature_list_data): ''' create_tree函数的主要作用是构建决策树 :param x_data: :param y_data: :param feature_list: :return: 返回决策树 ''' feature_list = feature_list_data[:] if is_all_same(y_data): return y_data[0] if len(x_data) == 0: return node_classfy(y_data) feature = choose_feature(x_data,y_data) node_name = feature_list[feature] tree = &#123;node_name:&#123;&#125;&#125; del feature_list[feature] count_x,count_y = feature_split(x_data,y_data,feature) for i in count_x.keys(): fealist = feature_list[:] count_x_del = del_feature(count_x[i],feature) tree[node_name][i] = create_tree(count_x_del,count_y[i],fealist) return tree feature_list的作用是复制feature_list_data，因为后面要进行删除操作，我们要保证删除操作只能影响函数内部变量，不能对函数的实参造成影响。我们生成的决策树保存在tree字典中，每执行一次递归操作，相当于将当前特征作为一个字典的key，递归操作返回的即是一个子树（即字典）。 ID3算法的缺陷&emsp;&emsp;通过对ID3算法进行分析，我们可以知道，ID3算法主要存在以下缺陷： ID3没有考虑连续型特征，数据集的特征必须是离散型特征 ID3算法采用信息增益大的特征优先建立决策树的结点，但是再计算信息增益的过程中我们发现，在相同条件下，取值比较多的特征比取值少的特征信息增益大 ID3没有对缺失值情况进行处理，现实任务中常会遇到不完整的样本，即样本的某些属性值缺失。 没有考虑过拟合问题 C4.5算法对ID3算法的改进&emsp;&emsp;C4.5算法是对ID3算法存在的缺陷进行改进的一种算法，它通过将连续特征离散化来解决ID3算法不能处理离散型数据的问题（这个会在后面的CART算法中讲到）；通过引入信息增益比来解决信息增益的缺陷；通过增加剪枝操作来解决过拟合的问题。 CART算法&emsp;&emsp;CART算法是一种应用广泛的决策树算法，它的基本流程与C4.5算法类似，它既可以应用于回归任务，也可以应用于分类任务（这里主要讲解分类树），需要注意的是CART算法生成的决策树是二叉树，而ID3和C4.5算法生成的决策树不一定是二叉树。 特征选择&emsp;&emsp;CART算法的决策规则由基尼指数决定，选择基尼指数最小的特征及其切分点作为最优特征和最优切分点。 1234567891011121314151617def Gini_index(y_data): ''' Gini_index函数的主要作用是计算数据集的基尼指数 :param y_data: 数据集类别 :return: 返回基尼指数 ''' m = len(y_data) count = &#123;&#125; num = 0.0 for i in y_data: if i in count: count[i] += 1 else: count[i] = 1 for item in count.keys(): num = num + pow(1.0 * count[item] / m,2) return (1.0-num) 在前面提到过ID3算法只能处理离散型特征，而CART算法既能处理离散型特征，又能处理连续型特征。CART算法处理连续型特征的方法与C4.5算法类似，都是将连续特征离散化，即将连续特征的所有取值进行排序，然后计算相邻取值的平均值作为切分点，在以此计算基尼指数。 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556ef Gini_D_A(x_data,y_data,feature): ''' Gini_D_A函数的主要作用是计算某一离散特征各个取值的基尼指数，选取最优切分点 :param x_data: 数据集合 :param y_data: 数据集类别 :param feature: 特征 :return: 该特征的最优切分点 ''' Gini_data = list(x_data[:,feature]) y_data = list(y_data[:]) m = len(Gini_data) Gini = &#123;&#125; classfy_data = &#123;&#125; for e in range(m): if Gini_data[e] not in classfy_data: classfy_data[Gini_data[e]] = [] classfy_data[Gini_data[e]].append(y_data[e]) for item in classfy_data.keys(): l1 = len(classfy_data[item]) r = y_data[:] for i in classfy_data[item]: r.remove(i) l2 = len(r) num = 1.0 * l1 / m * Gini_index(classfy_data[item]) + 1.0 * l2 / m * Gini_index(r) Gini[item] = num sor = sorted(Gini.items(), key=operator.itemgetter(1)) return sor[0]def Gini_continuous(x_data,y_data,feature): ''' Gini_continous函数的主要作用是计算某一连续特征各个取值的基尼指数，选取最优切分点 :param x_data: 数据集合 :param y_data: 数据集类别 :param feature: 特征 :return: 该特征的最优切分点 ''' Gini_data = list(x_data[:,feature]) m = len(Gini_data) y_data = list(y_data[:]) sort_data = sorted(Gini_data) Gini = &#123;&#125; split_point = [] for i in range(m-1): num = (sort_data[i] + sort_data[i+1]) / 2.0 split_point.append(num) for e in split_point: count_y = &#123;0:[],1:[]&#125; for k in range(m): if Gini_data[k] &lt;= e: count_y[0].append(y_data[k]) else: count_y[1].append(y_data[k]) cal = 1.0 * len(count_y[0]) / m * Gini_index(count_y[0]) + 1.0 * len(count_y[1]) / m * Gini_index(count_y[1]) Gini[e] = cal sor = sorted(Gini.items(), key=operator.itemgetter(1)) return sor[0] 当数据集中同时含有离散型变量和连续型变量时，进行特征选择就稍微有些复杂了，以下便是特征选择的代码,这里需要注意的是对不同类型特征的标识和对计算基尼指数时返回值的统一处理。dis_or_con是一个列表，用来标识特征是连续型还是离散型，0表示离散，1表示连续。 12345678910111213141516171819def choose_feature(x_data,y_data,dis_or_con): ''' choose_feature函数的主要作用是从各个特征的各个切分点中选择基尼指数最小的切分点 :param x_data: 数据集合 :param y_data: 数据类别 :return: 切分点 ''' w = np.size(x_data,axis=1) count = [] count_label = &#123;&#125; for i in range(w): if dis_or_con[i] == 0: a = Gini_D_A(x_data,y_data,i) else: a = Gini_continuous(x_data,y_data,i) count.append(a[1]) count_label[i] = a id = count.index(min(count)) return id,count_label[id][0] 决策树的生成&emsp;&emsp;决策树的生成大致与ID3算法类似 1234567891011121314151617181920def create_tree(x_data,y_data,dis_or_con_data,feature_list_data): feature_list = feature_list_data[:] dis_or_con = dis_or_con_data[:] if dis_or_con == []: return most_y_data(y_data) if is_all_same(y_data): return y_data[0] w,f = choose_feature(x_data,y_data,dis_or_con) count_x, count_y = feature_split(x_data,y_data,w,f,dis_or_con[w]) node_name = feature_list[w] tree = &#123;(node_name,f):&#123;&#125;&#125; del feature_list[w] del dis_or_con[w] for i in count_x.keys(): fealist = feature_list[:] dis_con = dis_or_con[:] count_x_del = del_feature(count_x[i], w) tree[(node_name,f)][i] = create_tree(count_x_del, count_y[i], dis_con,fealist) return tree 剪枝&emsp;&emsp;本文暂未实现剪枝算法，有待后续补充]]></content>
      <categories>
        <category>机器学习算法</category>
      </categories>
      <tags>
        <tag>机器学习</tag>
        <tag>决策树</tag>
        <tag>ID3</tag>
        <tag>CART</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[验证码识别：找回四六级准考证号]]></title>
    <url>%2Fverify-code.html%2F</url>
    <content type="text"><![CDATA[摘要 一晃时间过的真快，距离上次更新博客已经将近10天了，这十天来也没闲着，回家终于把杀千刀的科目三过了，再也不用看到教练那张凶神恶煞的脸。前段时间四六级考试成绩公布了，小伙伴们是不是都第一时间忙着去查自己的成绩，相信有很多小伙伴跟我一样苦逼，幸幸苦苦复习了好长时间，查成绩的时候却忘了自己的准考证号（温馨提示：以后考试之前一定要记得把准考证拍一张存起来）。在网上试过无数种找回办法后，我彻底绝望了。既然别人不靠谱，咱就靠自己，经过两天的努力之后，终于成功的找回了准考证号。这篇博客主要来介绍解决这个问题的一些方法和思路。 文章概览 基本思路 训练模型 获取训练数据 处理数据 生成模型 查询操作 发送请求 使用代理 多线程 使用教程 基本思路&emsp;&emsp;对于查询四六级成绩来说，官方的查询入口有学信网和中国教育考试网，查询成绩需要提交的数据包括准考证号、姓名和验证码。要想查询到成绩，最简单的办法就是手工枚举准考证号，一个一个的尝试。我们知道四六级准考证的组成如下所示（第10位表示类别，四级是1，六级是2）： 也就是说对于在同在一个考点的人来说前十位都是一致的（四级和六级不同），后面五位分别表示考场号和座位号（座位号从01到30），在我们忘记了考场号和座位号的情况下，我们至少要手工枚举几千次才有可能查询到成绩，这个工作难度可想而知。那如果我们不采用手工的方式进行枚举，而采用程序自动进行枚举呢？通过程序枚举准考证号不是什么问题，但是查询参数中包含验证码，现在需要解决地就是如何识别验证码。对于验证码地识别问题，我们可以利用机器学习的相关算法，建立识别模型，再利用识别模型来进行识别验证码。对于学信网和中国教育考试网两个网站，它们采用的验证码不同，学信网的验证码比较复杂，包含汉字等特殊字符，识别难度大，而中国教育考试网的验证码相对来说比较常规，识别难度相对小一点，本文的查询操作都是基于后者而言的。&emsp;&emsp;那么我们解决问题地大致思路就是：首先我们要获取大量的验证码数据，然后选择算法训练识别验证码的模型，最后通过重复识别查询页面的验证码，提交查询数据，分析响应数据来获得最终的结果。 训练模型 获取训练数据&emsp;&emsp;通过抓取请求相应过程中的数据包，我们可以得到获取验证码的地址。 其中ik表示准考证号，我们可以随便填一个，t表示时间戳（这个可以不用管），我们可以不断地向这个地址发送请求，服务器的响应结果即为验证码的地址，我们再向获取到的验证码的地址发送请求，就可以得到验证码。 具体代码如下所示（该项目的所有代码都可以在我的Github中找到）： 12345678910# 获取验证码def save_image_to_file(): myid = "123456789110211" new_id = myid.format(id=myid) img_api_url = image_api.format(id=new_id) img_api_resp = requests.get(img_api_url, headers=img_api_headers,timeout=10) img_url, filename = get_image_url_and_filename(img_api_resp.text) r = requests.get(img_url) with open("images/raw_picture/" + filename, "wb+") as f: f.write(r.content) 处理数据&emsp;&emsp;获取到一定数量的验证码图片后（大概需要100多张，收集的图片越多越好，之后我们会讲到一种快速收集和标注验证码的方法），接下来我们需要对获取到的验证码进行相应的处理。因为对于验证码的识别，我们一般采取监督学习的算法训练模型，所以首先要对获取到的验证码进行标注，即将验证码图片的文件名改为验证码对应的数字和字母组合，这一步必须要人工进行操作。然后，为了提高验证码识别的准确率，训练更好的识别模型，我们需要对验证码图片进行相应的处理，如灰度处理、二值化、降噪。经过这些手段处理后的验证码更能体现出图片本身的特征，同时也减小了训练模型时的计算量，具体代码如下所示。 1234567891011121314# 灰度处理，二值化（降噪部分的代码去掉了，效果不是太理想）def img_denoise(img, threshold): def init_table(threshold=threshold): table = [] for i in range(256): if i &lt; threshold: table.append(0) else: table.append(1) return table img = img.convert("L").point(init_table(), '1') return img &emsp;&emsp;下面我们要对验证码进行分割，因为在识别的时候，我们是识别单个的数字或字母，所以我们要将验证码进行切分，提取出每个字符对应的区域，切割后的每张图片大小一致。 1234567891011121314# 图片分割,参数img_split_start指定起始位置，参数img_split_width指定切割图片宽度def img_split(img,img_split_start,img_split_width): start = img_split_start width = img_split_width top = 0 height = img.size[1] img_list = [] for i in range(4): new_start = start + width * i box = (new_start, top, new_start + width, height) piece = img.crop(box) #piece.save("%s.jpg" % i) img_list.append(piece) return img_list &emsp;&emsp;图片切割完成后，数据处理的最后一步是将切割后的图片转化为numpy array的形式。 123456# 将Image对象转换为array_listdef img_list_to_array_list(img_list): array_list = [] for img in img_list: array_list.append(array(img).flatten()) return array_list 以上这些操作大家可以在我的GitHub的项目文件中通过preprocessing()、make_train_data()和img_to_array()三个函数实现。 生成模型&emsp;&emsp;生成模型主要用到的就是sklearn机器学习库中相关的算法，验证码识别属于分类任务，对于分类任务我们可以采用K近邻、支持向量机、决策树和神经网络等算法，这里我们采用的是支持向量机。 123456789# 训练模型def svm_model(x_data,y_data): SVM = svm.SVC() x_train,x_test,y_train,y_test = train_test_split(x_data,y_data,random_state=14) SVM.fit(x_train,y_train) y_predict = SVM.predict(x_test) average_accuracy = np.mean(y_test==y_predict)*100 print("准确率为：&#123;0:.1f&#125;%".format(average_accuracy)) pickle.dump(SVM, open("model.pkl", "wb+")) 模型训练好之后，将模型对象存储在model.pkl文件中，需要识别验证码时，只需要读取model.pkl文件即可获得识别模型，不需要再次训练。 查询操作发送请求&emsp;&emsp;模型训练好之后，我们就可以进行查询操作了。这一阶段的大致思路是，先获取查询页面的验证码，通过识别模型进行识别，然后再向服务器提交请求参数，包括枚举的准考证号、姓名和验证码。如果服务器返回验证码错误，则重复以上操作。如果服务器返回查询结果为空则说明验证码正确，但是准考证号和姓名不一致，此时可以枚举下一个准考证号，重复操作一直到获得正确结果为止。 &emsp;&emsp;由于一开始我们训练模型时使用的训练数据量很小，所以该识别模型识别的准确率比较低，那么如何提高模型识别的准确率呢。最好的办法就是增大训练数据的数量，训练新的模型。这里提供一个更快更方便获取训练数据的方法，在发送请求的代码中，我们加入两行代码（倒数第三行和倒数第二行），该代码的作用时将识别正确的验证码加入到训练数据的文件夹中，并且会自动进行标注，可以通过该方式一边查询，一边收集大量的训练数据。我的项目中，一开始手工标注的验证码有200张，训练模型后采用这种方式自动收集了1600多张验证码，然后利用所有的训练数据重新建立模型，识别的准确率提高了30%。（但是这样的做法存在一个过拟合的问腿，训练模型对于类似于一开始200张验证码的图片的识别准确率比较高，而对于其他类型的图片识别的准确率比较低。不过这个问题对于我们找回准考证号影响不大，提高准确率最好的就是一开始手工标注更多的验证码） 123456789101112131415161718192021222324252627282930313233343536# 发送请求def send_query_until_true(num): # 生成准考证号 global proxy new_id = myid.format(id=num) # 获取验证码图片地址 img_api_url = image_api.format(id=new_id) while True: try: img_api_resp = requests.get(img_api_url, headers=img_api_headers,timeout=10,proxies=proxy) img_url, filename = get_image_url_and_filename(img_api_resp.text) # 获取验证码图片并猜测 img_resp = requests.get(img_url, timeout=10, proxies=proxy) if img_resp.status_code == 200: images = Image.open(BytesIO(img_resp.content)) code = img_verify_code(images) else: code = "xxxx" except Exception: print("重新获取代理") p = str(get_proxy()) proxy = &#123;'http': 'http://' + p, 'https': 'http://' + p&#125; else: break # CET4成绩查询选项 # data = &#123;"data": "CET4_181_DANGCI,&#123;id&#125;,&#123;name&#125;".format(id=new_id, name=name),"v": code&#125; # CET6成绩查询选项 data = &#123;"data": "CET6_181_DANGCI,&#123;id&#125;,&#123;name&#125;".format(id=new_id, name=name),"v": code&#125; query_resp = requests.post(query_api, data=data, headers=query_api_headers) query_text = query_resp.text log_info(query_text.split("'")[3],new_id) if "验证码错误" in query_text: query_text = send_query_until_true(num) # elif "您查询的结果为空" in query_text: # images.save("images/save_picture/" + code + ".png") return query_text 使用代理&emsp;&emsp;在上面那段代码中，我们在请求过程中使用了代理，是为了防止频繁请求导致ip被封，代理功能可以自动切换代理，保证程序的正常运行。在测试过程中我们发现，该网站不会对ip进行封锁，所以代理可有可无。这里大致说一下代理功能是如何实现的。 &emsp;&emsp;代理功能使用的代理池是Github上的开源项目，它通过从代理平台抓取可用的代理ip存储到本地Redis中，需要使用代理时，即从本地Redis中取出。使用代理功能需要进行相应的配置。 123安装并开启Redis服务器安装依赖 pip3 install -r requirements.txt开启代理服务 python run.py &emsp;&emsp;在上述代码中，我们使用了捕捉异常的语句，因为在使用代理的过程中我们发现代理ip可能存在网络不稳定，传输有延时等问题。总的来说，使用代理的查询速度很慢，不想使用代理的话直接将proxy配置成本地的ip和端口即可。 多线程&emsp;&emsp;在开发过程中，想过用多线程，但是效果不太理想（对并行编程不熟悉），后来想想对于查找准考证号这种问题可以根据实际情况灵活，可能有些人会大致记得自己的考场位于哪个区间之内，所以在项目中，提供了输入查询区间的接口。如果想提高查询速度，可以开启多个终端，每个终端输入不同的查询区间，这样就类似于开启了多进程（一般查询的时候开启10个终端，每个终端的考场区间为10，10分钟内可以查询到结果）。 使用教程&emsp;&emsp;简单介绍一下该项目的文件结构，如图所示。 images：主要用来存放验证码图片，images中包含多个目录，row_picture存放原始验证码，change_picture存 * 放灰度化、二值化处理后的验证码，train_data存放分割后的验证码 proxypool：实现代理功能的相关代码 acquire_picture.py：包含验证码获取、处理相关操作的代码 model.pkl：存放识别模型 recongnition_code.py：项目的执行入口，包含向服务器发送请求、代理等相关代码 setting.py：项目相关的配置文件 train_data_preprocessing.py：整合验证码获取和处理相关操作 train_model.py：训练模型 &emsp;&emsp;该项目使用的大致流程如下（要求python版本不低于3.5，该项目在win10环境测试运行无误）。 123456安装相关依赖PIL、requests、numpyy、sklearn等修改recongnition_code.py文件中的myid（准考证号前10位）、name（自己的名字）修改recongnition_code.py文件中成绩查询选项如果需要使用代理，需要配置代理相关环境在项目文件夹中打开终端输入：python recongnition_code.py 开始区间 结束区间可同时开启多个终端，每个设置不同的区间，加快查找速度]]></content>
      <categories>
        <category>机器学习算法</category>
      </categories>
      <tags>
        <tag>机器学习</tag>
        <tag>验证码识别</tag>
        <tag>爬虫</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[2018年下半年学习计划]]></title>
    <url>%2Fplan.html%2F</url>
    <content type="text"><![CDATA[摘要: 即将开始研究生阶段的学习生活了，我希望这对于我来说是一个全新的开始。所有伟大的梦想都源于一个切实可行的计划，为了迎接即将开始的旅程，我也需要这样一个计划，希望在它的鞭策和激励下让我不断成长，奋力前行！ 计划大纲 2018.8.16-2018.9.1 机器学习算法实现系列博客（贝叶斯分类器和决策树） 学习python数据结构和算法 2018.9.2-2018.9.30 阅读高质量论文两篇 完成机器学习算法实现系列博客（大概5篇） 开始阅读《凸优化理论》 2018.10.1-2018.10.31 阅读高质量论文三篇 完成《凸优化理论》的学习 开始阅读流畅的python 博客写作三篇 2018.11.1-2018.11.30 阅读高质量论文三篇 完成流畅的python 学习javascript 博客写作三篇 2018.12.1-2018.12.31 阅读高质量论文三篇 学习node.js 阅读HTTP协议详解 博客写作三篇 2018.1.1-2018.1.31 阅读高质量论文三篇 搭建node.js网站 学习docker 博客写作三篇 八月学习计划完成情况]]></content>
      <categories>
        <category>学习计划</category>
      </categories>
      <tags>
        <tag>plan</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[HTTPS详解：SSL/TLS协议]]></title>
    <url>%2FTLS.html%2F</url>
    <content type="text"><![CDATA[摘要: 最近在看关于web安全相关的书籍，说到web安全HTTP和HTTPS之间的联系和区别是一个无法回避的问题。很长时间以来，我也被这个问题所困扰，对于其中涉及的细节问题更是难以触及，但是现在是该去好好考虑这个问题了。在网上查阅了大量资料之后，我发现很多文章在解释这个问题的时候含糊不清，让人难以理解。所以我写下了这篇博客，希望能给大家提供帮助，如有不足之处，欢迎指正！ 文章概览 区别与联系 HTTP和HTTPS 联系 区别 SSL和TLS 联系 区别 SSL/TLS协议 密码套件 记录协议 握手协议 前向安全 OPENSSL 区别与联系 HTTP和HTTPS联系&emsp;&emsp;HTTP（超文本传输协议）是一个客户端终端（用户）和服务器端（网站）请求和应答的标准（TCP）。通过使用网页浏览器、网络爬虫或者其它的工具，客户端发起一个HTTP请求到服务器上指定端口（默认端口为80）。我们称这个客户端为用户代理程序（user agent）。应答的服务器上存储着一些资源，比如HTML文件和图像。我们称这个应答服务器为源服务器（origin server）。在用户代理和源服务器中间可能存在多个“中间层”，比如代理服务器、网关或者隧道（tunnel）。可以从HTTP头、HTTP请求方法、HTTP状态码和统一资源定位符URL四个方面深入理解HTTP协议。&emsp;&emsp;超文本传输协议HTTP协议被用于在Web浏览器和网站服务器之间传递信息。HTTP协议以明文方式发送内容，不提供任何方式的数据加密，易遭受窃听、篡改、劫持等攻击，因此HTTP协议不适合传输一些敏感信息，比如信用卡号、密码等。为了解决HTTP协议的这一缺陷，需要使用另一种协议：安全套接字层超文本传输协议HTTPS。为了数据传输的安全，HTTPS在HTTP的基础上加入了SSL/TLS协议，SSL/TLS依靠证书来验证服务器的身份，并为浏览器和服务器之间的通信加密，一次HTTPS协议实现了数据传输过程中的保密性、完整性和身份认证性。区别&emsp;&emsp;HTTP和HTTPS的主要区别如下包括:HTTPS协议需要到CA申请证书,而HTTP协议则不用；HTTP是超文本传输协议，信息是明文传输，而HTTPS则是加密传输；HTTP和HTTPS使用完全不同的连接方式，所占用的端口也不一样，前者占用80端口，后者占用443端口；HTTPS传输过程比较复杂，对服务端占用的资源比较多，由于握手过程的复杂性和加密传输的特性导致HTTPS传输的效率比较低；HTTP的连接很简单，是无状态的；HTTPS协议是由SSL+HTTP协议构建的可进行加密传输、身份认证的网络协议，比HTTP协议安全。SSL和TLS 联系&emsp;&emsp;SSL(Secure Sockets Layer 安全套接层)为Netscape所研发，用以保障在Internet上数据传输之安全，利用数据加密(Encryption)技术，可确保数据在网络上之传输过程中不会被截取及窃听。SSL协议位于TCP/IP协议与各种应用层协议之间，为数据通讯提供安全支持。SSL协议可分为两层： SSL记录协议（SSL Record Protocol）：它建立在可靠的传输协议（如TCP）之上，为高层协议提供数据封装、压缩、加密等基本功能的支持。 SSL握手协议（SSL Handshake Protocol）：它建立在SSL记录协议之上，用于在实际的数据传输开始前，通讯双方进行身份认证、协商加密算法、交换加密密钥等。&emsp;&emsp;安全传输层协议（TLS）用于在两个通信应用程序之间提供保密性和数据完整性。该协议由两层组成： TLS 记录协议（TLS Record）和 TLS 握手协议（TLS Handshake）。较低的层为 TLS 记录协议，位于某个可靠的传输协议（例如 TCP）上面，与具体的应用无关，所以，一般把TLS协议归为传输层安全协议。TLS 的最大优势就在于：TLS 是独立于应用协议。高层协议可以透明地分布在 TLS 协议上面。然而，TLS 标准并没有规定应用程序如何在 TLS 上增加安全性；它把如何启动 TLS 握手协议以及如何解释交换的认证证书的决定权留给协议的设计者和实施者来判断。&emsp;&emsp;SSL是Netscape开发的专门用户保护Web通讯的，而TLS1.0是IETF(工程任务组)制定的一种新的协议，它建立在SSL 3.0协议规范之上，是SSL 3.0的后续版本。两者差别很小，可以理解为SSL 3.1，它是写入了RFC的。为了兼顾各种说法本文将SSL和TLS统称为SSL/TLS，但是请注意，本文所涉及的关于各种协议的解析是基于TLS1.2版本（这是目前使用最广泛的版本）。 区别&emsp;&emsp;SSL和TLS的主要区别如下： 版本号：TLS记录格式与SSL记录格式相同，但版本号的值不同。 报文鉴别码：SSLv3.0和TLS的MAC算法及MAC计算的范围不同。TLS使用了RFC-2104定义的HMAC算法。SSLv3.0使用了相似的算法，两者差别在于SSLv3.0中，填充字节与密钥之间采用的是连接运算，而HMAC算法采用的是异或运算。但是两者的安全程度是相同的。 伪随机函数：TLS使用了称为PRF的伪随机函数来将密钥扩展成数据块，是更安全的方式。 报警代码：TLS支持几乎所有的SSLv3.0报警代码，而且TLS还补充定义了很多报警代码，如解密失败、记录溢出、未知、拒绝访问等。 加密计算：TLS与SSLv3.0在计算主密值（master secret）时采用的方式不同。 填充：用户数据加密之前需要增加的填充字节。在SSL中，填充后的数据长度要达到密文块长度的最小整数倍。而在TLS中，填充后的数据长度可以是密文块长度的任意整数倍（但填充的最大长度为255字节），这种方式可以防止基于对报文长度进行分析的攻击。 SSL/TLS协议 密码套件（cipher suite）&emsp;&emsp;密码套件（Cipher suite）是传输层安全（TLS）/安全套接字层（SSL）网络协议中的一个概念。在TLS 1.2中，密码套件的名称是以协商安全设置时使用的身份验证、加密、消息认证码（MAC）和密钥交换算法组成。TLS 1.3中的密码套件格式已经修改。在目前的TLS 1.3草案文档中，密码套件仅用于协商加密和HMAC算法。在创建一个TLS连接后，一次也称TLS握手协议的握手发生。在这个握手，一条ClientHello和一条ServerHello消息被发出。首先，客户端按照偏好的顺序发送它支持的密码套件的列表。然后服务器回复它从客户端的列表中选择的密码套件。&emsp;&emsp;例如TLS_ECDHE_RSA_WITH_AES_128_GCM_SHA256,每个密码套件名称定义一个密钥交换算法、一个批量加密算法、一个消息认证码（MAC）算法，以及一个伪随机函数(PRF) 密钥交换算法，例如ECDHE_RSA，用于决定客户端与服务器之间在握手时如何身份验证 批量加密算法，例如AES_128_GCM，用于加密消息流。它还包括密钥大小及显式和隐式初始化向量（密码学随机数）的长度 消息认证码算法，例如SHA256，用于创建消息摘要，消息流每个数据块的加密散列 伪随机函数，例如TLS 1.2的伪随机函数使用MAC算法的散列函数来创建一个主密钥——连接双方共享的一个48字节的私钥。主密钥在创建会话密钥（例如创建MAC）时作为一个熵来源 理解密码套件的作用以及组成部分对我们理解握手协议的过程十分重要，因为使用不同的密码套件在握手协议的实现细节上有很大的不同，特别是密钥交换的过程。本文将主要讲解ECDHE_RSA密钥交换算法下的握手过程（也可以理解为三种不同密码套件的握手过程，对于握手过程而言不同套件的差异主要体现在密钥交换的过程，批量加密算法和消息验证码算法的不同主要体现在加密传输的过程，伪随机算法的不同体现在产生分组加密初始向量的过程）。 记录协议&emsp;&emsp;TLS记录协议位于TLS握手协议的下层，在可靠的传输协议(如TCP/IP)上层。TLS记录协议的一条记录包含长度字段、描述字段和内容字段。TLS记录协议处理数据的加密，即记录协议得到要发送的消息之后，将数据分成易于处理的数据分组，进行数据压缩处理(可选)，计算数据分组的消息认证码MAC，加密数据然后发送数据；接收到的消息首先被解密，然后校验MAC值，解压缩，重组，最后传递给协议的高层客户。记录协议有四种类型的客户：握手协议、警告协议、改变密码格式协议和应用数据协议。通常使用一个对称算法，算法的密钥由握手协议提供的值生成。TLS 记录协议提供的连接安全性具有两个基本特性: 私有――对称加密用以数据加密（DES 、RC4 等）。对称加密所产生的密钥对每个连接都是唯一的，且此密钥基于另一个协议（如握手协议）协商。记录协议也可以不加密使用 可靠――信息传输包括使用密钥的 MAC 进行信息完整性检查。安全哈希功能（ SHA、MD5 等）用于 MAC 计算。记录协议在没有 MAC 的情况下也能操作，但一般只能用于这种模式，即有另一个协议正在使用记录协议传输协商安全参数 握手协议&emsp;&emsp;TLS握手协议处理对等用户的认证，在这一层使用了公共密钥和证书，并协商算法和加密实际数据传输的密钥，该过程在TLS记录协议之上进行。TLS握手协议是TLS协议中最复杂的部分，它定义了10种消息，客户端和服务器利用这10种消息相互认证，协商哈希函数和加密算法并相互提供产生加密密钥的机密数据。TLS记录协议会在加密算法中用到这些加密密钥，从而提供数据保密性和一致性保护。&emsp;&emsp;我们先来分析基于ECDHE_RSA密钥交换算法的握手过程，在这之前先来解释一下ECDHE是什么。ECDHE_RSA = EC（椭圆曲线加密算法)+ DH(Diffie-Hellman密钥交换算法)+ E(临时的temporary)+ RSA(用于签名，防止中间人攻击)，所以ECDHE的意思是结合椭圆曲线的生成临时会话密钥的密钥交换算法。对于这个算法的具体计算过程，这里不详细讨论。&emsp;&emsp;下图是EDCHE_RSA密钥交换算法的大致流程，接下来我会结合wireshark抓取的数据包来分析握手的过程： 在客户端和服务器开始握手之前先进行TCP三次握手，这部分的内容本文不会讨论。三次握手之后，开始握手协议，先在这展示一下抓到的所有握手包。 Client hello：由于客户端对一些加解密算法的支持程度不一样，但是SSL/TLS协议传输过程中必须要求客户端与服务器端使用相同的加解密算法。所以在client hello阶段，客户端要首先告知服务端，自己支持哪些密码套件，客户端将支持的密码套件列表发送给服务端；同时客户端还会产生一个随机数，这个随机数双方都要保存（生成主密钥）；session id字段是用于维持会话，如果客户端与服务端关闭会话之后，客户端又要重新发起会话，session id可用于双方协商是否要进行重新握手过程；extension字段用于添加一些拓展功能；compress表示支持的压缩方法。 server hello：server hello是根据客户端发送过来的密码套件和压缩方法选择双方都支持的类型，同时服务器端也会生成一个随机数，双方都要保存。 certificate：该过程中服务器用私钥签名证书，发送给客户端以认证身份 server key exchange：对于ECDHE_RSA密钥交换算法来说这一过程是必须的，在此过程服务端将生成一对公钥和私钥，私钥保留（用于服务器端生成预主密钥），并将公钥发送给客户端（用于客户端生成预主密钥），同时将前一阶段所有的会话内容利用私钥进行前面发给客户端，用于验证服务端身份，防止中间人攻击。而对于RSA_RSA密钥交换算法，没有这一过程，同样有这个过程的还有DHE密钥交换算法，有没有这一过程都是却决于密钥交换算法自身。在这个数据包中，还给出了服务端生成公私钥所用的算法sec256r1 server hello done：表示server hello结束，这是个空消息 client key exchange：客户端也生成一对公钥和私钥，私钥保留（用于客户端生成预主密钥），公钥发给服务端（用于服务端生成预主密钥） change cipher spec：客户端根据交互过程中获得的信息，以及应用服务端规定的密码套件，已经生成了相应的密钥。通过这条消息，客户端告诉服务器端：从现在起，我将使用双方约定的密码规范进行通信 encrypted handshake message：客户端利用生成的密钥加密一段finishde数据传送给服务端，此数据是为了在正式传输应用之前对刚刚握手建立起来的加解密通道进行验证 new session ticket：服务端告知客户端将生成新的session ticket用于保持会话（session ticket与前面提到的session id作用类似，但两者实现方式不同） change cipher spec：同样服务端也要发送这段信息，作用与客户端一致 encrypted handshake message：作用与客户端一致至此，握手协议结束，双方开始建立加密通道。 &emsp;&emsp;值得注意的是在这个过程中客户端和服务器端都各自产生一对公钥和私钥还有一个随机数，这些都是作为生成预主密钥的元素。预主密钥分别在客户端和服务器端生成，算法的特性能够保证二者生成的预主密钥相同。那么由预主密钥如何生成会话密钥呢，这就要用到前面提到的伪随机函数，通过预主密钥我们将生成客户端验证密钥、服务器端验证密钥、客户端加密密钥、服务器端加密密钥以及客户端分组加密的初始向量和服务器端的分组加密初始向量，具体生成过程可以参考这篇博客。对基于DH算法和RSA算法的握手过程可以参见下图，也可以参考这篇博客。 前向安全&emsp;&emsp;在这里有必要提一下关于前向安全的定义：前向安全或前向保密，有时也被称为完美前向安全（Perfect Forward Secrecy，缩写：PFS），是密码学中通讯协议的安全属性，指的是长期使用的主密钥泄漏不会导致过去的会话密钥泄漏。前向安全能够保护过去进行的通讯不受密码或密钥在未来暴露的威胁。如果系统具有前向安全性，就可以保证在主密钥泄露时历史通讯的安全，即使系统遭到主动攻击也是如此。在传输层安全协议（TLS）中，提供了基于迪菲-赫尔曼密钥交换（DHE）的前向安全通讯，分别为（DHE-RSA）和DHE-DSA），还有基于椭圆曲线迪菲-赫尔曼密钥交换（ECDHE）的前向安全通讯，包括（ECDHE-RSA与ECDHE-ECDSA）。理论上，从SSLv3开始，就已经可以使用支持前向安全的密码算法进行通讯。之前我们提到ECDHE算法在sever key exchage阶段会生成一个临时的公私钥对，公钥发送给用户，私钥用于对数据进行RSA签名来验证服务器的身份，如果服务器的私钥泄露，这些会话不会受到影响，无法解密。对于有些算法而言，它在握手过程中不会有这个生成公私钥对的过程，它将使用服务器的私钥进行签名。如果服务器的私钥泄露，这些会话都将被暴露，这就是所谓的前向安全。 openssl]]></content>
      <categories>
        <category>web安全</category>
      </categories>
      <tags>
        <tag>HTTPS和HTTP</tag>
        <tag>SSL安全套接层</tag>
        <tag>TLS安全传输层协议</tag>
        <tag>openssl</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[K近邻算法详解]]></title>
    <url>%2FKNN.html%2F</url>
    <content type="text"><![CDATA[摘要: K近邻（简称KNN）是一种基于统计的数据挖掘算法，它是在一组历史数据记录中寻找一个或者若干个与当前记录最相似的历史记录的特征值来预测当前记录的未知的特征值，因此具有直观、无需先验统计知识等特点，同时K近邻算法适用于分类和回归两种不同的应用场景，本文主要介绍K近邻算法在回归任务场景下的应用。 文章概览 K近邻概述 K近邻三要素 距离度量 K值选择 分类决策规则 K近邻的实现 线性查找 空间分割 kd树的生成 kd树的搜索 scikit_learn中的K近邻算法 总结 K近邻概述 &emsp;&emsp;K近邻算法简单直观，下面举一个简单的例子帮助大家理解。在一个城市当，居住着许多不同民族的居民，相同民族的人们大多聚集在一起，形成一个小型的部落。现在你想知道其中一个部落是属于哪个民族的，并且你已经掌握很多关于部落和民族的信息，你会怎么做？其实我们可以通过观察这个部落的人们的生活习惯、节日风俗、衣着服饰等特点，在与我们掌握其他部落的特点进行对比，找出与该部落在这些方面最接近的几个部落（已知这几个部落分别属于哪个民族），如果这几个部落的多数属于哪个民族，那么在很大程度上我们可以猜测该部落可能也属于这个民族，从而得到我们想要的答案。&emsp;&emsp;对于K近邻稍微正式一点的描述是：给定一个训练数据集，对新的输入实例，在训练数据集中找到与该实例最近邻的K个实例，这K个实例的多数属于某个类，就把该输入实例分为这个类。从这段描述中我们可以看出，K近邻算法的学习过程只是简单的存储已知的训练数据，当遇到新的查询实例时，再从存储器中取出一系列相似的实例，用来分类新的查询实例。我们把K近邻算法的这种分类特点称为消极学习方法,具有同样特点的学习算法还有局部加权回归，它的优点在于不是在整个实例空间上一次性的估计目标函数，而是针对每个待分类的新实例做出局部的和相异的估计。而与之对应的分类算法，我们称之为积极学习方法，例如：支持向量机、神经网络等等，它的特点是在新的查询实例到来之前，通过训练实例总结归纳出相似判断的目标函数。&emsp;&emsp;K近邻同样可以应用于回归任务。K近邻做分类预测时，一般是选择多数表决法，即训练集里和预测的样本特征最近的K个样本，预测为里面有最多类别数的类别。而K近邻做回归时，一般是选择平均法，即最近的K个样本的样本输出的平均值作为回归预测值。由于两者区别不大，虽然本文主要是讲解K近邻的分类方法，但思想对K近邻的回归方法也适用。 K近邻三要素 距离度量&emsp;&emsp;回到刚才那个例子，我们假设从生活习惯、节日风俗、衣着服饰、宗教信仰等多个方面来考察部落之间的相似程度。所谓的相似程度用另外一种说法来表达即差异，差异越小，相似程度越大。在机器学习中，我们使用距离来度量差异。一般情况下我们采用欧式距离来度量差异（其他的距离度量方式如曼哈顿距离等同样适用）。欧式距离： 设特征空间\({\chi}\)是\(n\)维实数向量空间\(R^n,x_i,x_j\)属于\({\chi}\)，\(x_i=(x^1_i,x^2_i,…,x^n_i),x_j=(x^1_j,x^2_j,…,x^n_j),x_i,x_j\)的欧式距离定义为$$\sqrt{\sum_{l=1}^{n}|x_i^l-x_j^l|^2}$$&emsp;&emsp;刚才说到我们将通过生活习惯、节日风俗、衣着服饰、宗教信仰等多个方面来考察部落之间的相似程度，但是现在我们需要考虑这样一种情况，我们观察发现这些部落虽然在有些方面存在很大的差异，但是这些差异却不能成为区分不同民族的依据，比如说，A和B两个部落都属于C民族，但是A部落信仰D教，B部落信仰E教。也就是说，应用k-近邻的一个实践问题是，实例间的距离是根据实例的所有属性计算的，但是这些属性当中存在着对分类无关的属性，这些无关的属性可能在实例空间中相距很远，这样一来近邻间的距离会被大量的不相关属性所支配。这种由于存在很多不相关属性所导致的难题，有时被称为维度灾难。解决该问题的一个方法是，当计算两个实例间的距离时对每个属性加权，从不断的测试中获得启发，给对分类影响大的属性赋予更高的权值。 K值选择&emsp;&emsp;K值的选择会对K近邻法的结果产生巨大的影响。如果选择较小的K值，学习的近似误差会减小，只有与输入实例较近的训练实例才会对预测结果起作用，这样做存在的问题是预测结果对近邻点过于敏感，如果近邻点恰巧是噪声，预测结果就会出错。如果选择较大的K值，其优点是可以减少学习得估计误差，缺点是与输入实例较远的训练实例也会对预测起作用。K值选择的原则往往是经过大量独立测试数据、多个模型来验证最佳选择。 分类决策规则&emsp;&emsp;K近邻中的分类决策往往是多数表决，即由输入实例的K个近邻的训练实例中的多数类决定输入实例的类。这样的决策规则存在一个问题，假设我们现在已知A、B两个部落属于同一个民族，C、D、E三个部落属于同一个民族。为了测试我们的模型，我们将A部落作为实例，输入到模型中进行测试，K值设为4。经过计算我们发现，得到的K个近邻实例分别为B、C、D、E，并且A、B部落之间的特征距离很小，而A与C、D、E三个部落之间的特征距离很大。但是由于分类决策规则是依据多数进行表决的，所以我们最终会将A判断为与C、D、E部落相同的民族。由此可以看出，多数表决的决策规则是不合理的。解决这一问题的方法是对距离进行加权，B部落与A部落的差异比较小，所以在K个实例当中B部落应该对最终的决策产生更大的影响，而距离越远影响力越小。 K近邻的实现 线性查找&emsp;&emsp;K近邻的核心思想是寻找与输入实例距离最近的K个实例，那么一个最朴素的想法是计算输入实例和所有训练实例之间的距离，然后从中挑选出距离最近的K个实例，这就是线性查找的思想，具体实现如下（大家可以在我的github中找到本文所有的源代码）：123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081#!/usr/bin/env python#_*_ coding:utf-8 _*_#通过直接对k-近邻算法的描述来构建鸢尾花数据集的模型，并利用该模型对鸢尾花类型进行预测import numpy as npimport randomimport operatorfrom init_data import load_datadef data_split(): """ data_split函数的主要作用是将原始数据分为训练数据和测试数据，其中训练数据和测试数据的比例为2：1 """ x_data,y_data = load_data() x_training = [] x_test= [] y_training = [] y_test = [] for i in range(len(x_data)): if random.random() &gt; 0.67: x_training.append(x_data[i]) y_training.append(y_data[i]) else: x_test.append(x_data[i]) y_test.append(y_data[i]) x_training = np.array(x_training) y_training = np.array(y_training) x_test = np.array(x_test) y_test = np.array(y_test) return (x_training,x_test,y_training,y_test)def euclidean_distance(x_training,row): """ euclidean_distance函数的主要功能是计算每一条测试数据和训练数据集的欧式距离 :param x_training: 训练数据集 :param row: 一条测试数据 :return: 表示欧式距离的矩阵 """ dis = np.sum((row - x_training)**2,axis=1) dis = np.sqrt(dis) return disdef predict(dis,y_training,k): """ predict函数的主要作用是通过计算所得的欧式距离的集合，从中选取k个距离最小的数据点，统计这k个数据点中各个类别所出现的次数，出现次数最多的类别即为预测值 :param dis: 表示欧式距离的矩阵 :param y_training: 训练数据的类别 :param k: 选取k个距离最近的数据 :return: 预测值 """ dis_sort = np.argsort(dis)#对欧式距离集合进行排序，返回的dis_sort表示的是排序（从小到大）后的数据在原数组中的索引 statistics = &#123;&#125;#定义字典，用于统计k个数据点中各个类别的鸢尾花出现的次数 for i in range(k): rank = dis_sort[i] if y_training[rank] in statistics: statistics[y_training[rank]] = statistics[y_training[rank]] + 1 else: statistics[y_training[rank]] = 1 sort_statis = sorted(statistics.items(), key=operator.itemgetter(1), reverse=True)#对statistics字典按照value进行排序（从大到小） y_predict = sort_statis[0][0] return y_predictif __name__ == "__main__": x_training, x_test, y_training, y_test = data_split() num = 0 i = 0 for row in x_test: dis = euclidean_distance(x_training,row) y_predict = predict(dis,y_training,5) if y_predict == y_test[i]: num = num + 1 i = i + 1 print('The accuracy is &#123;0:1f&#125;%'.format(num/i)) 空间分割&emsp;&emsp;基于线性查找的思想，存在一个严重的问题是，如果训练集合很大，计算非常耗时，这种方法在实际中难以应用。为了提高K近邻的搜索效率，我们考虑将搜索空间进行分割，通过这种方法来提高搜索效率，减少计算距离的次数。具体的方法有很多，这里主要介绍kd树。&emsp;&emsp;关于kd树的算法和结构定义大家可以参考《统计学习方法》和这篇博文,本文主要关注kd树python实现的一些细节问题。 kd树的生成 每次对子空间的划分时，怎样确定在哪个维度上进行划分：在《统计学习方法》中采用的是轮流的方式，即如果这次选择了在第i维上进行数据划分，那下一次就在第j(j≠i)维上进行划分，例如：j = (i mod k) + 1。但是这样忽略了不同属性数据之间的分散程度，有的属性值比较分散，有的属性值比较集中，如果我们以数据分布比较分散的属性作为数据分割的依据，可以更大程度的分割数据，这样更有利于提高搜索的效率。方差可以衡量数据集合的分散程度，所以一般情况下我们采用最大方差分割法对数据集合进行分割。 以下是kd树生成算法的python描述，代码标注了详细的解释，可以在我的github中找到完整的代码。123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960def splitdata(data): """ splitdata函数的作用是对输入数据集合进行分割，具体规则：求出方差值最大的那一维特征，然后将整个数据集合根据这一维特征进行排序，中位数为分割点 :param data: 数据集合 :return: 分割数据的属性，数据分割点，分割后两个部分的数据集合 """ n,m = np.shape(data) #获取numpy矩阵维度 right_data = [] left_data = [] num = 0 row_mean = np.sum(data,axis=0) / n #求矩阵每一列的平均值 row_variance = np.sum((data - row_mean)**2,axis=0) #求矩阵每一列的方差 max_row_variance = np.where(row_variance == np.max(row_variance))[0][0] #方差值最大的那一列的索引 sort_data = np.argsort(data,axis=0) #data矩阵按照列排序，返回排序后的对应的索引 split_row = sort_data[:,max_row_variance] #方差值最大的那一列排序后的索引值 split_index = int(n/2) #中位数 for line in split_row: #将data中的数据分成两个部分，索引排在中位数之前的放进left_data,反之放进right_data if num &gt; split_index: if right_data == []: right_data = data[line,:] right_data = np.array([right_data]) else: right_data = np.concatenate((right_data,[data[line,:]]),axis=0) elif num &lt; split_index: if left_data == []: left_data = data[line,:] left_data = np.array([left_data]) else: left_data = np.concatenate((left_data,[data[line,:]]),axis=0) num = num + 1 #用于计数 split_data = data[split_row[split_index]] #取对应原始数据中的分割点值 print("分割结点为：",split_data,"--------- 分割维度为：",max_row_variance) return(max_row_variance,split_data,right_data,left_data) #返回值分别为分割数据的属性，数据分割点，分割后两个部分的数据class KNode(object): """ 定义节点类 """ def __init__(self,row = None,point = None,right = None,left = None,parent = None): self.row = row #分割数据集合的特征 self.point = point #数据分割点 self.right = right #右子树 self.left = left #左子树def create_tree(dataset,knode): """ create_tree函数的主要作用是通过递归的方式来建立kd树 :param dataset: 数据集合 :param knode: 根结点 :return: 返回kd树 """ length = len(dataset) if length == 0: return row,point,right_data,left_data = splitdata(dataset) knode = KNode(row,point) knode.right = create_tree(right_data,knode.right) knode.left = create_tree(left_data,knode.left) return knode kd树的搜索 kd树的搜索分为两个过程，首先找出包含输入实例的叶子结点，然后在从叶子结点回溯寻找K个近邻实例。在寻找叶子结点的过程中，我们会建立三个列表，一个列表用于存储搜索路径，一个列表用于存储K个近邻点，另外一个列表用于存储K个近邻点所对应的与输入实例的距离，在搜索叶子结点的过程中就计算K近邻点有利于简化回溯过程的搜索。在该过程中，每到达一个结点，我们先将该结点加入搜索路径，然后计算该结点与输入实例之间的距离，如果K近邻点列表中不足K个结点，直接将该结点加入K近邻点列表，同时将计算的距离加入对应的距离列表；如果K近邻列表中已经有K个结点，则选择距离列表中距离最大值与该结点计算的距离进行比较。如果该结点的距离小，则删除最大距离对应的结点，加入该结点，反之无需改变。 kd树的回溯过程稍微麻烦一点,大家可以参照我给出的代码注释进行理解。在看代码的时候一定要学会去调试，可以帮助我们理解。123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263def find_KNN(point,kdtree,k): """ k近邻查找 :param point: 测试数据点 :param kdtree: 建立好的kd树 :param k: k值 :return: k个近邻点 """ current = kdtree #当前节点 nodelist = [] #搜索路径 nodek = [] #存储k个近邻点与测试数据点之间的距离 nodek_point = [] #存储k个近邻点对应的值 min_dis = euclidean_distance(point,kdtree.point) print("---------------------------------------------------------------------------------------") while current: #找到测试点所对应的叶子结点，同时将搜索路径中的结点进行k近邻判断 nodelist.append(current) #将当前结点加入搜索路径 dis = euclidean_distance(point,current.point) if len(nodek) &lt; k: #nodek中不足k个结点时，直接将当前结点加入nodek_point nodek.append(dis) nodek_point.append(current.point) print(current.point,"加入k近邻列表") else: #nodek中有k个结点时，删除距离最大的哪个结点，再将该结点加入nodek_point max_dis = max(nodek) if dis &lt; max_dis: index = nodek.index(max_dis) print(current.point, "加入k近邻列表;",nodek_point[index],"离开k近邻列表") del(nodek[index]) del(nodek_point[index]) nodek.append(dis) nodek_point.append(current.point) ind = current.row #该结点进行分割时的特征 if point[ind] &gt;= current.point[ind]: current = current.right else: current = current.left while nodelist: #回溯寻找k近邻 back_point = nodelist.pop() ind = back_point.row max_dis = max(nodek) if len(nodek) &lt; k or abs(point[ind] - back_point.point[ind])&lt;max_dis: #如果nodek_point中存储的节点数少于k个，或者测试数据点和当前结点在分割特征维度上的差值的绝对值小于k近邻中的最大距离 if point[ind] &lt;= back_point.point[ind]: #注意理解这一段判断的代码，因为在之前寻找叶子结点的过程中，我们决定搜索路径的判断方法是大于即搜索右子树，小于即搜索左子树，这里的判断恰恰相反，是为了遍历之前没有搜索的结点 current = back_point.right else: current = back_point.left if current: nodelist.append(current) dis = euclidean_distance(point,current.point) if max_dis &gt; dis and len(nodek) == k: index = nodek.index((max_dis)) print(current.point, "加入k近邻列表;", nodek_point[index], "离开k近邻列表") del(nodek[index]) del (nodek_point[index]) nodek.append(dis) nodek_point.append(current.point) elif len(nodek) &lt; k: nodek.append(dis) nodek_point.append(current.point) print(current.point, "加入k近邻列表") return nodek_point scikit_learn中的K近邻算法scikit_learn中的K近邻算法的具体解释大家可以参照scikit_learn官网的文档，这里给出一段利用scikit_learn解决鸢尾花数据集的代码。这篇文章中给出的代码都是基于UCI鸢尾花数据集实现的，大家可以比较一下这三种实现方式的预测准确率。1234567891011121314151617#!/usr/bin/env python#_*_ coding:utf-8 _*_#通过sklearn库中所提供的关于k-近邻算法相关的包来实现对鸢尾花数据集的建模与预测from init_data import load_dataimport numpy as npfrom sklearn.neighbors import KNeighborsClassifierfrom sklearn.cross_validation import train_test_splitx_data,y_data = load_data() #获取数据x_training,x_test,y_training,y_test = train_test_split(x_data,y_data,random_state=10) #将数据分为训练集和测试集estimotor = KNeighborsClassifier() #构造k-近邻分类器estimotor.fit(x_training,y_training) #训练模型y_predicted = estimotor.predict(x_test) #用训练的模型进行预测accuracy = np.mean(y_test == y_predicted)*100 #计算预测结果的准确率print('The accuracy is &#123;0:1f&#125;%'.format(accuracy)) 总结K近邻算法的优点在于 算法简单直观，易于实现 K近邻在进行类别决策时只于少量的相邻样本有关，可以避免样本数量不平衡问题 K近邻最直接的利用了样本之间的关系，减少了类别特征选择不当对分类结果造成的不利影响，可以最大程度减少分类过程中的误差项 同时K近邻算法存在的问题也很突出 当样本数量大、特征多的时候计算量非常大 样本不平衡的时候，对稀有类别的预测准确率降低 预测速度慢 &emsp;&emsp;花了两天的时间找资料、写代码，又花了一天的时间写文章，终于结束了！有一点不太满意的地方是对kd树的回溯没有进行详尽的描述，原因是真的找不出一种好的描述方法，文字描述看起来累，用图表的话工作量太大，大家可以看看我推荐的博文结合代码，理解起来也不会太难。好了，到这里结束了，好好加油，坚持！]]></content>
      <categories>
        <category>机器学习算法</category>
      </categories>
      <tags>
        <tag>KNN</tag>
        <tag>KD树</tag>
        <tag>机器学习</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[搭建博客：HEXO+GITHUB+CODING]]></title>
    <url>%2Fhex.html%2F</url>
    <content type="text"><![CDATA[摘要: 进入计算机行业已经好几年了，这么多年的摸爬滚打，我终于意识到了一个血的教训:好记性不如烂键盘!当我们遇到问题并解决问题之后，我们应该及时的把我们处理问题的过程记录下来，一来可以防止我们在此遇到同样的问题时又要重复造轮子，二来可以为遇到同样问题的小伙伴提供经验，所以对于我们来说有一个属于自己的博客尤为重要。本文记录的是搭建hexo个人博客平台过程中遇到的一些问题和心得，希望能对小伙伴们有所启发。 文章概览 HEXO简介 静态博客与动态博客 基本流程 运行机制 HEXO NEXT主题美化 HEXO部署到GITHUB 提交搜索引擎 提交谷歌搜索引擎 提交百度搜索引擎 总结 HEXO简介Hexo是一个基于node.js开发的快速、简洁且高效的静态博客框架。Hexo 使用 Markdown（或其他渲染引擎）解析文章，在几秒内，即可利用靓丽的主题生成静态网页。hexo具有以下这些特性： hexo基于node.js，非常小巧，安装部署简单 hexo开源，主题丰富，插件丰富，自定义能力强 hexo支持markdown语法，易于博客写作 hexo是纯静态博客，不需要数据库支持 静态博客系统与动态博客系统一个网站最基础的部分就是网页，如果想从HTML页面写起，显然成本太高，好在大牛们已经做好了博客生成器来解决网页编写的问题。一般来说，博客生成器分为动态和静态两种。其中，动态博客生成器典型代表有：WordPress、FarBox、Ghost等，静态的博客生成器典型代表有：Hexo、Jekyll、Octopress、Hugo等。关于动态和静态的区别主要有以下几点： 资源占用上，静态博客相对于动态博客占用服务器资源少，可以托管在github pages上，而动态博客往往需要一台相对独立的服务器 数据管理和更新操作上，由于动态博客有独立的数据库和后台管理系统，对资源的管理和发布相对比较容易；而静态博客往往需要一些第三方平台的支持，如评论系统以及图床，数据管理更新比较繁琐。 安全性上，静态博客比动态博客安全性更好 基本流程&emsp;&emsp;这里我大致叙述一下搭建hexo博客系统的大致流程。我们首先要搭建hexo博客系统的开发环境，这里我主要讲解windows环境下的安装配置，其他系统的安装配置可以参考官方文档。1234567安装git安装node.js执行命令 npm install -g hexo-cli（安装hexo命令行工具）执行命令 hexo init [文件夹名]进入刚才初始化的文件夹，执行命令 npm install 即创建了一个原始的hexo博客系统执行命令 hexo g 生成站点文件执行命令 hexo d 把博客部署到站点 运行机制&emsp;&emsp;首先我们来分析一下hexo文件夹的结构 _config.yml:站点的配置文件 db.json:缓存文件 node_modules:安装的插件以及hexo所需要的一些node.js模块 package.json：应用程序信息，配置hexo运行需要的js包 public：生成的站点文件 scaffolds：模板文件夹，新建文章时，会默认包含对应模板内容 source：资源文件夹是存放用户资源的地方。所有的源文件都会被保存在_post文件夹中 themes：hexo站点所使用的主题 &emsp;&emsp;为了搞清楚hexo的运行机制，我们有必要了解一下hexo的模板引擎（hexo使用的模板引擎是ejs编写的），模板引擎的作用就是将界面与数据分离最简单的原理是将模板内容中指定的地方替换成数据，实现业务代码与逻辑代码分离。我们可以注意到，在hexo中，source文件夹和themes文件夹是同级的，我们可以将source文件夹理解为数据库，而主题文件夹相当于界面，当执行hexo g命令时，就相当于将数据嵌入到界面中，生成静态文件public。&emsp;&emsp;具体来说在hexo中，从markdown文件到生成html的过程中大致经历了两次渲染的过程： 通过解析markdown文件，并结合站点配置文件和source目录下的相关文件，生成相应的数据对象 将生成的数据对象嵌入到themes主题中的渲染引擎生成站点文件 HEXO NEXT主题美化&emsp;&emsp;基于hexo博客系统的主题有很多，你可以在这里找到你喜欢的主题。我的博客采用的是next主题，我个人觉得next主题看上去简洁大方，用起来很舒服。这里我不会详细的介绍next主题的配置过程，我会分享一些我在配置过程中遇到的一些问题。 next主题的官方网站详细阐述了主题的基本配置过程，我也是参照它一步步进行配置的 在配置主题之前推荐大家安装一款node.js的开发工具，有利于提高效率。我安装的是webstorm，它也支持markdown文件的编写，强烈推荐（这个公司提供的开发工具都很强大，pycharm也是这个公司的产品之一）。 在配置主题的过程中要注意区分两个配置文件，一个是主题的配置文件_config.yml，一个是站点的配置文件_config.yml。因为有些配置操作实在主题的配置文件中进行的，有的实在站点的配置文件进行的，一定不能弄混了。 推荐一篇next主题美化的博文 对主题进行配置时，我的建议是每修改一项之后都在本地运行一下（先运行hexo g命令，在运行hexo s命令，在浏览器中查看），看看有没有出错，这样我们可以及时找到出错的地方。 hexo的配置文件是yaml格式的，它通过缩进来表示层级关系，修改配置文件时要主要缩进问题 HEXO部署到GITHUB&emsp;&emsp;在对主题修改完成之后，下一步的工作就是将hexo部署到github pages。在部署之前，需要做好一些准备工作，具体的操作过程可以参考这篇博客。 注册github账号，添加ssh key 在github中新建仓库，仓库名为：username.github.io 修改站点配置文件的deploy选项 执行命令hexo deploy &emsp;&emsp;在这些操作过程中我们需要注意一些问题： 在deploy的过程中可能会出现速度过慢的问题，这是由于GFW对github的限制造成的，可以通过代理或者修改hosts文件来提高访问速度 有时候会因为一些莫名其妙的问题导致deploy失败，无法解决这一问题时，我们可以通过复制生成的public文件，通过git提交到远程仓库，为了使整个过程更加自动化，我们可以在根目录下写一个脚本文件deploy.sh 123456hexo generate cp -R public/* chaoge123456.github.io cd chaoge123456.github.io git add . git commit -m “update” git push origin master 每次提交更新时只需要在根目录下执行命令：./deploy.sh即可 提交搜索引擎&emsp;&emsp;部署完成之后，现在我们可以通过浏览器访问到我们的博客，但是还有一件非常重要的事我们需要去完成。虽然我们可以通过浏览器访问到我们的博客，但是我们无法通过搜索引擎搜索到我们的博客，所以我们需要将我们的博客地址提交给搜索引擎。这时我们需要注意，github屏蔽了百度搜索引擎的爬虫，这也就意味着通过百度是无法搜索到我们在github上的博客（googl不存在这样的问题）。所以为了在国内也能访问到我们的博客，我们需要将我们的博客托管到国内的类似于github的平台——coding（coding的博客地址和github的博客地址不一样，所以接下来我们需要做的是将github的博客地址提交给google，将coding的地址提交给百度）。部署到coding的流程跟github类似，为了将站点同时更新到coding和github，我们需要在站点配置文件下的deploy选项的repo同时添加github和coding的远程仓库123456deploy: type: git repo: github: git@github.com:chaoge123456/chaoge123456.github.io.git coding: git@git.coding.net:chao3236gmailco/chao3236gmailco.git branch: master 提交谷歌搜索引擎Google搜索引擎提交入口 提交百度搜索引擎百度搜索引擎入口&emsp;&emsp;将站点地址提交给搜索引擎的步骤也比较简单，具体操作可以参考这篇博文,这个过程中需要注意的问题是： 将验证文件提交给站点时，有人会认为，直接将验证文件放入public文件夹中然后执行hexo g和hexo d就可以将验证文件提交的远程仓库。这样做确实可以将验证文件提交的远程仓库，但是需要注意的是此时的验证文件经过了hexo渲染，和原来的验证文件已经不一致，这样的验证文件时无效的 正确的做法是通过git clone获得远程仓库，在将验证文件加入刚刚获得的远程文件，然后通过向远程仓库提交该文件 验证完毕后，要向搜索引擎提交站点地图，方便爬虫爬取站点 总结&emsp;&emsp;HEXO+GITHUB+CODING博客搭建大概就是这些流程，希望大家看了我的博客会有所收获。这是我博客上的第一篇博文，确实不容易，希望以后能好好坚持下去吧。好了，夜已深了，晚安世界！]]></content>
      <categories>
        <category>HEXO相关</category>
      </categories>
      <tags>
        <tag>HEXO部署和美化</tag>
        <tag>github pages</tag>
        <tag>建站教程</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[markdown书写规范]]></title>
    <url>%2Fmarkdown%E4%B9%A6%E5%86%99%E8%A7%84%E8%8C%83.html%2F</url>
    <content type="text"><![CDATA[NOTE: This is Traditional Chinese Edition Document ofMarkdown Syntax. If you are seeking for English EditionDocument. Please refer to Markdown: Syntax. Markdown: Syntax 概述 哲學 行內 HTML 特殊字元自動轉換 區塊元素 段落和換行 標題 區塊引言 清單 程式碼區塊 分隔線 區段元素 連結 強調 程式碼 圖片 其它 跳脫字元 自動連結 感謝 注意：這份文件是用 Markdown 寫的，你可以看看它的原始檔 。 概述 哲學 Markdown 的目標是實現「易讀易寫」。 不過最需要強調的便是它的可讀性。一份使用 Markdown 格式撰寫的文件應該可以直接以純文字發佈，並且看起來不會像是由許多標籤或是格式指令所構成。Markdown 語法受到一些既有 text-to-HTML 格式的影響，包括 Setext、atx、Textile、reStructuredText、Grutatext 和 EtText，然而最大靈感來源其實是純文字的電子郵件格式。 因此 Markdown 的語法全由標點符號所組成，並經過嚴謹慎選，是為了讓它們看起來就像所要表達的意思。像是在文字兩旁加上星號，看起來就像*強調*。Markdown 的清單看起來，嗯，就是清單。假如你有使用過電子郵件，區塊引言看起來就真的像是引用一段文字。 行內 HTML Markdown 的語法有個主要的目的：用來作為一種網路內容的寫作用語言。 Markdown 不是要來取代 HTML，甚至也沒有要和它相似，它的語法種類不多，只和 HTML 的一部分有關係，重點不是要創造一種更容易寫作 HTML 文件的語法，我認為 HTML 已經很容易寫了，Markdown 的重點在於，它能讓文件更容易閱讀、編寫。HTML 是一種發佈的格式，Markdown 是一種編寫的格式，因此，Markdown 的格式語法只涵蓋純文字可以涵蓋的範圍。 不在 Markdown 涵蓋範圍之外的標籤，都可以直接在文件裡面用 HTML 撰寫。不需要額外標註這是 HTML 或是 Markdown；只要直接加標籤就可以了。 只有區塊元素──比如 &lt;div&gt;、&lt;table&gt;、&lt;pre&gt;、&lt;p&gt; 等標籤，必須在前後加上空行，以利與內容區隔。而且這些（元素）的開始與結尾標籤，不可以用 tab 或是空白來縮排。Markdown 的產生器有智慧型判斷，可以避免在區塊標籤前後加上沒有必要的 &lt;p&gt; 標籤。 舉例來說，在 Markdown 文件裡加上一段 HTML 表格： This is a regular paragraph. &lt;table&gt; &lt;tr&gt; &lt;td&gt;Foo&lt;/td&gt; &lt;/tr&gt; &lt;/table&gt; This is another regular paragraph. 請注意，Markdown 語法在 HTML 區塊標籤中將不會被進行處理。例如，你無法在 HTML 區塊內使用 Markdown 形式的*強調*。 HTML 的區段標籤如 &lt;span&gt;、&lt;cite&gt;、&lt;del&gt; 則不受限制，可以在 Markdown 的段落、清單或是標題裡任意使用。依照個人習慣，甚至可以不用Markdown 格式，而採用 HTML 標籤來格式化。舉例說明：如果比較喜歡 HTML 的 &lt;a&gt; 或 &lt;img&gt; 標籤，可以直接使用這些標籤，而不用 Markdown 提供的連結或是影像標示語法。 HTML 區段標籤和區塊標籤不同，在區段標籤的範圍內， Markdown 的語法是有效的。 特殊字元自動轉換 在 HTML 文件中，有兩個字元需要特殊處理： &lt; 和 &amp; 。 &lt; 符號用於起始標籤，&amp; 符號則用於標記 HTML 實體，如果你只是想要使用這些符號，你必須要使用實體的形式，像是 &amp;lt; 和 &amp;amp;。 &amp; 符號其實很容易讓寫作網路文件的人感到困擾，如果你要打「AT&amp;T」 ，你必須要寫成「AT&amp;amp;T」 ，還得轉換網址內的 &amp; 符號，如果你要連結到： http://images.google.com/images?num=30&amp;q=larry+bird 你必須要把網址轉成： http://images.google.com/images?num=30&amp;amp;q=larry+bird 才能放到連結標籤的 href 屬性裡。不用說也知道這很容易忘記，這也可能是 HTML 標準檢查所檢查到的錯誤中，數量最多的。 Markdown 允許你直接使用這些符號，但是你要小心跳脫字元的使用，如果你是在HTML 實體中使用 &amp; 符號的話，它不會被轉換，而在其它情形下，它則會被轉換成 &amp;amp;。所以你如果要在文件中插入一個著作權的符號，你可以這樣寫： &amp;copy; Markdown 將不會對這段文字做修改，但是如果你這樣寫： AT&amp;T Markdown 就會將它轉為： AT&amp;amp;T 類似的狀況也會發生在 &lt; 符號上，因為 Markdown 支援 行內 HTML ，如果你是使用 &lt; 符號作為 HTML 標籤使用，那 Markdown 也不會對它做任何轉換，但是如果你是寫： 4 &lt; 5 Markdown 將會把它轉換為： 4 &amp;lt; 5 不過需要注意的是，code 範圍內，不論是行內還是區塊， &lt; 和 &amp; 兩個符號都一定會被轉換成 HTML 實體，這項特性讓你可以很容易地用 Markdown 寫 HTML code （和 HTML 相對而言， HTML 語法中，你要把所有的 &lt; 和 &amp; 都轉換為 HTML 實體，才能在 HTML 文件裡面寫出 HTML code。） 區塊元素 段落和換行 一個段落是由一個以上相連接的行句組成，而一個以上的空行則會切分出不同的段落（空行的定義是顯示上看起來像是空行，便會被視為空行。比方說，若某一行只包含空白和 tab，則該行也會被視為空行），一般的段落不需要用空白或斷行縮排。 「一個以上相連接的行句組成」這句話其實暗示了 Markdown 允許段落內的強迫斷行，這個特性和其他大部分的 text-to-HTML 格式不一樣（包括 MovableType 的「Convert Line Breaks」選項），其它的格式會把每個斷行都轉成 &lt;br /&gt; 標籤。 如果你真的想要插入 &lt;br /&gt; 標籤的話，在行尾加上兩個以上的空白，然後按 enter。 是的，這確實需要花比較多功夫來插入 &lt;br /&gt; ，但是「每個換行都轉換為 &lt;br /&gt;」的方法在 Markdown 中並不適合， Markdown 中 email 式的 區塊引言 和多段落的 清單 在使用換行來排版的時候，不但更好用，還更好閱讀。 標題 Markdown 支援兩種標題的語法，Setext 和 atx 形式。 Setext 形式是用底線的形式，利用 = （最高階標題）和 - （第二階標題），例如： This is an H1 ============= This is an H2 ------------- 任何數量的 = 和 - 都可以有效果。 Atx 形式則是在行首插入 1 到 6 個 # ，對應到標題 1 到 6 階，例如： # This is an H1 ## This is an H2 ###### This is an H6 你可以選擇性地「關閉」atx 樣式的標題，這純粹只是美觀用的，若是覺得這樣看起來比較舒適，你就可以在行尾加上 #，而行尾的 # 數量也不用和開頭一樣（行首的井字數量決定標題的階數）： # This is an H1 # ## This is an H2 ## ### This is an H3 ###### Blockquotes Markdown 使用 email 形式的區塊引言，如果你很熟悉如何在 email 信件中引言，你就知道怎麼在 Markdown 文件中建立一個區塊引言，那會看起來像是你強迫斷行，然後在每行的最前面加上 &gt; ： &gt; This is a blockquote with two paragraphs. Lorem ipsum dolor sit amet, &gt; consectetuer adipiscing elit. Aliquam hendrerit mi posuere lectus. &gt; Vestibulum enim wisi, viverra nec, fringilla in, laoreet vitae, risus. &gt; &gt; Donec sit amet nisl. Aliquam semper ipsum sit amet velit. Suspendisse &gt; id sem consectetuer libero luctus adipiscing. Markdown 也允許你只在整個段落的第一行最前面加上 &gt; ： &gt; This is a blockquote with two paragraphs. Lorem ipsum dolor sit amet, consectetuer adipiscing elit. Aliquam hendrerit mi posuere lectus. Vestibulum enim wisi, viverra nec, fringilla in, laoreet vitae, risus. &gt; Donec sit amet nisl. Aliquam semper ipsum sit amet velit. Suspendisse id sem consectetuer libero luctus adipiscing. 區塊引言可以有階層（例如：引言內的引言），只要根據層數加上不同數量的 &gt; ： &gt; This is the first level of quoting. &gt; &gt; &gt; This is nested blockquote. &gt; &gt; Back to the first level. 引言的區塊內也可以使用其他的 Markdown 語法，包括標題、清單、程式碼區塊等： &gt; ## This is a header. &gt; &gt; 1. This is the first list item. &gt; 2. This is the second list item. &gt; &gt; Here&apos;s some example code: &gt; &gt; return shell_exec(&quot;echo $input | $markdown_script&quot;); 任何標準的文字編輯器都能簡單地建立 email 樣式的引言，例如 BBEdit ，你可以選取文字後然後從選單中選擇增加引言階層。 清單 Markdown 支援有序清單和無序清單。 無序清單使用星號、加號或是減號作為清單標記： * Red * Green * Blue 等同於： + Red + Green + Blue 也等同於： - Red - Green - Blue 有序清單則使用數字接著一個英文句點： 1. Bird 2. McHale 3. Parish 很重要的一點是，你在清單標記上使用的數字並不會影響輸出的 HTML 結果，上面的清單所產生的 HTML 標記為： &lt;ol&gt; &lt;li&gt;Bird&lt;/li&gt; &lt;li&gt;McHale&lt;/li&gt; &lt;li&gt;Parish&lt;/li&gt; &lt;/ol&gt; 如果你的清單標記寫成： 1. Bird 1. McHale 1. Parish 或甚至是： 3. Bird 1. McHale 8. Parish 你都會得到完全相同的 HTML 輸出。重點在於，你可以讓 Markdown 文件的清單數字和輸出的結果相同，或是你懶一點，你可以完全不用在意數字的正確性。 如果你使用懶惰的寫法，建議第一個項目最好還是從 1. 開始，因為 Markdown 未來可能會支援有序清單的 start 屬性。 清單項目標記通常是放在最左邊，但是其實也可以縮排，最多三個空白，項目標記後面則一定要接著至少一個空白或 tab。 要讓清單看起來更漂亮，你可以把內容用固定的縮排整理好： * Lorem ipsum dolor sit amet, consectetuer adipiscing elit. Aliquam hendrerit mi posuere lectus. Vestibulum enim wisi, viverra nec, fringilla in, laoreet vitae, risus. * Donec sit amet nisl. Aliquam semper ipsum sit amet velit. Suspendisse id sem consectetuer libero luctus adipiscing. 但是如果你很懶，那也不一定需要： * Lorem ipsum dolor sit amet, consectetuer adipiscing elit. Aliquam hendrerit mi posuere lectus. Vestibulum enim wisi, viverra nec, fringilla in, laoreet vitae, risus. * Donec sit amet nisl. Aliquam semper ipsum sit amet velit. Suspendisse id sem consectetuer libero luctus adipiscing. 如果清單項目間用空行分開， Markdown 會把項目的內容在輸出時用 &lt;p&gt;標籤包起來，舉例來說： * Bird * Magic 會被轉換為： &lt;ul&gt; &lt;li&gt;Bird&lt;/li&gt; &lt;li&gt;Magic&lt;/li&gt; &lt;/ul&gt; 但是這個： * Bird * Magic 會被轉換為： &lt;ul&gt; &lt;li&gt;&lt;p&gt;Bird&lt;/p&gt;&lt;/li&gt; &lt;li&gt;&lt;p&gt;Magic&lt;/p&gt;&lt;/li&gt; &lt;/ul&gt; 清單項目可以包含多個段落，每個項目下的段落都必須縮排 4 個空白或是一個 tab ： 1. This is a list item with two paragraphs. Lorem ipsum dolor sit amet, consectetuer adipiscing elit. Aliquam hendrerit mi posuere lectus. Vestibulum enim wisi, viverra nec, fringilla in, laoreet vitae, risus. Donec sit amet nisl. Aliquam semper ipsum sit amet velit. 2. Suspendisse id sem consectetuer libero luctus adipiscing. 如果你每行都有縮排，看起來會看好很多，當然，再次地，如果你很懶惰，Markdown 也允許： * This is a list item with two paragraphs. This is the second paragraph in the list item. You&apos;re only required to indent the first line. Lorem ipsum dolor sit amet, consectetuer adipiscing elit. * Another item in the same list. 如果要在清單項目內放進引言，那 &gt; 就需要縮排： * A list item with a blockquote: &gt; This is a blockquote &gt; inside a list item. 如果要放程式碼區塊的話，該區塊就需要縮排兩次，也就是 8 個空白或是兩個 tab： * A list item with a code block: &lt;code goes here&gt; 當然，項目清單很可能會不小心產生，像是下面這樣的寫法： 1986. What a great season. 換句話說，也就是在行首出現數字-句點-空白，要避免這樣的狀況，你可以在句點前面加上反斜線。 1986\. What a great season. 程式碼區塊 和程式相關的寫作或是標籤語言原始碼通常會有已經排版好的程式碼區塊，通常這些區塊我們並不希望它以一般段落文件的方式去排版，而是照原來的樣子顯示，Markdown 會用 &lt;pre&gt; 和 &lt;code&gt; 標籤來把程式碼區塊包起來。 要在 Markdown 中建立程式碼區塊很簡單，只要簡單地縮排 4 個空白或是 1 個 tab 就可以，例如，下面的輸入： This is a normal paragraph: This is a code block. Markdown 會轉換成： &lt;p&gt;This is a normal paragraph:&lt;/p&gt; &lt;pre&gt;&lt;code&gt;This is a code block. &lt;/code&gt;&lt;/pre&gt; 這個每行一階的縮排（4 個空白或是 1 個 tab），都會被移除，例如： Here is an example of AppleScript: tell application &quot;Foo&quot; beep end tell 會被轉換為： &lt;p&gt;Here is an example of AppleScript:&lt;/p&gt; &lt;pre&gt;&lt;code&gt;tell application &quot;Foo&quot; beep end tell &lt;/code&gt;&lt;/pre&gt; 一個程式碼區塊會一直持續到沒有縮排的那一行（或是文件結尾）。 在程式碼區塊裡面， &amp; 、 &lt; 和 &gt; 會自動轉成 HTML 實體，這樣的方式讓你非常容易使用 Markdown 插入範例用的 HTML 原始碼，只需要複製貼上，再加上縮排就可以了，剩下的 Markdown 都會幫你處理，例如： &lt;div class=&quot;footer&quot;&gt; &amp;copy; 2004 Foo Corporation &lt;/div&gt; 會被轉換為： &lt;pre&gt;&lt;code&gt;&amp;lt;div class=&quot;footer&quot;&amp;gt; &amp;amp;copy; 2004 Foo Corporation &amp;lt;/div&amp;gt; &lt;/code&gt;&lt;/pre&gt; 程式碼區塊中，一般的 Markdown 語法不會被轉換，像是星號便只是星號，這表示你可以很容易地以 Markdown 語法撰寫 Markdown 語法相關的文件。 分隔線 你可以在一行中用三個或以上的星號、減號、底線來建立一個分隔線，行內不能有其他東西。你也可以在星號中間插入空白。下面每種寫法都可以建立分隔線： * * * *** ***** - - - --------------------------------------- 區段元素 連結 Markdown 支援兩種形式的連結語法： 行內和參考兩種形式。 不管是哪一種，連結的文字都是用 [方括號] 來標記。 要建立一個行內形式的連結，只要在方塊括號後面馬上接著括號並插入網址連結即可，如果你還想要加上連結的 title 文字，只要在網址後面，用雙引號把 title 文字包起來即可，例如： This is [an example](http://example.com/ &quot;Title&quot;) inline link. [This link](http://example.net/) has no title attribute. 會產生： &lt;p&gt;This is &lt;a href=&quot;http://example.com/&quot; title=&quot;Title&quot;&gt; an example&lt;/a&gt; inline link.&lt;/p&gt; &lt;p&gt;&lt;a href=&quot;http://example.net/&quot;&gt;This link&lt;/a&gt; has no title attribute.&lt;/p&gt; 如果你是要連結到同樣主機的資源，你可以使用相對路徑： See my [About](/about/) page for details. 參考形式的連結使用另外一個方括號接在連結文字的括號後面，而在第二個方括號裡面要填入用以辨識連結的標籤： This is [an example][id] reference-style link. 你也可以選擇性地在兩個方括號中間加上空白： This is [an example] [id] reference-style link. 接著，在文件的任意處，你可以把這個標籤的連結內容定義出來： [id]: http://example.com/ &quot;Optional Title Here&quot; 連結定義的形式為： 方括號，裡面輸入連結的辨識用標籤 接著一個冒號 接著一個以上的空白或 tab 接著連結的網址 選擇性地接著 title 內容，可以用單引號、雙引號或是括弧包著 下面這三種連結的定義都是相同： [foo]: http://example.com/ &quot;Optional Title Here&quot; [foo]: http://example.com/ &apos;Optional Title Here&apos; [foo]: http://example.com/ (Optional Title Here) 請注意：有一個已知的問題是 Markdown.pl 1.0.1 會忽略單引號包起來的連結 title。 連結網址也可以用方括號包起來： [id]: &lt;http://example.com/&gt; &quot;Optional Title Here&quot; 你也可以把 title 屬性放到下一行，也可以加一些縮排，網址太長的話，這樣會比較好看： [id]: http://example.com/longish/path/to/resource/here &quot;Optional Title Here&quot; 網址定義只有在產生連結的時候用到，並不會直接出現在文件之中。 連結辨識標籤可以有字母、數字、空白和標點符號，但是並不區分大小寫，因此下面兩個連結是一樣的： [link text][a] [link text][A] 預設的連結標籤功能讓你可以省略指定連結標籤，這種情形下，連結標籤和連結文字會視為相同，要用預設連結標籤只要在連結文字後面加上一個空的方括號，如果你要讓 “Google” 連結到 google.com，你可以簡化成： [Google][] 然後定義連結內容： [Google]: http://google.com/ 由於連結文字可能包含空白，所以這種簡化的標籤內也可以包含多個文字： Visit [Daring Fireball][] for more information. 然後接著定義連結： [Daring Fireball]: http://daringfireball.net/ 連結的定義可以放在文件中的任何一個地方，我比較偏好直接放在連結出現段落的後面，你也可以把它放在文件最後面，就像是註解一樣。 下面是一個參考式連結的範例： I get 10 times more traffic from [Google] [1] than from [Yahoo] [2] or [MSN] [3]. [1]: http://google.com/ &quot;Google&quot; [2]: http://search.yahoo.com/ &quot;Yahoo Search&quot; [3]: http://search.msn.com/ &quot;MSN Search&quot; 如果改成用連結名稱的方式寫： I get 10 times more traffic from [Google][] than from [Yahoo][] or [MSN][]. [google]: http://google.com/ &quot;Google&quot; [yahoo]: http://search.yahoo.com/ &quot;Yahoo Search&quot; [msn]: http://search.msn.com/ &quot;MSN Search&quot; 上面兩種寫法都會產生下面的 HTML。 &lt;p&gt;I get 10 times more traffic from &lt;a href=&quot;http://google.com/&quot; title=&quot;Google&quot;&gt;Google&lt;/a&gt; than from &lt;a href=&quot;http://search.yahoo.com/&quot; title=&quot;Yahoo Search&quot;&gt;Yahoo&lt;/a&gt; or &lt;a href=&quot;http://search.msn.com/&quot; title=&quot;MSN Search&quot;&gt;MSN&lt;/a&gt;.&lt;/p&gt; 下面是用行內形式寫的同樣一段內容的 Markdown 文件，提供作為比較之用： I get 10 times more traffic from [Google](http://google.com/ &quot;Google&quot;) than from [Yahoo](http://search.yahoo.com/ &quot;Yahoo Search&quot;) or [MSN](http://search.msn.com/ &quot;MSN Search&quot;). 參考式的連結其實重點不在於它比較好寫，而是它比較好讀，比較一下上面的範例，使用參考式的文章本身只有 81 個字元，但是用行內形式的連結卻會增加到 176 個字元，如果是用純 HTML 格式來寫，會有 234 個字元，在 HTML 格式中，標籤比文字還要多。 使用 Markdown 的參考式連結，可以讓文件更像是瀏覽器最後產生的結果，讓你可以把一些標記相關的資訊移到段落文字之外，你就可以增加連結而不讓文章的閱讀感覺被打斷。 強調 Markdown 使用星號（*）和底線（_）作為標記強調字詞的符號，被 * 或 _ 包圍的字詞會被轉成用 &lt;em&gt; 標籤包圍，用兩個 * 或 _ 包起來的話，則會被轉成 &lt;strong&gt;，例如： *single asterisks* _single underscores_ **double asterisks** __double underscores__ 會轉成： &lt;em&gt;single asterisks&lt;/em&gt; &lt;em&gt;single underscores&lt;/em&gt; &lt;strong&gt;double asterisks&lt;/strong&gt; &lt;strong&gt;double underscores&lt;/strong&gt; 你可以隨便用你喜歡的樣式，唯一的限制是，你用什麼符號開啟標籤，就要用什麼符號結束。 強調也可以直接插在文字中間： un*frigging*believable 但是如果你的 * 和 _ 兩邊都有空白的話，它們就只會被當成普通的符號。 如果要在文字前後直接插入普通的星號或底線，你可以用反斜線： \*this text is surrounded by literal asterisks\* 程式碼 如果要標記一小段行內程式碼，你可以用反引號把它包起來（`），例如： Use the `printf()` function. 會產生： &lt;p&gt;Use the &lt;code&gt;printf()&lt;/code&gt; function.&lt;/p&gt; 如果要在程式碼區段內插入反引號，你可以用多個反引號來開啟和結束程式碼區段： ``There is a literal backtick (`) here.`` 這段語法會產生： &lt;p&gt;&lt;code&gt;There is a literal backtick (`) here.&lt;/code&gt;&lt;/p&gt; 程式碼區段的起始和結束端都可以放入一個空白，起始端後面一個，結束端前面一個，這樣你就可以在區段的一開始就插入反引號： A single backtick in a code span: `` ` `` A backtick-delimited string in a code span: `` `foo` `` 會產生： &lt;p&gt;A single backtick in a code span: &lt;code&gt;`&lt;/code&gt;&lt;/p&gt; &lt;p&gt;A backtick-delimited string in a code span: &lt;code&gt;`foo`&lt;/code&gt;&lt;/p&gt; 在程式碼區段內，&amp; 和方括號都會被轉成 HTML 實體，這樣會比較容易插入 HTML 原始碼，Markdown 會把下面這段： Please don&apos;t use any `&lt;blink&gt;` tags. 轉為： &lt;p&gt;Please don&apos;t use any &lt;code&gt;&amp;lt;blink&amp;gt;&lt;/code&gt; tags.&lt;/p&gt; 你也可以這樣寫： `&amp;#8212;` is the decimal-encoded equivalent of `&amp;mdash;`. 以產生： &lt;p&gt;&lt;code&gt;&amp;amp;#8212;&lt;/code&gt; is the decimal-encoded equivalent of &lt;code&gt;&amp;amp;mdash;&lt;/code&gt;.&lt;/p&gt; 圖片 很明顯地，要在純文字應用中設計一個 「自然」的語法來插入圖片是有一定難度的。 Markdown 使用一種和連結很相似的語法來標記圖片，同樣也允許兩種樣式： 行內和參考。 行內圖片的語法看起來像是： ![Alt text](/path/to/img.jpg) ![Alt text](/path/to/img.jpg &quot;Optional title&quot;) 詳細敘述如下： 一個驚嘆號 ! 接著一對方括號，裡面放上圖片的替代文字 接著一對普通括號，裡面放上圖片的網址，最後還可以用引號包住並加上選擇性的 ‘title’ 文字。 參考式的圖片語法則長得像這樣： ![Alt text][id] 「id」是圖片參考的名稱，圖片參考的定義方式則和連結參考一樣： [id]: url/to/image &quot;Optional title attribute&quot; 到目前為止， Markdown 還沒有辦法指定圖片的寬高，如果你需要的話，你可以使用普通的 &lt;img&gt; 標籤。 其它 自動連結 Markdown 支援比較簡短的自動連結形式來處理網址和電子郵件信箱，只要是用方括號包起來， Markdown 就會自動把它轉成連結，連結的文字就和連結位置一樣，例如： &lt;http://example.com/&gt; Markdown 會轉為： &lt;a href=&quot;http://example.com/&quot;&gt;http://example.com/&lt;/a&gt; 自動的郵件連結也很類似，只是 Markdown 會先做一個編碼轉換的過程，把文字字元轉成 16 進位碼的 HTML 實體，這樣的格式可以混淆一些不好的信箱地址收集機器人，例如： &lt;address@example.com&gt; Markdown 會轉成： &lt;a href=&quot;&amp;#x6D;&amp;#x61;i&amp;#x6C;&amp;#x74;&amp;#x6F;:&amp;#x61;&amp;#x64;&amp;#x64;&amp;#x72;&amp;#x65; &amp;#115;&amp;#115;&amp;#64;&amp;#101;&amp;#120;&amp;#x61;&amp;#109;&amp;#x70;&amp;#x6C;e&amp;#x2E;&amp;#99;&amp;#111; &amp;#109;&quot;&gt;&amp;#x61;&amp;#x64;&amp;#x64;&amp;#x72;&amp;#x65;&amp;#115;&amp;#115;&amp;#64;&amp;#101;&amp;#120;&amp;#x61; &amp;#109;&amp;#x70;&amp;#x6C;e&amp;#x2E;&amp;#99;&amp;#111;&amp;#109;&lt;/a&gt; 在瀏覽器裡面，這段字串會變成一個可以點擊的「address@example.com」連結。 （這種作法雖然可以混淆不少的機器人，但並無法全部擋下來，不過這樣也比什麼都不做好些。無論如何，公開你的信箱終究會引來廣告信件的。） 跳脫字元 Markdown 可以利用反斜線來插入一些在語法中有其它意義的符號，例如：如果你想要用星號加在文字旁邊的方式來做出強調效果（但不用 &lt;em&gt; 標籤），你可以在星號的前面加上反斜線： \*literal asterisks\* Markdown 支援在下面這些符號前面加上反斜線來幫助插入普通的符號： \ 反斜線 ` 反引號 * 星號 _ 底線 {} 大括號 [] 方括號 () 括號 # 井字號 + 加號 - 減號 . 英文句點 ! 驚嘆號]]></content>
      <categories>
        <category>博客写作</category>
      </categories>
      <tags>
        <tag>markdown</tag>
      </tags>
  </entry>
</search>
