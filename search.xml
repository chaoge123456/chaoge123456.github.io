<?xml version="1.0" encoding="utf-8"?>
<search>
  <entry>
    <title><![CDATA[生成对抗网络解读]]></title>
    <url>%2F%E7%94%9F%E6%88%90%E5%AF%B9%E6%8A%97%E7%BD%91%E7%BB%9C.html%2F</url>
    <content type="text"><![CDATA[摘要：生成对抗网络（ Generative Adversarial Networks， GAN）是通过对抗训练的方式来使得生成网络产生的样本服从真实数据分布。在生成对抗网络中，有两个网络进行对抗训练。一个是判别器，目标是尽量准确地判断一个样本是来自于真实数据还是由生成器产生；另一个是生成器，目标是尽量生成判别网络无法区分来源的样本 。两者交替训练，当判别器无法判断一个样本是真实数据还是生成数据时，生成器即达到收敛状态。以上是对生成对抗网络的简单描述，本文将对生成对抗网络的内在原理以及相应的优化机制进行介绍。 文章概览 概率生成模型 生成对抗网络 生成对抗网络的理论解释 生成对抗网络的求解过程 生成对抗网络的优化 fGAN WGAN 生成对抗网络的实现 概率生成模型&emsp;&emsp;概率生成模型，简称生成模型，是指一系列用于随机生成可观测数据的模型。假设在一个连续或离散的空间$\chi$中，存在一个随机向量$X$服从一个未知的数据分布$P_{data}(x)$，$x \in \chi$。生成模型是根据一些可观测的样本$x^1, x^2, …, x^m$来学习m一个参数化模型$P_G(\theta;x)$来近似未知分布，并可以用这$P_{data}(x)$个模型来生成一些样本，使得生成的样本和真实的样本尽可能的相似。对于一个低维空间中的简单分布而言，我们可以采用最大似然估计的方法来对$p_\theta(x)$进行求解。假设我们要统计全国人民的年均收入的分布情况，如果我们对每个$P_{data}(x)$样本都进行统计，这将消耗大量的人力物力。为了得到近似准确的收入分布情况，我们可以先假设其服从高斯分布，我们比如选取某个$P_G(x;\theta)$城市的人口收入$x^1, x^2, …, x^m$，作为我们的观察样本结果，然后通过最大似然估计来计算上述假设中的高斯分布的参数。$$L=\prod_{i=1}^{m} P_{G}\left(x^{i} ; \theta\right)$$ $${\theta}^{*}=arg \max_{\theta} \sum_{i=1}^{m} \log P_{G}(x^{i} ; \theta)$$ 由于$P_G(x;\theta)$服从高斯分布，我们将其带入即可求得最终的近似的分布情况。下面我们对上述过程进行一些拓展，我们从$P_{data}(x)$尽可能采样更多的数据，此时可以得到$${\theta}^{*}=arg \max_{\theta} \sum_{i=1}^{m} \log P_{G}(x^{i} ;\theta)\approx arg \max_{\theta} E_{x \sim P_{\text {data }}}[\log P_{G}(x ; \theta)]$$ 对该式进行一些变换，可以得到$${\theta}^{*}=arg \min_{\theta} K L\left(P_{\text {data }} | P_{G}\right)$$ &emsp;&emsp;由此可以看出，最大似然估计的过程其实就是最小化$P_{data}(x)$分布和$P_G $分布之间$KL$散度的过程。从本质上讲，所有的生成模型的问题都可以转换成最小化$P_{data}(x)$分布和$P_G $分布之间距离的问题，$KL$散度只是其中一种度量方式。 &emsp;&emsp;如上所述，对于低维空间的简单分布而言，我们可以显式的假设样本服从某种类型的分布，然后通过极大似然估计来进行求解。但是对于高维空间的复杂分布而言，我们无法假设样本的分布类型，因此无法采用极大似然估计来进行求解，生成对抗网络即属于这样一类生成模型。 生成对抗网络生成对抗网络的理论解释&emsp;&emsp;在生成对抗网络中，我们假设低维空间中样本$z$服从标准类型分布，利用神经网络可以构造一个映射函数$G$（即生成器）将$z$映射到真实样本空间。我们希望映射函数$G$能够使得$P_G(x)$分布尽可能接近$P_{data}(x)$分布，即$P_G$与$P_{data}$之间的距离越小越好：$$G^{*}=arg \min_{G} {\operatorname{Div}}\left(P_{G}, P_{\text {data }}\right)$$ 由于$P_G$与$P_{data}$的分布都是未知的，所以无法直接求解$P_G$与$P_{data}$之间的距离。生成对抗网络借助判别器来解决这一问题。首先我们分别从$P_G$与$P_{data}$中取样，利用取出的样本训练一个判别器：我们希望当输入样本为$P_{data}$时，判别器会给出一个较高的分数；当输入样本为$P_G$时，判别器会给出一个较低的分数。例如，我们可以将判别器的目标函数定义成以下形式（与二分类的目标函数一致，即交叉熵）：$$V(G, D)=E_{x \sim P_{\text {data }}}[\log D(x)]+E_{x \sim P_{G}}[\log (1-D(x))]$$ 我们希望得到这样一个判别器（$G$固定）：$$D^{*}=arg \max_{D} V(D, G)$$ 从本质上来看，$\max _{D} V(D, G)$即表示$P_G$与$P_{data}$之间的$JS$散度（具体推导参见李宏毅老师的课程），即：$$\max_{D} V(G, D)=V\left(G, D^{*}\right)=-2 \log 2+2 J S D\left(P_{\text {data }} | P_{G}\right)$$ $$D^{*}(x)=\frac{P_{\text {data }}(x)}{P_{\text {data }}(x)+P_{G}(x)}$$ 因此通过构建判别器可以度量$P_G$与$P_{data}$之间的距离，所以$G^{*}$可以表示为：$$G^{*}=arg \min_{G} \max_{D} V(G, D)$$ 生成对抗网络的求解过程$G^{*}$的求解过程大致如下： 初始化生成器$G$和判别器$D$ 迭代训练 固定生成器$G$，更新判别器$D$的参数 固定生成器$D$，更新判别器$G$的参数 对上述算法过程进行几点说明： 在之前的描述中，$V(D,G)$表示的是目标函数的期望，但在实际计算过程中是通过采样平均的方式来逼近其期望值。 判别器的训练需要重复$k$次的原因是希望能尽可能使得$V(D,G)$接近最大值，这样才能满足$\max _{D} V(D, G)$即表示$P_G$与$P_{data}$之间的$JS$散度”这一假设。 在更新生成器参数时，$\frac{1}{m} \sum_{i=1}^{m} \log D\left(x^{i}\right)$这一项可以忽略，因为$D$固定，其相当于一个常数项。 在更新生成器参数时，我们使用$\tilde{V}=\frac{1}{m} \sum_{i=1}^{m} -\log \left(D\left(G\left(z^{i}\right)\right)\right)$代替$\tilde{V}=\frac{1}{m} \sum_{i=1}^{m} \log \left(1-D\left(G\left(z^{i}\right)\right)\right)$，这样做的目的是加速训练过程。 生成对抗网络的优化fGAN&emsp;&emsp;通过上面的分析我们可以知道，构建生成模型需要解决的关键问题是最小化$P_G$和$P_{data}$之间的距离，这就涉及到如何对$P_G$和$P_{data}$之间的距离进行度量。在上述GAN的分析中，我们通过构建一个判别器来对$P_G$和$P_{data}$之间的距离进行度量，其中采用的目标函数为：$$V(G, D)=E_{x \sim P_{\text {data }}}[\log D(x)]+E_{x \sim P_{G}}[\log (1-D(x))]$$ 通过证明可知，$V(G, D)$其实度量的是$P_G$和$P_{data}$之间的$JS$散度。如果我们希望采用其他方式来衡量两个分布之间的距离，则需要对判别器的目标函数进行修改。根据论文fGAN，可以将判别器的目标函数定义成如下形式：$$D_{f^{*}}(P_{\text {data }} | P_{G})=\max_{\mathrm{D}}{E_{x \sim P_{\text {data }}}[D(x)]-E_{x \sim P_{G}}[f^{*}(D(x)]}$$ 则$G^{*}$可以表示为：$$G^{*}=arg \min_{G} D_{f^{*}}\left(P_{\text {data }} | P_{G}\right)$$ $f^{*}$取不同表达式时，即表示不同的距离度量方式。 令$f^{*}(t)=-log(1-exp(t))$，$D(x)$取$log$，代入$D_{f^{*}}\left(P_{\text {data }} | P_{G}\right)$即可得到$V(G,D)$。 WGAN&emsp;&emsp;自2014年Goodfellow提出以来，GAN就存在着训练困难、生成器和判别器的loss无法指示训练进程、生成样本缺乏多样性等问题。针对这些问题，Martin Arjovsky进行了严密的理论分析，并提出了解决方案，即WGAN（WGAN的详细解读可参考这篇博客)。 判别器越好，生成器梯度消失越严重。根据上面的分析可知，当判别器训练到最优时，$\max _{D} V(D, G)$衡量的是$P_G$与$P_{data}$之间的$JS$散度。问题就出在这个JS散度上，我们希望如果两个分布之间越接近它们的JS散度越小，通过优化JS散度就能将$P_G$拉向$P_{adta}$。这个希望在两个分布有所重叠的时候是成立的，但是如果两个分布完全没有重叠的部分，或者它们重叠的部分可忽略，$J S D\left(P_{\text {data }} | P_{G}\right)=log2$。在训练过程中，$P_G$与$P_{data}$都是通过采样得到的，在高维空间中两者之间几乎不存在交集，从而导致$\max _{D} V(D, G)$接近于0，生成器因此也无法得到有效训练。 最小化生成器loss函数$E_{x \sim P_{G}}[\log (1-D(x))$，会等价于最小化一个不合理的距离衡量，导致两个问题，一是梯度不稳定，二是collapse mode即多样性不足。假设当前的判别器最优，经过推导可以得到下面等式：$$E_{x \sim P_{G}}[\log (1-D(x))=KL\left(P_{\text {G }} | P_{data}\right)-2 J S D\left(P_{\text {data }} | P_{G}\right)$$这个等价最小化目标存在两个严重的问题。第一是它同时要最小化生成分布与真实分布的KL散度，却又要最大化两者的JS散度，一个要拉近，一个却要推远！这在直观上非常荒谬，在数值上则会导致梯度不稳定，这是后面那个JS散度项的毛病。第二，即便是前面那个正常的KL散度项也有毛病，因为KL散度不是一个对称的衡量$KL\left(P_{\text {G }} | P_{data}\right)$和$KL\left(P_{\text {data}} | P_{G}\right)$是有差别的。 原始GAN的主要问题就出在距离度量方式上面，Martin Arjovsky提出利用Wasserstein距离来进行衡量。Wasserstein距离相比KL散度、JS散度的优越性在于，即便两个分布没有重叠，Wasserstein距离仍然能够反映它们的远近。$$W(G, D)=E_{x \sim P_{\text {data }}}[D(x)]-E_{x \sim P_{G}}[D(x)]\$$ &emsp;&emsp;由以上算法可以看出，WGAN与原始的GAN在算法实现方面只有四处不同：（1）判别器最后一层去掉sigmoid；（2）生成器和判别器的loss不取log；（3）每次更新判别器的参数之后把它们的绝对值截断到不超过一个固定常数c；（4）不要用基于动量的优化算法（包括momentum和Adam），推荐RMSProp，SGD也行。 生成对抗网络的实现&emsp;&emsp;本文实现了几种常见的生成对抗网络模型，包括原始GAN、CGAN、WGAN、DCGAN。开发环境为jupyter lab，所使用的深度学习框架为pytorch，并结合tensorboard动态观测生成器的训练效果，具体代码请参考我的github。 GAN12345678910111213141516171819202122232425real_label = torch.ones(batch_size, 1)fake_label = torch.zeros(batch_size, 1)# 训练判别器d_real = D(real_img)d_real_loss = criterion(d_real, real_label)z = torch.normal(0, 1, (batch_size, latent))fake_img = G(z)d_fake = D(fake_img)d_fake_loss = criterion(d_fake, fake_label)optimizer_D.zero_grad()d_loss = d_real_loss + d_fake_lossd_loss.backward()optimizer_D.step()# 训练生成器fake_img = G(z)d_fake = D(fake_img)g_loss = criterion(d_fake, real_label)optimizer_G.zero_grad()g_loss.backward()optimizer_G.step() CGAN1234567891011121314151617181920212223242526real_label = torch.ones(batch_size, 1)fake_label = torch.zeros(batch_size, 1)z = torch.normal(0, 1, (batch_size, latent))# 训练判别器d_real = D(real_img, label)d_real_loss = criterion(d_real, real_label)fake_img = G(z, label)d_fake = D(fake_img, label)d_fake_loss = criterion(d_fake, fake_label)optimizer_D.zero_grad()d_loss = (d_real_loss + d_fake_loss)d_loss.backward()optimizer_D.step()# 训练生成器fake_img = G(z, label)d_fake = D(fake_img, label)g_loss = criterion(d_fake, real_label)optimizer_G.zero_grad()g_loss.backward()optimizer_G.step() WGAN12345678910111213141516171819202122232425262728# 训练判别器d_real = D(real_img)#d_real_loss = criterion(d_real, real_label)d_real_loss = d_realz = torch.normal(0, 1, (batch_size, latent))fake_img = G(z)d_fake = D(fake_img)#d_fake_loss = criterion(d_fake, fake_label)d_fake_loss = d_fakeoptimizer_D.zero_grad()#d_loss = d_real_loss + d_fake_lossd_loss = torch.mean(d_fake_loss) - torch.mean(d_real_loss)d_loss.backward()optimizer_D.step()for p in D.parameters(): p.data.clamp_(-clip_value, clip_value)# 训练生成器fake_img = G(z)d_fake = D(fake_img)#g_loss = criterion(d_fake, real_label)g_loss = - torch.mean(d_fake)optimizer_G.zero_grad()g_loss.backward()optimizer_G.step()]]></content>
      <categories>
        <category>深度学习</category>
      </categories>
      <tags>
        <tag>生成对抗网络</tag>
        <tag>GAN</tag>
        <tag>WGAN</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[卷积神经网络实现解析]]></title>
    <url>%2F%E5%8D%B7%E7%A7%AF%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E5%AE%9E%E7%8E%B0%E8%A7%A3%E6%9E%90.html%2F</url>
    <content type="text"><![CDATA[摘要：卷积神经网络是受生物学中感受野机制的启发而提出的，是一种具有局部连接、权重共享等特性的深层前馈神经网络，其主要应用于图像和视频分析等领域。2012年，卷积神经网络在ImageNet大规模视觉识别挑战竞赛中大放异彩，一定程度上引领了深度学习袭卷全球的潮流。这篇文章聚焦于卷积神经网络的实现细节，本文我们将使用pytorch中的tensor实现一个简单的卷积神经网络框架（很多文章采用的是numpy实现），并在FashionMnist数据集中进行测试。文中给出实现过程中的部分代码，完整的代码可以在我的github中找到。 文章概览 卷积神经网络简介 卷积层的实现细节 前向计算 反向传播 池化层的实现细节 前向计算 反向传播 测试结果 卷积神经网络简介&emsp;&emsp;在前面的文章中我们讲到了全连接前馈网络，并且在FashionMnist数据集上对我们构建的网络框架进行了测试。简单回顾一下当时的数据处理过程，FashionMnist数据集中的图片通道数为1、图片尺寸为28×28，原始数据集表示为$ X=(N, 1, 28 ,28) $。在训练之前，我们需要将训练数据转化为 $ X=(N, 1*28*28) $，相当于将数据集中每一张图片的像素点展开成向量的形式。显而易见，将图片展开为向量会丢失空间信息，这会对模型的泛化性能产生很大的影响。除此之外，利用全连接前馈网络处理图像数据往往会需要很多的参数。举例来说，假设现在数据集中的图片为100×100的彩色图片，此时每张图片中包含的像素点为$ 3*100*100 $，即$ X=(N, 3*100*100) $。如果第一个隐藏层有1000个神经元，那么仅该层包含的参数个数为$ 30000*1000 + 1000 $。过多的参数会给模型的训练过程造成很大的负担，同时也会导致过拟合等问题。综上所述，一般情况下我们不采用全连接前馈网络来处理图像数据。 &emsp;&emsp;卷积神经网络最早主要是用来处理图像信息，是受生物学中感受野机制的启发而提出的（感受野是指卷积神经网络每一层输出的特征图上的像素点在输入图片上映射的区域大小。通俗点的解释是，特征图上的一个点对应输入图上的区域）。1998年，LeCun提出了经典的卷积网络模型LeNet-5，第一次较为完整的阐述了卷积神经网络的框架和结构。卷积神经网络由输入层、卷积层、激活层、池化层、全连接层及输出层构成。卷积层和池化层一般会取若干个，采用卷积层和池化层交替设置，即一个卷积层连接一个池化层，池化层后再连接一个卷积层，依此类推。与全连接前馈网络相比，卷积神经网络在结构上具有局部连接、 权重共享、降采样等特点，并且在训练过程中会完整保留数据的空间信息。这些特性使得卷积神经网络图像处理领域表现更加出色，并且使用的参数更少。 CNN层次结构 作用 输入层 卷积网络的原始输入，可以是原始或预处理后的像素矩阵 卷积层 参数共享、局部连接，利用平移不变性从全局特征图提取局部特征 激活层 将卷积层的输出结果进行非线性映射 池化层 进一步筛选特征，可以有效减少后续网络层次所需的参数量 全连接层 将多维特征展平为2维特征，通常低维度特征对应任务的学习目标（类别或回归值） 以下主要介绍卷积神经网络中卷积层和池化层的实现细节，激活层和全连接层的实现与全连接前馈网络基本一致，这里不再赘述，具体代码参考我的github（需要注意的是，本文实现代码中，默认输入图片高度和宽度相同）。 卷积层实现细节&emsp;&emsp;开始介绍之前，简要说明参数设置情况。输入数据尺寸为$s=5$，通道数为$in_channel=3$，即$ input=(n, 3, 5, 5)$；单个卷积核尺寸为$k=2$，卷积层输出通道数为$out_channel=10$，即$kernel=(10, 3, 2, 2)$；假设卷积步幅$stride=1$，$pad=0$，根据特征图大小计算公式$$p=\frac{s-k+2*pad}{stride}+1$$则卷积层输出为$output=(N, 10, 4, 4)$。 前向计算&emsp;&emsp;在卷积神经网络中所使用的卷积，一般是指互相关操作，其本质上就是利用卷积核在输入数据上进行滑动，并在滑动窗口内计算点积，计算过程如如下图所示。 &emsp;&emsp;如果将该操作拓展到多通道的情况，我们需要将每一个通道数据与其对应的卷积核分别进行互相关操作，每个通道都会得到一个输出，然后将所有输出相同位置相加，得到最终的特征图，计算过程如下所示。 &emsp;&emsp;这一过程最简单的思路是通过多重循环来实现，但是这种实现方式会极大降低训练过程的效率。im2col算法通过将这一过程转化为矩阵乘法的形式可以极大程度提升计算效率，该算法的主要思想如下图所示。 &emsp;&emsp;im2col的核心思想是将每个卷积核在其对应通道上滑动过程中滑动窗口位置上的数据重新排列，组合成一维行向量的形式，而卷积核自身则排列成列向量的形式。卷积层输出的特征图中的每一个值都能表示成上述行向量和列向量相乘的形式。如果含有多层卷积核，则如下图方式排列。 通过这种方式，我们的输入数据可以成$input=(n*p*p, k*k*in_channel)$，卷积核可表示为$kernel=(k*k*in_channel, out_channel)$。该过程的代码如下所示： 123456789101112131415def im2col(self, input): n, in_channel, s, _ = input.shape p = (s - self.k_size) // self.stride + 1 im = torch.zeros((n*p*p, k*k*in_channel)) for i in range(p): i_start = i*self.stride i_end = i_start + self.k_size for j in range(p): j_start = j*self.stride j_end = j_start + self.k_size im[i*p+j::p*p, :] = input[:,:,i_start:i_end,j_start:j_end].reshape(n, -1) w = self.w.reshape(self.k_size**2*c, self.out_channel) output = torch.matmul(im, w) + self.b return output.reshape(n, self.out_channel, p, p) 反向传播&emsp;&emsp;在反向传播过程中，卷积层主要做两件事：（1）利用卷积操作的输出层的误差项$\delta^l$来求卷积操作输入层的误差项$\delta^{l-1}$，并将结果继续反向传播；（2）求解卷积核的梯度，并对其进行更新（反向传播的过程可以参考这篇博客）。首先我们来求解输入层的误差项$\delta^{l-1}$，在将具体实现之前，我们先看下面这幅图 图的左侧表示输入，右侧表示输出。在输入数据中，数字0所标识的位置分别经历了四次卷积过程，分别对输出层的四个位置产生影响，卷积核中的4、3、2、1标识的是其每次参与运算的权重。如果我们现在需要计算0所在位置的误差项，只需要将卷积核中4、3、2、1四个权重与输出层对应位置的误差项进行点积运算即可。这种做法与互相关相同，但是需要注意的是这里的卷积核与之前的不同，是前向传播中的卷积核旋转180度的结果。我们给出输入层的误差项$\delta^{l-1}$的公式$$\delta^{l-1}=\delta^{l} * \operatorname{rot} 180\left(W^{l}\right)$$除此之外，由于输出层的尺寸要小于输入层，所以在进行卷积之前需要对输出层进行padding，如下图所示。 123456789101112131415161718192021def nexteta(self, grad): # 计算卷积输入层的误差项 n, c, s, _ = grad.shape ln, lc, ls, _ = self.param['inputshape'] pad = math.ceil((self.stride*(ls-1) + self.k_size - s) / 2) grad = self.paddings(grad, pad) w = torch.flip(self.w, dims=[2,3]) # 卷积核翻转 p = (s - self.k_size) // self.stride + 1 w = w.reshape(lc, self.k_size**2*c) im = self.im2col(grad, n, c, s, p) eta = torch.matmul(im, w.T).reshape(n, lc, p, p) return eta def paddings(self, input, pad): # padding实现接口 n, c, s, _ = input.shape outshape = pad*2 + s out = torch.zeros((n, c, outshape, outshape)) out[:,:,pad:outshape-pad,pad:outshape-pad] = input return out 接下来，我们要对卷积核进行更新。首先我们给出计算公式，$$\frac{\partial J(W, b)}{\partial W^{l}}=a^{l-1} * \delta^{l}$$其中$a^{l-1}$表示卷积层的输入， $\delta^{l}$表示卷积操作的输出层的误差项。根据前面的描述我们可以知道，$a^{l-1}$是一个$(n, 3, 5, 5)$的矩阵，$\delta^{l}$是$(n, 10, 4, 4)$的矩阵，而我们要求解的卷积核的梯度是$(10, 3, 2, 2)$的矩阵。大致的计算过程是这样的：将$\delta^{l}$视为卷积核，每次取第$i$通道$(n, i, 4, 4)$分别与对应数据的$(n, 3, 5, 5)$的三个通道进行卷积操作，由此可以得到$(n, 3, 2, 2)$，然后对$n$维数据进行求和取平均即得到$(1, 3, 2, 2)$。将此过程进行10次，即可得到卷积核的梯度值，具体代码如下所示。 12345678910111213141516def backward(self, input, grad): dw = torch.zeros(self.w.shape) db = torch.zeros(self.b.shape) n, c, s, _ = grad.shape for i in range(self.k_size): for j in range(self.k_size): cons = input[:,:,i*self.stride:i*self.stride+s,\n i*self.stride:i*self.stride+s] for k in range(c): dw[k,:,i,j] = torch.sum(cons * (grad[:, \n k, :, :])[:, None], axis=(0,2,3)) / n db = torch.sum(grad, axis=(0,2,3)) / n self.w -= self.learning_rate*dw # 更新权重 self.b -= self.learning_rate*db # 更新偏置 eta = self.nexteta(grad) return eta 池化层&emsp;&emsp;池化层又称为降采样层，作用是对感受域内的特征进行筛选，提取区域内最具代表性的特征，能够有效地降低输出特征尺度，进而减少模型所需要的参数量。按操作类型通常分为最大池化、平均池化和求和池化，它们分别提取感受域内最大、平均与总和的特征值作为输出，最常用的是最大池化（这里主要介绍最大池化）。 &emsp;&emsp;开始介绍之前，简要说明参数设置情况。输入数据尺寸为$s=4$，通道数为$in_channel=10$，即$ input=(n, 10, 4, 4)$；池化窗口尺寸为$k=2$，池化步幅$stride=2$，根据特征图大小计算公式$$p=\frac{s-k}{stride}$$则池化层输出为$output=(N, 10, 2, 2)$。 前向计算&emsp;&emsp;与卷积操作相比，池化层的计算较为简单，且池化层不改变通道数量，不包含训练参数。最大池化提取感受域内最大的值作为输出，计算过程如下图所示 需要注意的是，进行池化操作时需要记录感受域内最大值的位置，在反向传播时会用到，具体代码如下所示（因为需要记录位置，暂时没想到好的解决办法，所以直接用的循环）。 1234567891011121314151617def pool(self, input): self.param['shape'] = input.shape n, c, s, _ = input.shape p = (s - self.k_size) // self.stride + 1 pl = torch.zeros((n, c, p, p)) index = [] for i in range(n): for j in range(c): for k in range(p): for z in range(p): inp = input[i,j,k*self.stride:k*self.stride+self.k_size, z*self.stride:z*self.stride+self.k_size] pl[i,j,k,z] = torch.max(inp) ix = torch.where(inp==pl[i,j,k,z]) index.append((i,j,k*self.stride+ix[0],z*self.stride+ix[1])) self.param['index'] = index return pl 反向传播&emsp;&emsp;池化操作的反向传播主要是计算其输入层的误差项。首先我们定义一个全0矩阵，其大小与池化操作输入相同，对于最大池化而言，将池化操作输出层的误差项的值放在之前做前向传播算法得到最大值的位置，如下图所示。 1234567def backward(self, input, grad): # 反向传播 grad = grad.reshape(-1) eta = torch.zeros(self.param['shape']) for i in range(len(self.param['index'])): eta[self.param['index'][i]] = grad[i] return eta 测试结果&emsp;&emsp;我们定义了一个简单的卷积神经网络模型，并在FashionMnist做了一组测试。由于在池化层等操作上使用了循环操作，再加上笔记本性能不是特别好，跑了一个epoch，模型在测试集上的预测准确率达到了71.3%，但是花费的时间较长，大概40分钟，整体的代码确实需要做一些优化。 123456789learning_rate = 0.1network = []network.append(Convolutional(1,16,5,2,learning_rate=learning_rate))network.append(ReLU())network.append(MaxPooling(16,2,2))network.append(Dense(576,20,learning_rate=learning_rate))network.append(ReLU())network.append(Dense(20,10,learning_rate=learning_rate))network.append(Softmax()) &emsp;&emsp;总的来说，完整实现一个卷积神经网络还是有难度的。整个过程我觉得最难的是理解数据在卷积网络中的流动过程，因为这些数据都是以张量的形式呈现，所以理解起来有些困难。除此之外就是代码的调试，因为其中存在很多细节，一个地方出问题就会对整个结果产生影响。但是，经历这个过程之后，确实对卷积网络有了更多的认识。]]></content>
      <categories>
        <category>深度学习</category>
      </categories>
      <tags>
        <tag>卷积网络</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[深度神经网络实现解析]]></title>
    <url>%2F%E6%B7%B1%E5%BA%A6%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E5%AE%9E%E7%8E%B0%E8%A7%A3%E6%9E%90.html%2F</url>
    <content type="text"><![CDATA[摘要：最近一段时间在回顾深度学习的一些基本知识，感觉对有些内容的理解比较模糊，于是萌生了手动来实现的想法。其实类似的工作之前也做过，写过决策树、支持向量机、神经网络等，不过当时是用numpy写的。因为现在一直在使用pytorch，pytorch中的tensor与numpy中的array很相似，所以这次的代码主要使用tensor来实现。目前实现的代码包括逻辑回归、softmax回归、深度神经网络和卷积神经网络，所有的代码都可以在我的github中找到。这篇博客主要来记录在实现深度神经网络过程中的一些思路，以及遇到的问题。 文章概览 深度神经网络简介 深度神经网络框架实现 整体思路 前向计算 反向传播 交叉熵损失函数与softmax激活函数 深度神经网络简介&emsp;&emsp;网上对深度神经网络（DNN）介绍的文章很多，这里不再赘述。推荐一些资料：刘建平博客 、复旦大学邱锡鹏教授神经网络与深度学习。 深度神经网络框架实现整体思路&emsp;&emsp;在使用pytorch、tensorflow等框架来构建一个深度神经网络模型的时候，通常全连接层和激活函数层分开进行定义的。通过这种模块化的方式有利于自由的设计模型，在本文的代码中依然沿用这种方式。首先，我们需要定义一个父类，所有的全连接层以及激活函数层均继承自该父类。神经网络中的每一层都实现统一的方法接口，包括前向计算和反向传播，这样我们可以实现数据在神经网络模型中流动时的一致性。每一层可以根据自身的处理逻辑来重写继承自父类的方法，并且可以根据需要来增加成员方法和变量，例如全连层需要定义权重和偏值，而激活函数层则不需要。除此之外，在反向传播过程中，我们使用SGD算法来对参数进行更新。由于我们采用模块化的方式构建模型，所以神经网络的不同层之间相对独立，对于不同的层我们可以设置不同的学习率。 12345678# 全连接层以及激活函数层均继承自该父类class Layer: def __init__(self): pass def forward(self, input): return input def backward(self, input, grad): pass 前向计算&emsp;&emsp;前向计算的过程较为简单，网络中每一层所需要做的就是将该层输入传入self.forward函数，根据内部逻辑返回输出，该输出又将作为下一层的输入。对于全连接层，假设输入为$a^{l-1}$，计算$z^l=(W^l)^T*a^{l-1}+b^l$，再将$z^l$传递到下一层；对于激活函数层，假设输入为$z^l$，计算$a^l=\sigma(z^l)$，再将$a^l$传递到下一层。需要注意的是，我们需要记录下神经网络中的每一层的输入值$a^i$，在反向传播更新权重时会用到。 12345678def forward(network, x): # 前向传播 activations = [] input = x for layer in network: activations.append(layer.forward(input)) input = activations[-1] return activations 反向传播&emsp;&emsp;对于反向传播而言，每一层的处理逻辑大致相同，即将该层输出值的误差项作为self.backward函数的输入，经过计算得到该层输入值的误差项，继续反向传播。需要注意的是，由于全连接层含有偏置和权重，在反向传播时除了需要计算误差项之外，还需要更新偏置和权重。反向传播算法的推导过程可以参考这篇博客，这里给出以下结论：$$\delta^{l}=(W^{l+1})^{T} \delta^{l+1} \odot \sigma^{\prime}(z^{l})$$ &emsp;&emsp;在很多介绍神经网络的书中，通常将一个全连接层和一个激活函数构成的整体当作神经网络的一层。但是，在我们代码中是将二者分开的，所以我们需要对上式进行改写。上式中$\delta^{l+1}$表示神经网络第$l+1$层输入值的误差项，即$a^l$的误差项；$\delta^{l}$表示神经网络第$l$层输入值的误差项，即$a^{l-1}$的误差项。由于我们将第$l$层拆分为两层，所以我们先要计算激活函数层输入值的误差项（记做$\delta^l_*$)，再计算全连接层输入值的误差项$\delta^{l}$。所以我们得到下面的公式：$$\delta^{l}_{*}= \delta^{l+1} \odot \sigma^{\prime}(z^{l})$$ $$\delta^{l}=(W^{l+1})^{T} \delta^{l}_{*}$$ &emsp;&emsp;通过这种转换，实现了不同层之间数据流动的一致性，即反向传播时，不论是全连接层还是激活函数层都是接受其输出值的误差项，返回其输入值的误差项。除此之外，由于在全连接层需要对权重和偏置进行更新，需要$a^{l-1}$作为参数，所以在self.backward的参数列表中加入该项。虽然激活函数层不需要更新参数，但是为了统一写法，也会加入这一参数。以下给出全连接层的backward函数。$$\frac{\partial J(W, b, x, y)}{\partial W^{l}}=\delta^{l}\left(a^{l-1}\right)^{T}$$ $$\frac{\partial J(W, b, x, y)}{\partial b^{l}}=\delta^{l}$$ 1234567def backward(self, input, grad): grad_input = torch.mm(grad, self.w.T) dw = torch.mm(input.T, grad) / input.shape[0] db = torch.sum(grad, axis=0) / input.shape[0] self.w -= self.learning_rate*dw self.b -= self.learning_rate*db return grad_input 交叉熵损失函数与softmax激活函数&emsp;&emsp;使用神经网络处理多分类问题的时候，我们往往会使用交叉熵损失函数与softmax激活函数组合的形式，即将神经网络最后一层的激活函数设置为softmax，模型整体的损失采用交叉熵来进行计算。在这之前，我对二者的理解停留在比较浅显的层面：softmax函数将多个神经元的输出，映射到（0,1）区间内，可以看成概率来理解；交叉熵损失函数可以衡量两个概率分布之间的相似性，即softmax的输出和训练数据标签onehot编码之间的相似性。除此之外，交叉熵损失函数与softmax激活函数组合更深层次的原因体现在计算层面，它能够简化反向传播的计算过程。以下，我们对该问题进行分析。 &emsp;&emsp;假设我们现在要计算softmax激活函数层输入值的误差项$\delta^{l}_{*}$，根据我们上面的得到的公式，我们需要分别计算$\delta^{l+1}$和$\sigma^{\prime}(z^{l})$。因为softmax是神经网络的最后一层，所以这里的$\delta^{l+1}$等于交叉熵损失函数对softmax输出值的导数$\frac{\partial J}{\partial a^{l}}$。而$\sigma^{\prime}(z^{l})$则表示softmax函数对其输入进行求导$\frac{\partial a^{l}}{\partial z^{l}}$。 &emsp;&emsp;按照常规思路，我们需要单独计算这两个过程。但是对于交叉熵损失函数与softmax激活函数组合情况而言，可以将这两个过程进行合并，可以直接推导出$\frac{\partial J}{\partial z^{l}}$的值。具体的推导过程，可以参考这篇博客，这篇博客是我目前看过最简单易懂的，这里给出结论：$$\delta^{l}_{*}=\frac{\partial J}{\partial z^{l}}=a^l-y$$可以看出这个结论十分简洁优美，所以当我们使用交叉熵损失函数与softmax激活函数组合形式的时候，softmax层的backwrad函数只需要将$a^l-y$返回即可，无需进行任何操作。 123456789101112# softmax激活函数作为神经网络最后一层class Softmax(Layer): def __init__(self): pass def forward(self,input): exp = torch.exp(input) exp_sum = torch.sum(exp, axis=1, keepdims=True) return exp / exp_sum def backward(self,input,grad): return grad &emsp;&emsp;值得注意的一点是，在使用pytorch框架构建神经网络来实现多分类任务时，如果我们的代价函数使用torch.nn.CrossEntropyLoss()（交叉熵），网络的最后一层无需再定义softmax层，并且数据标签也不需要修改为onehot编码，这些逻辑在其内部应该都会实现，具体操作请参考我的代码。]]></content>
      <categories>
        <category>深度学习</category>
      </categories>
      <tags>
        <tag>深度神经网络</tag>
        <tag>softmax函数</tag>
        <tag>交叉熵损失</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[对抗样本生成系列：JSMA目标扰动]]></title>
    <url>%2F%E5%AF%B9%E6%8A%97%E6%A0%B7%E6%9C%AC%E7%94%9F%E6%88%90%E7%B3%BB%E5%88%97%EF%BC%9AJSMA%E7%9B%AE%E6%A0%87%E6%89%B0%E5%8A%A8.html%2F</url>
    <content type="text"><![CDATA[摘要： 在之前的博客中介绍了三种对抗样本的生成算法，分别是FGSM、DeepFool和Universal Perturbation。这三种算法生成的对抗样本样本有一个共同的特点：其对抗性样例没有具体的目标，即我们无法控制目标模型对对抗性样例的分类结果。举例来说，如果我们构建了一个识别小动物的分类模型，现在我们需要对一张狗的照片生成其对抗性样例。先前的算法生成的对抗性样例只能达到让分类器分类错误的目的，比如说将其识别为一只鸡或者一只猫，分类结果是随机的、不可控的。更进一步，我们希望构建一种针对性更强的对抗性样例，比如说我们希望分类器对对抗性样例的分类结果只能是一只猫或者是由我们预先指定的一种小动物。本文介绍的JSMA算法可以达到这样一种目的。 文章概览 论文解读 攻击模型 JSMA算法 JSMA代码实现 论文解读&emsp;&emsp;JSMA由Papernot等人提出，其论文发表于2016年S&amp;P。论文的主要工作包括：从攻击者的目标和背景知识（或能力）两个方面来构建攻击模型、提出JSMA对抗样本生成算法、对对抗性扰动进行度量并构建防御机制。 攻击模型&emsp;&emsp;分析攻击模型，主要从两个角度进行考虑：攻击者的目标和攻击者的背景知识。对于攻击者的目标而言，主要可以分为以下四类： Confidence reduction：减小输入分类的置信度，从而引入歧义 Misclassification：将输出分类更改为与原始类不同的任何类 Targeted misclassification：生成输入，强制输出分类为特定目标类 Source/target misclassification：强制将特定输入的输出分类强制为特定目标类 对于攻击者的背景知识而言，主要可以分为以下五类： Training data and network architecture：最强大的背景知识，包含训练数据以及模型的结构和详细的参数信息 Network architecture：了解目标模型的网络架构 Training data：了解生成目标模型的训练数据 Oracle：攻击者可以访问模型提供的接口，输入数据并获得反馈，并且可以观察到输入和输出的变化之间的关系 Sample：攻击者可以访问模型提供的接口，输入数据并获得反馈，但是不能观察到输入和输出的变化之间的关系 针对不同的场景，攻击者将采用不同的攻击手法，这也是对抗样本领域研究的重点。 JSMA算法&emsp;&emsp;JSMA算法的灵感来自于计算机视觉领域的显著图。简单来说，就是不同输入特征对分类器产生不同输出的影响程度不同。如果我们发现某些特征对应着分类器中某个特定的输出，我们可以通过在输入样本中增强这些特征来使得分类器产生指定类型的输出。JSMA算法主要包括三个过程：计算前向导数，计算对抗性显著图，添加扰动，以下给出具体解释。 &emsp;&emsp;所谓前向导数，其实是计算神经网络最后一层的每一个输出对输入的每个特征的偏导。以MNIST分类任务为例，输入的图片的特征数（即像素点）为784，神经网络的最后一层一般为10个输出（分别对应0-9分类权重），那对于每一个输出我们都要分别计算对784个输入特征的偏导，所以计算结束得到的前向导数的矩阵为（10，784）。前向导数标识了每个输入特征对于每个输出分类的影响程度，其计算过程也是采用链式法则。这里需要说明一下，前面讨论过的FGSM和DeepFool不同在计算梯度时，是通过对损失函数求导得到的，而JSMA中前向导数是通过对神经网络最后一层输出求导得到的。前向导数$\nabla \mathbf{F}(\mathbf{X})$具体计算过程如下所示，$j$表示对应的输出分类，$i$表示对应的输入特征。$$\nabla \mathbf{F}(\mathbf{X})=\frac{\partial \mathbf{F}(\mathbf{X})}{\partial \mathbf{X}}=\left[\frac{\partial \mathbf{F_j}(\mathbf{X})}{\partial x_i}\right]_{i \in 1 \ldots M, j \in 1 . . N}$$ $$\begin{aligned} \frac{\partial \mathbf{F_j}(\mathbf{X})}{\partial x_i}=&amp;\left(\mathbf{W}_{n+1, j} \cdot \frac{\partial \mathbf{H_n}}{\partial x_i}\right) \times \frac{\partial f_{n+1, j}}{\partial x_i}\left(\mathbf{W}_{n+1, j} \cdot \mathbf{H_n}+b_{n+1, j}\right) \end{aligned}$$ &emsp;&emsp;通过得到的前向导数，我们可以计算其对抗性显著图，即对分类器特定输出影响程度最大的输入。首先，根据扰动方式的不同（正向扰动和反向扰动），作者提出了两种计算对抗性显著图的方式，即： 但是在文章中第四部分的应用中作者发现，找到单个满足要求的特征很困难，所以作者提出了另一种解决方案，通过对抗性显著图寻找对分类器特定输出影响程度最大的输入特征对，即每次计算得到两个特征$$\arg \max _{\left(p_1, p_2\right)}\left(\sum_{i=p_1, p_2} \frac{\partial \mathbf{F_t}(\mathbf{X})}{\partial \mathbf{X_i}}\right) \times\left|\sum_{i=p_1, p_2} \sum_{j \neq t} \frac{\partial \mathbf{F_j}(\mathbf{X})}{\partial \mathbf{X_i}}\right|$$具体的计算过程可以参考文章中的算法3。（注：根据扰动方式的不同，这种算法也有两种计算对抗性显著图的方式） &emsp;&emsp;根据对抗性显著图所得到的特征，可以对其添加扰动。扰动方式包括正向扰动和反向扰动（$+\theta$或$-\theta$）。如果添加的扰动不足以使分类结果发生转变，我们利用扰动后的样本可以重复上述过程（计算前向导数-&gt;计算对抗性显著图-&gt;添加扰动)。这个过程需要注意两点 扰动过程的重复次数需要被约束，即修改的特征数有限 一旦添加扰动后，该特征达到临界值，那么该特征不再参与扰动过程 JSMA算法实现&emsp;&emsp;同样，为了便于理解，这里给出相关代码段（与之前的数据集和网络模型一致）。所有的代码基于python实现，使用的深度学习框架为pytorch，更加完整的算法实现参见我的github。（注：本文JSMA算法实现部分代码参考了DEEPSEC) 网络模型 1234567891011class Net(nn.Module): def __init__(self): super(Net, self).__init__() self.fc1 = nn.Linear(28*28, 300) self.fc2 = nn.Linear(300, 100) self.fc3 = nn.Linear(100, 10) def forward(self, x): x = F.relu(self.fc1(x)) x = F.relu(self.fc2(x)) x = self.fc3(x) return x 数据集（这里只给出测试集的数据定义） 12345# 定义数据转换格式mnist_transform = transforms.Compose([transforms.ToTensor(), transforms.Lambda(lambda x : x.resize_(28*28))])# 导入数据，定义数据接口testdata = torchvision.datasets.MNIST(root="./mnist", train=False, download=True, transform=mnist_transform)testloader = torch.utils.data.DataLoader(testdata, batch_size=256, shuffle=True, num_workers=0) &emsp;&emsp;对于前向导数，如果根据链式法则一层层的推导，计算过程十分复杂。但是在深度学习框架中，对于反向传播过程的梯度的计算都已经封装好，我们只需要简单的调用即可，所以前向导数的计算过程十分简单，如下所示 1234567891011121314def compute_jacobian(model, input): output = model(input) num_features = int(np.prod(input.shape[1:])) jacobian = torch.zeros([output.size()[1], num_features]) mask = torch.zeros(output.size()) # chooses the derivative to be calculated for i in range(output.size()[1]): mask[:, i] = 1 zero_gradients(input) output.backward(mask, retain_graph=True) # copy the derivative to the target place jacobian[i] = input._grad.squeeze().view(-1, num_features).clone() mask[:, i] = 0 # reset return jacobian &emsp;&emsp;计算对抗性显著图的过程也较为简单，一般根据算法的描述一步步实现并不难。但是在该过程中存在一个问题，我们每次需要找到满足要求的一对特征，那组合的方式一共有784*783种，如果采用普通的循环来实现计算代价很大。参考了DEEPSEC的代码，我发现他们将这个问题巧妙的转化为矩阵求解的问题，大大缩短了计算时间。 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647def saliency_map(jacobian, target_index, increasing, search_space, nb_features): domain = torch.eq(search_space, 1).float() # The search domain # the sum of all features' derivative with respect to each class all_sum = torch.sum(jacobian, dim=0, keepdim=True) target_grad = jacobian[target_index] # The forward derivative of the target class others_grad = all_sum - target_grad # The sum of forward derivative of other classes # this list blanks out those that are not in the search domain if increasing: increase_coef = 2 * (torch.eq(domain, 0)).float() else: increase_coef = -1 * 2 * (torch.eq(domain, 0)).float() increase_coef = increase_coef.view(-1, nb_features) # calculate sum of target forward derivative of any 2 features. target_tmp = target_grad.clone() target_tmp -= increase_coef * torch.max(torch.abs(target_grad)) alpha = target_tmp.view(-1, 1, nb_features) + target_tmp.view(-1, nb_features, 1) # calculate sum of other forward derivative of any 2 features. others_tmp = others_grad.clone() others_tmp += increase_coef * torch.max(torch.abs(others_grad)) beta = others_tmp.view(-1, 1, nb_features) + others_tmp.view(-1, nb_features, 1) # zero out the situation where a feature sums with itself tmp = np.ones((nb_features, nb_features), int) np.fill_diagonal(tmp, 0) zero_diagonal = torch.from_numpy(tmp).byte() # According to the definition of saliency map in the paper (formulas 8 and 9), # those elements in the saliency map that doesn't satisfy the requirement will be blanked out. if increasing: mask1 = torch.gt(alpha, 0.0) mask2 = torch.lt(beta, 0.0) else: mask1 = torch.lt(alpha, 0.0) mask2 = torch.gt(beta, 0.0) # apply the mask to the saliency map mask = torch.mul(torch.mul(mask1, mask2), zero_diagonal.view_as(mask1)) # do the multiplication according to formula 10 in the paper saliency_map = torch.mul(torch.mul(alpha, torch.abs(beta)), mask.float()) # get the most significant two pixels max_value, max_idx = torch.max(saliency_map.view(-1, nb_features * nb_features), dim=1) p = max_idx // nb_features q = max_idx % nb_features return p, q &emsp;&emsp;算法其余的实现可以参考我的gihub。代码完成后进行了一些测试，选取原始样本0，目标分类结果分别为0-9。我们采用JSMA算法对其添加噪声，得到的结果如下所示。 我们将这些图片分别丢入原始分类器中，分类结果与我们的目标全都一致。由此看来，一般情况下，JSMA算法的效果还是很不错的。]]></content>
      <categories>
        <category>机器学习安全</category>
      </categories>
      <tags>
        <tag>深度学习</tag>
        <tag>对抗样本</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[对抗样本生成系列：FGSM和DeepFool]]></title>
    <url>%2F%E5%AF%B9%E6%8A%97%E6%A0%B7%E6%9C%AC%E7%94%9F%E6%88%90%E7%B3%BB%E5%88%97%EF%BC%9AFGSM%E5%92%8CDeepfool.html%2F</url>
    <content type="text"><![CDATA[摘要：近些年来，深度学习技术在海量数据以及强大计算能力的驱动下取得了长足的发展，特别是在语音识别、计算机视觉、自然语言处理等领域，深度学习以其强大的网络表达能力刷新了一项又一项记录，各种各样基于深度学习的产品和服务也逐渐在产业界落地应用。正因为深度学习技术蕴含着巨大的商业价值，其背后潜在的安全问题更值得我们去深究。最近的研究表明，深度学习面临安全和隐私等多方面的威胁。该系列主要讨论深度学习领域最为热门的安全问题–对抗样本。本文讨论的FGSM和DeepFool是较为的早期对抗样本的生成算法，除此之外还会对DeepFool衍生出的Universal Perturbation进行解读。 文章概览 FGSM 算法概述 代码实现 DeepFool 算法概述 代码实现 Universal Perturbation FGSM算法概述&emsp;&emsp;在机器学习领域，对抗样本的问题始终存在。特别是在入侵检测、垃圾邮件识别等传统的安全应用场景，对抗样本的生成和识别一直是攻击者和防御者博弈的战场。2013年，Szegedy等人首次提出针对深度学习场景下的对抗样本生成算法–BFGS，作者认为，深度神经网络所具有的强大的非线性表达能力和模型的过拟合是可能产生对抗性样本原因之一。2014年，Goodfellow等人（包括Szegedy）对该问题进行了更深层次的研究，它们认为高维空间下深度神经网络的线性线性行为是导致该问题的根本原因。并根据该解释，设计出一种快速有效的生成对抗性样例的算法–FGSM。以下是对论文的解读。 &emsp;&emsp;对于线性模型$f(x)=w^Tx+b$来说，如果我们对输入$ x $添加一些扰动，使得$ \tilde{x}=x+\eta $。为了确保添加的扰动是极小的或者说是无法感知的，我们要求$ \left|\eta\right|_\infty&lt;\epsilon $。所以我们可以得到加噪后的模型输出为$$f(\tilde{x})=w^Tx+w^T\eta+b$$为了尽可能增大添加的噪声对输出结果的影响，令$\eta=\epsilon sign(w)$（sign()函数的定义)。如果$w$是一个$n$维的向量，每一维的均值为$m$，则$w^T\eta=\epsilon nm$。虽然$\epsilon$的值很小，但是当$w$的维度$n$很大时，$\epsilon nm$将是一个很大的值，这将会给模型的预测带来很大的影响。因此，高维特征和线性行为可以成为对抗性样本存在的一种解释。 &emsp;&emsp;上面的解释是基于线性模型而言的，而深度神经网络作为一种高度非线性模型，为什么会存在对抗性样例呢？深度神经网络的非线性单元赋予了其强大的表达能力，但是非线性单元的存在会降低学习的效率。为了提高学习效率，需要对非线性单元进行改进，通常的做法是通过降低其非线性来实现。从早期的sigmoid到tanh再到ReLU，我们会发现这些非线性单元的线性行为在不断增强，这也导致了深度神经网络中的线性能力的增强，这在一定程度上解释了深度神经网络中对抗性样例存在的原因。 &emsp;&emsp;根据这种解释，作者提出了生成对抗性样例的算法FGSM。我们将深度神经网络模型看作是一个线性模型，即类似于$f(x)=w^Tx+b$。在线性模型中，$w$为$f(x)$关于$x$的导数，而在深度神经网络模型中我们可以将$w$视为代价函数关于输入$x$的导数，即$$w=\nabla_{x}J(\theta,x,y)$$ $$\eta=\epsilon sign(\nabla_{x}J(\theta,x,y))$$ &emsp;&emsp;在文章的后半部分主要介绍了针对对抗性样本的防御机制，即通过对抗性训练来提高模型的鲁棒性，这些内容会在之后的工作中讨论。 代码实现&emsp;&emsp;为了更好的理解算法的实现过程，以下给出本文相关的代码段。这些代码段给出的定义，对于后面的DeepFool和Universal Perturbation算法的解读依赖有效。所有的代码基于python实现，使用的深度学习框架为pytorch，更加完整的算法实现参见我的github. 网络模型 1234567891011class Net(nn.Module): def __init__(self): super(Net, self).__init__() self.fc1 = nn.Linear(28*28, 300) self.fc2 = nn.Linear(300, 100) self.fc3 = nn.Linear(100, 10) def forward(self, x): x = F.relu(self.fc1(x)) x = F.relu(self.fc2(x)) x = self.fc3(x) return x 数据集（这里只给出测试集的数据定义） 12345# 定义数据转换格式mnist_transform = transforms.Compose([transforms.ToTensor(), transforms.Lambda(lambda x : x.resize_(28*28))])# 导入数据，定义数据接口testdata = torchvision.datasets.MNIST(root="./mnist", train=False, download=True, transform=mnist_transform)testloader = torch.utils.data.DataLoader(testdata, batch_size=256, shuffle=True, num_workers=0) 代价函数 1loss_function = nn.CrossEntropyLoss() &emsp;&emsp;FGSM算法的实现较为简单，其核心在于利用代价函数求解已知样本的梯度值。我们假设模型已经训练完成，首先我们加载已训练完成的深度网络模型 1net = torch.load('mnist_net_all.pkl') # 加载模型 然后我们选择一个测试样本，针对该测试样本生成其对应的对抗性样例。其原始图片格式如图所示。 123index = 100 # 选择测试样本image = Variable(testdata[index][0].resize_(1,784), requires_grad=True) # requires_grad存储梯度值label = torch.tensor([testdata[index][1]]) 接着将测试样本作为网络模型的输入，通过前向传播的过程计算其损失，然后利用其损失进行反向传播（即求导）。 123outputs = net(image) # 前向传播loss = loss_function(outputs, label) # 计算损失loss.backward() # 反向传播 在深度学习框架中，反向传播的过程对用户来说是透明的。计算结束后，其梯度值存储在image.data.grad中，添加扰动的过程如下 1234# FGSM添加扰动epsilon = 0.1 # 扰动程度x_grad = torch.sign(image.grad.data)x_adversarial = torch.clamp(image.data + epsilon * x_grad, 0, 1) 添加扰动后，得到对抗性样本如下所示 实验过程中我们发现，深度网络模型对于原始的图片能够正确的分类，而对于扰动之后的的样本不能正确分类（分类结果为2）。 DeepFool算法概述&emsp;&emsp;FGSM算法能够快速简单的生成对抗性样例，但是它没有对原始样本扰动的范围进行界定（扰动程度$\epsilon$是人为指定的），我们希望通过最小程度的扰动来获得良好性能的对抗性样例。2016年，Seyed等人提出的DeepFool算法很好的解决了这一问题。文章的核心思想是希望找到一种对抗性扰动的方法来作为对不同分类器对对抗性扰动鲁棒性评估的标准。简单来说就是，现在我需要两个相同任务的分类器A、B针对同一个样本生成各自的对抗性样例。对于分类器A而言，其生成对抗性样例所需要添加的最小扰动为$a$；对于分类器B而言，其生成对抗性样例所需要添加的最小扰动为$b$；通过对$a$、$b$的大小进行比较，我们就可以对这两个分类器对对抗性样例的鲁棒性进行评估。由于FGSM产生扰动是人为界定的，所以它不能作为评估的依据。DeepFool可以生成十分接近最小扰动的对抗性样例，因此它可以作为衡量分类器鲁棒性的标准。 &emsp;&emsp;DeepFool源于对分类问题的思考。对于如图所示的线性二分类问题，令$f(x)=w^Tx+b$，其中$\mathscr{F}={x : f(x)=0}$。此时$x_0$位于直线的下方，即$f(x_0)&gt;0$。现在我们希望对$x_0$添加扰动$r$，使得分类器$f(x_0+r)&lt;0$。那么如何添加扰动才能使得扰动的程度最小呢？这个问题可以转化为求点到直线之间的距离，我们通过$x_0$做直线$\mathscr{F}$的垂线，与$\mathscr{F}$相交于$p$（投影点），则$p$与$x_0$之间的距离即为$x_0$到分类边界$\mathscr{F}$的最短距离。所以当我们沿着分类边界法线方向对$x_0$进行扰动，可以保证扰动的程度最小$$r(x_0):={\arg\min_k}{|{r}|_{2}}=-{\frac{f(x_0)}{|{w}|_{2}^{2}}{w}}$$添加扰动之后将$x_0$映射到分类边界的投影点$p$，即$p=x_0+r(x_0)$。 &emsp;&emsp;同样，对于非线性的二分类问题（分类边界为曲线或者曲面），我们也需要计算从目标样本点到分类边界的最短距离。这个计算过程较为复杂，一般采用垂直逼近法来逐步的逼近$x_0$在分类边界上的投影点，所以在论文中算法1会有一个迭代过程。值得注意的是，我们得到最终的$\sum_i{r_i}$表示的是从$x_0$到分类边界投影点的距离向量，满足$f(x_0+\sum_i{r_i})=0$。如果要使分类结果改变，需要再添加一些极小的扰动，如$f(x_0+(1+\eta)\sum_i{r_i})&lt;0$（文中$\eta$取0.02）。 &emsp;&emsp;对于多分类任务，思路也大致相同。在线性多分类任务中，需要注意的大致有两点：首先，对于多分类任务的分类边界要重新进行定义。我们令$f(x)$为分类器，$\hat{k}(x)=\mathop{\arg\max}_{k}{f_k(x)}$表示其对应的分类结果（一共有$k$个类别）。分类边界定义为$$\mathscr{F}_k={x : f_k(x)-f_{\hat{k}(x_0)}(x)=0}$$其次，由于分类边界有多个，我们需要以此求出$x_0$到每个分类边界的距离（类似于进行多次二分类的计算过程），然后进行比较，选择其中最短距离向量作为最终的扰动。$$\hat{l}(x_0)=\mathop{\arg\min_{k \neq \hat{k}_{x_0} }}{\frac{|f_k(x_0)-f_{\hat{k}(x_0)}(x_0)|}{|{w_k -w_{\hat{k}_{x_0}}}|_2}}$$之后通过计算$\boldsymbol{r}(x_0)$得到$x_0$在分类边界上的投影。 &emsp;&emsp;在非线性多分类任务中，与线性多分类的区别在于其分类边界是不确定的，所以我们需要采用类似于非线性二分类任务中的方法来逼近分类边界。获得分类边界之后的计算与线性多分类的过程类似。此后，重复该过程多次，即可获得最终$x_0$在分类边界的投影。 代码实现&emsp;&emsp;以下讨论多分类情况下DeepFool算法的代码实现，模型结构以及数据集等与上述一致，不再重复。首先，我们选择一个测试样本，生成该样本的对抗性样例。 1234net = torch.load('mnist_net_all.pkl') # 加载模型index = 100 # 选择测试样本image = Variable(testdata[index][0].resize_(1,784), requires_grad=True)label = torch.tensor([testdata[index][1]]) 接下来，通过前向传播的过程，我们获得该样本对每一类别可能的取值情况，并将其从大到小排列起来，列表$I$对应其索引值，$label$即$\hat{k}_{x_0}$。 1234567f_image = net.forward(image).data.numpy().flatten() # f_image: [-1.1256416 , -1.0344085 , 2.0596995 , -2.0181773 , -0.24274658, -0.53373957, 6.6361637 , -2.250309 , 0.06580263, -3.0854702 ]I = (np.array(f_image)).flatten().argsort()[::-1]# I: [6, 2, 8, 4, 5, 1, 0, 3, 7, 9]label = I[0] # 该样本的标签为6 然后，我们定义一些需要用到的变量 123456789101112input_shape = image.data.numpy().shape # 获取原始样本的维度pert_image = copy.deepcopy(image) # 深度复制原始样本w = np.zeros(input_shape) r_tot = np.zeros(input_shape)loop_i = 0 max_iter = 50 # 最多迭代次数overshoot = 0.02 x = Variable(pert_image, requires_grad=True)fs = net.forward(x)fs_list = [fs[0][I[k]] for k in range(len(I))] # 每个类别的取值情况，及其对应的梯度值k_i = label 下面是算法实现的核心部分，参考论文中的伪代码，其中orig_grad表示$\nabla f_{\hat{k}_{x_0}}(x_i)$，cur_grad表示$\nabla f_k(x_i)$，fs[0][I[k]]表示$f_k(x_i)$，fs[0][I[0]]表示$f_{\hat{k}_{x_0}}(x_i)$。通过内部的for循环可以获得x到各分类边界的距离；在外部的while循环中，我们利用内部循环获得的所有边界距离中的最小值对x进行更新。重复这一过程，直到$x$的分类标签发生变化。 1234567891011121314151617181920212223242526while k_i == label and loop_i &lt; max_iter: pert = np.inf fs[0][I[0]].backward(retain_graph=True) orig_grad = x.grad.data.numpy().copy() for k in range(len(I)): zero_gradients(x) fs[0][I[k]].backward(retain_graph=True) cur_grad = x.grad.data.numpy().copy() w_k = cur_grad - orig_grad f_k = (fs[0][I[k]] - fs[0][I[0]]).data.numpy() pert_k = abs(f_k) / np.linalg.norm(w_k.flatten()) if pert_k &lt; pert: # 获得最小的分类边界距离向量 pert = pert_k w = w_k r_i = (pert + 1e-4) * w / np.linalg.norm(w) r_tot = np.float32(r_tot + r_i) # 累积扰动 pert_image = image + (1+overshoot)*torch.from_numpy(r_tot) # 添加扰动 x = Variable(pert_image, requires_grad=True) fs = net.forward(x) k_i = np.argmax(fs.data.numpy().flatten()) # 扰动后的分类标签 loop_i += 1r_tot = (1+overshoot)*r_tot # 最终累积的扰动 对原始图片，添加扰动获得如下图片（分类器将其错误分类为2，有些奇怪的地方在于其扰动的程度要大FGSM，算法应该没有问题，一直没找到原因）。 Universal Perturbation&emsp;&emsp;前面介绍的FGSM和DeepFool算法，它们都是针对单个样本生成其对抗性样例，也就是说每个对抗性样例的扰动程度都不同。那么是否能找到一种通用性的扰动边界，能够为不同的样本生成对抗性样例。在DeepFool工作的基础上，Seyed 等人提出了Universal Perturbation，该算法为寻找通用性扰动边界提供了可能。以下简单介绍一下论文的核心思想： 从数据集中随机选取部分测试样本作为生成通用性扰动边界的范例，通过这些测试样本生成的通用性扰动适用于整个数据集。 算法的计算过程：输入第一个样本后，通过DeepFool算法找到该样本的最小扰动距离向量，将其累积到通用扰动向量$v$中（对$v$的扰动程度会有限制和调整，${|v|_p&lt;\xi}$）；当输入第二个样本之后，对其添加$v$的扰动之后，然后再通过DeepFool计算扰动后的样本的最小扰动距离向量，将其累积到通用扰动向量$v$中（对$v$的扰动程度会有限制和调整,${|v|_p&lt;\xi}$）。重复这一过程，直到最后一个测试样本。然后，我们使用通用扰动向量$v$，对原始的测试样本进行扰动，测试其生成对抗性样例的成功率。如果小于预先设置的阈值$1-\delta$，则跳出循环返回结果。否则，重复上述过程。 通用性扰动对不同的网络模型依然有效。也就是说，利用网络模型A生成的通用性扰动，同样适用于生成网络模型B的对抗性样例（A、B是不同类型的网络架构）。]]></content>
      <categories>
        <category>机器学习安全</category>
      </categories>
      <tags>
        <tag>深度学习</tag>
        <tag>对抗样本</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[校园网环境下服务器双网卡配置]]></title>
    <url>%2F%E6%A0%A1%E5%9B%AD%E7%BD%91%E7%8E%AF%E5%A2%83%E4%B8%8B%E6%9C%8D%E5%8A%A1%E5%99%A8%E5%8F%8C%E7%BD%91%E5%8D%A1%E9%85%8D%E7%BD%AE.html%2F</url>
    <content type="text"><![CDATA[摘要：最近一段时间在忙着写论文、看论文，博客一直没有更新了。这几天实验室添了两台交换机和五台服务器，这对于我这个爱折腾的人来说，确实是个大喜事，老师也把实验室设备的管理工作全权交予我来负责。忙活了几天时间，装好了系统，建立了一个小型的Hadoop集群和私有云。这其中我觉得最有意思的是关于实验室网络环境的配置，所以在这里做一些分享。 文章概览 网络环境简介 双网卡配置 Linux密码 网络环境简介&emsp;&emsp;一开始，实验室的服务器都没有分配校园网固定IP，为了对服务器进行安装配置，我用了一台闲置的交换机和一个二手路由器组建了一个简单的网络环境（服务器—&gt;交换机—&gt;路由器）。但是存在的问题是，服务器只能在实验室的网络环境才能访问，无法通过校园网访问。一些内网映射的工具又不太稳定，使用起来很不方便，最关键的是路由器成为了整个网络的瓶颈（几十块钱的路由器）。后来通过学校网络中心分配了几个固定的校园网IP，但是校园网IP不能直接连接外网，需要拨号才能联网。五台服务器也就意味着需要五个账号才能使所有的服务器同时连接外网，我也没有这么多账号。仔细考虑了一下我们当前的需求和配置： 所有的服务器能通过校园网直接访问 所有的服务器能同时连接外网 宽带账号只有一个，路由器一个，交换机一个 综合这些因素，配置服务器双网卡可能是一个好的选择。每台服务器都有多个网口，将每台服务器同时连接在校园网和路由器两个网络中就能满足这些需求。通过给连接在校园网的网口配置校园网固定IP，可以保证校园网对服务器的访问；通过路由器拨号上网，将服务器的另一个网口连接到路由器上，即可保证服务器对外网的访问。 双网卡配置&emsp;&emsp;配置过程需要注意以下几点： 再给两个网卡配置静态IP的时候，不需要GATEWAY字段 删除系统默认的网关（在 /etc/sysconfig/network文件中修改） 使用ip route命令添加默认网关 （ip route add default via 网关地址 dev 网卡1）。一般我们将路由器的网关地址作为服务器的默认网关，这样能保证服务器可以访问外网。 添加默认路由（ip route add 校园网网段 via 校园网静态ip的网关地址 dev 网卡2）。为了在校园网内部可以对服务器进行访问，我们需要添加一条默认的路由。对于校园网网段的访问，不通过路由器，而是通过校园网静态ip的网关地址。 Linux密码&emsp;&emsp;之前一个同学在实验室主机上装过centos，可能太长时间没用，所以root密码忘了。找回LInux密码的过程挺有意思，一般情况下，通过单用户模式可以对root密码进行修改，但是系统设置了grub密码（防止修改用户密码）。至于grub密码就更不记得了，于是我们又想通过救援模式去修改grub密码，然后再修改root密码。参考了很多教程，我们发现修改不了grub密码。就在准备放弃的时候，我发现救援模式下可以直接访问/etc/shadow文件，我试着将文件中root用户记录删除，竟然修改成功了。重新开机之后，奇迹发生了，可以使用root用户直接登录了。这里给出参考的博客链接，希望对大家有所帮助。]]></content>
      <categories>
        <category>网络安全</category>
      </categories>
      <tags>
        <tag>Linux</tag>
        <tag>校园网</tag>
        <tag>服务器</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[tor匿名网络]]></title>
    <url>%2FTor%E5%8C%BF%E5%90%8D%E7%BD%91%E7%BB%9C.html%2F</url>
    <content type="text"><![CDATA[摘要： 前段时间看到一篇报道，暗网最大的托管商遭到黑客攻击，6500+网站被删。根据知道创宇平台的暗网雷达显示，一夜之间活跃在暗网中的网站从12000+下降到5000+，几乎遭到团灭，这一事件让暗网再次进入人们的视野。一直以来，暗网被赋予了很多传奇的色彩，本文将带你一步步揭开暗网的神秘面纱。 文章概览 基本概念 深网 暗网 黑暗网络 tor匿名网络 tor基本概述 tor网络结构 tor路由技术 tor匿名服务 tor客户端代理 基本概念&emsp;&emsp;在介绍暗网之前，我们先来了解一下经常会混淆的三个概念“深网”(Deep web)、“暗网”(Dark web) 和“黑暗网络”(Darknet) 。 深网&emsp;&emsp;深网是指服务器上可通过标准的网络浏览器和连接方法访问的页面和服务，但主流搜索引擎不会收录这些页面和服务。搜索引擎之所以不会收录深网，通常是因为网站或服务的配置错误、拒绝爬虫爬取信息、需要付费查看、需要注册查看或其他内容访问限制。据不完全统计，互联网世界中只有4％是对公众开放，剩下的96％的网站和数据则隐藏在深网中。 暗网&emsp;&emsp;暗网是深网中相对较小的一部分，与被故意隐藏的 Web 服务和页面有关。仅使用标准浏览器无法直接访问这些服务和页面，必须依靠使用覆盖网络 (Overlay Network)；而这种网络需要特定访问权限、代理配置、专用软件或特殊网络协议。 黑暗网络&emsp;&emsp;黑暗网络是在网络层访问受限的框架，例如 tor 或 I2P，私有 VPN 也属于这个类别。通过这些框架的网络流量会被屏蔽。当进行数据传输时，系统只会显示您连接的黑暗网络以及您传输了多少数据，而不一定会显示您访问的网站或所涉及数据的内容。与之相反的是，直接与明网（Clean Net）或与未加密的表网服务和深网服务交互。在这种情况下，您与所请求资源之间的互联网服务提供商和网络运营商可以看到您传输的流量内容。 tor匿名网络基本概述&emsp;&emsp;经过上面的描述，我们对暗网整体的框架有个大致的了解。暗网不同于普通的互联网络，它拥有特殊的运作方式和网络协议，tor 是目前世界范围内是最大的暗网。tor又名洋葱网络，最初是由美国军方的情报机构开发并且提供财务支持的。洋葱网络使用特殊的路由转发技术，即洋葱路由技术。洋葱路由技术利用 P2P 网络,把网络流量随机地通过 P2P 的节点进行转发，这样可以掩盖源地址与目标地址的路径，使得在 Internet 上难以确定使用者的身份和地址。这就类似于你给某人送一封匿名信，你不是自己去送或者通过邮局的邮差去送，而是在大街上随便找几个不认识的人让他帮你送，这样收信人就很难往回追踪找到你。 &emsp;&emsp;洋葱路由项目最初进展缓慢,废弃了其中的几个版本。直到 2002 年才由麻省理工的两位毕业生 Roger Dingledine 和 Nick Mathewson 以及原项目组成员 PaulSyverson 一起开发出一个新版的洋葱路由，也就是Tor。“洋葱路由”的最初目的并不是保护大众的隐私，它的目的是帮助政府情报人员隐藏身份，网上活动不被敌对国家监控。后来为了混淆系统的流量，而不是让系统仅仅拥有来自美国安全部门网络的流量，他们又让系统中加入来自其它网络的流量。于是，tor 的普通版本被推广给了普通大众，让普通大众也能使用 tor 来保护自己的隐私。这样就可以把政府情报人员的流量与社会上各行各业用户的复杂流量混在一起，能让流量分析更加困难，提供更强地隐私保护。 &emsp;&emsp;目前全球范围内，tor网络由超过一万七千个中继节点组成，每个中继节点都是由全球志愿者免费提供，这些中继节点大部分分布在欧洲和北美。暗网除了能给我们提供匿名的网络服务之外，在按暗网中也存在各种类型的网站，这些网站只有通过暗网才能够访问。暗网中的网站大部分都是提供一些“特殊的服务”，包括枪支、毒品、网络攻击等等。 网络结构Tor 匿名网络结构主要由目录服务器、洋葱代理、洋葱路由器这三部分组成。 目录服务器：它负责收集和更新网络中所有可运行的中继节点信息，为客户端提供节点公钥等建立链路所需的必要信息。目前，绝大多数的目录服务器在欧美地区,只有极少数分布在亚洲。 洋葱代理：它是 tor 用户的客户端代理程序，负责下载目录服务器中的路由信息，选择节点和建立路径，并对通信信息进行加解密。 洋葱路由器（又称中继节点）：它是构建匿名通信链路的基础，负责转发 tor 客户端和网络服务器的通信信息，是实现匿名通信的关键。目前整个 tor 网络中有几千个路由节点分布在世界各地，这些中继节点又分为三种类型: Entry/Guard 中继节点──这是 tor 网络的入口节点。这些中继节点运行一段时间后，如果被证明是稳定的，并具有高带宽，就会被选来作为 Guard 中继节点。 Middle 中继节点──Middle 中继节点是位于中间节点位置上的洋葱路由器，充当流量从 Guard 中继节点传输到 Exit 中继节点的桥梁，这可以避免 Guard 中继节点和 Exit 中继节点探查到彼此的位置。 Exit 中继节点──位于出口节点位置上的洋葱路由器，负责将 Tor 网络内的流量转发到网络外部的互联网中去。每个出口节点都有一个相关的出口政策，该政策规定该节点能通过哪个端口转发何种协议的流量来防止滥用 tor 网络。 &emsp;&emsp;tor 网络主要依赖于这些中继节点转发用户流量，tor 通过随机选取遍布于全球由志愿者运行的三个中继节点，然后分别与选择的入口节点、中间节点、出口节点协商会话密钥。用这些协商的密钥将通信数据先进行多层加密，然后再将加密的数据在三个洋葱路由器组成的通信链路上传送。数据每经过一个洋葱路由器就像是剥去一层洋葱皮一样解密去掉一个加密层，以此得到下一跳路由信息，然后将数据继续发往下一个洋葱路由器，不断重复此过程,直到数据送达目的地。这种转发方式能防止那些知道数据发送端以及接收端的中间人窃听数据内容。 tor路由技术以下步骤讲述了Alice在使用tor与Bob的服务器进行通讯时，tor是如何工作的。 Alice开启tor客户端代理，获取来自tor 目录服务器（Dave）中的tor节点（或中继的列表）以及它们的公钥。 Alice 选择三个节点建立通信链路。Alice 得到入口中继节点的 IP 地址和身份摘要，和入口节点协商一个只用于两者之间通信的短暂会话密钥。成功建立一跳的链路以后，Alice 使用同样地方法要求入口节点拓展链路到中间节点，得到了 Alice与中间节点的短暂会话密钥。Alice 重复此过程直至建立一条含有三跳的通信链路并且获得三个她与三个节点独一无二的短暂会话密钥。整个链路建立过程中的所有连接都是加密的。然后Alice 向 Bob 发送服务请求，并且将请求内容使用三个会话密钥按由远到近的顺序依次加密。入口节点接收到解密消息后会使用会话密钥将其解密一层，并将仍然加密的信息转发给中间节点。直至到达出口节点后，才将信息解密为明文。图中的虚线表示出口节点与 Bob 之间的连接是未加密的，出口节点将原始数据发送给 Bob。Bob 回复请求内容，并且每个节点会以相反的加密顺序加密一层,最终送还给 Alice。 如果 Alice 与 Bob 通信时间较长，Alice 每隔几分钟会重新选择三个节点建立新的通信链路以防攻击者窃听。如果 Alice 想要访问另一服务器 Jane，她也会重新选择中继节点建立新的通信链路。 tor匿名服务&emsp;&emsp;之前提到暗网中也存在很多网站，这些网站在提供相应的服务时（暗网中大部分网络服务都是违法的）也希望是匿名的，即用户无法追踪到关于该网站的相关信息。下面主要讲解用户Alice和匿名服务器Bob交互的具体过程。 服务器Bob与tor网络中的一些中继节点连接，请求这些中继节点作为匿名服务的接入点，并将Bob的公钥发送给这些中继节点。注意Bob和中继节点之间的连接也是匿名的，这些中继节点无法获取关于Bob的相关位置信息。 Bob将之前建立的接入点的相关信息和自己的公钥组装成描述符，并用自己的私钥该描述符进行签名，然后将其发送到目录服务器。通过Bob的公钥可以生成一个16位的字符串，记为XYZ。当客户端请求XYZ.onion时就可以找到对应Bob的描述符。 Alice通过某些渠道获得了tor域名XYZ.onion，Alice想要访问该服务器。她通过tor客户端访问XYZ.onion，此时可以从目录服务器中获取对应服务器的描述符，通过描述符可以知道Bob服务器的接入点和公钥。与此同时，Alice会提前建立另外一条私密临时会话点，用于下一步与Bob交互。 当确认描述符存在且临时通道准备好之后，Alice用描述符中的公钥加密一条信息，包括临时会话点和会话秘钥，将加密后的信息发送给描述符中的接入点，之后接入点会将加密信息发送到对应的服务器（即Bob的服务器）。 Bob收到加密信息后将其解密，获取临时会话点和会话秘钥。然后和临时会话点建立匿名连接，连接成功后，临时会话点会通知Alice。之后Alice和Bob可以通过临时会话点进行通信，注意通信过程中建立的连接都是匿名的。 tor客户端代理&emsp;&emsp;如果只是希望匿名浏览web网页，我们可以通过tor浏览器来实现。tor浏览器是基于火狐浏览器改造而来，可以方便的帮助我们连接到tor，实现网络匿名。如果想要在更多应用中实现网络匿名，我们可以安装tor客户端代理。以下配置过程基于ubuntu，且默认已安装shadowsocks。 安装polipo&emsp;&emsp;polipo是轻量级的跨平台代理服务器，可以实现http和socks代理，polipo本地服务端口为8123。 1sudo apt-get install polipo 安装完成后，修改配置文件/etc/polipo/config 12socksParentProcy = "localhost:9050" #tor服务本地端口为9050socksProxyType = socks5 安装tor1sudo apt-get install tor 安装完成后，修改配置文件/etc/tor/torrc，添加以下内容。 1SOCKS5Proxy 127.0.0.1:1080 #shadowsocks本地服务端口为1080 测试&emsp;&emsp;完成上述操作后，开启polipo、tor和shadowsocks。如果我们希望在chrome浏览器中实现网络匿名，可以通过添加tor代理来实现。 1代理协议：socks5 代理服务器：127.0.0.1 代理端口：9050 如果我们希望在命令行中实现网络匿名，可以通过polipo代理来 1http_proxy=http://localhost:8123 "需要执行的操作“]]></content>
      <categories>
        <category>网络安全</category>
      </categories>
      <tags>
        <tag>洋葱路由</tag>
        <tag>计算机网络</tag>
        <tag>暗网</tag>
        <tag>代理</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Hadoop分布式集群搭建]]></title>
    <url>%2FHadoop%E5%88%86%E5%B8%83%E5%BC%8F%E9%9B%86%E7%BE%A4%E6%90%AD%E5%BB%BA.html%2F</url>
    <content type="text"><![CDATA[摘要：Hadoop，是一个分布式系统基础架构，由Apache基金会开发。用户可以在不了解分布式底层细节的情况下，开发分布式程序。充分利用集群的威力高速运算和存储。简单地说来，Hadoop是一个可以更容易开发和运行处理大规模数据的软件平台。该平台使用的是面向对象编程语言Java实现的，具有良好的可移植性。本文将介绍Hadoop相关的技术框架以及搭建Hadoop平台的详细过程。 文章概览 Hadoop简介 Hadoop体系结构 HDFS分布式文件系统 MapReduce编程模型 Hadoop平台搭建 Hadoop简介Hadoop体系结构 HDFS分布式文件系统&emsp;&emsp;在正式讨论HDFS分布式文件系统之前，我们首先了解一下什么是文件系统。文件系统实际上可以看作是一个用户与底层数据交互的一个接口，对于底层数据而言它定义了数据的存储和组织方式，同时也提供了存储空间的管理功能；而对于用户而言它使用文件和树形目录的抽象逻辑概念代替了存储设备中块的概念，用户使用文件系统来操作数据不必关心数据实际保存在硬盘（或者光盘）的地址为多少的数据块上，只需要记住这个文件的所属目录和文件名（关于文件系统更详细的介绍参见维基百科和这篇博客)。传统文件系统适用于存储容量小等一些没有特殊要求的应用场景。 &emsp;&emsp;但是随着信息技术的不断发展，人们可以获取的数据成指数倍的增长，单纯通过增加硬盘个数来扩展计算机文件系统的存储容量的方式，在容量大小、容量增长速度、数据备份、数据安全等方面的表现都差强人意。为了满足这些特殊应用场景的需求，分布式文件系统应运而生。分布式文件系统可以有效解决数据的存储和管理难题：将固定于某个地点的某个文件系统，扩展到任意多个地点/多个文件系统，众多的节点组成一个文件系统网络。每个节点可以分布在不同的地点，通过网络进行节点间的通信和数据传输。人们在使用分布式文件系统时，无需关心数据是存储在哪个节点上、或者是从哪个节点从获取的，只需要像使用本地文件系统一样管理和存储文件系统中的数据（我们需要知道的是在分布式文件系统的每个数据结点上，数据的存储方式是建立在传统文件系统的基础上的，所谓分布式文件系统它提供的是数据宏观上的存储和管理方式）。 &emsp;&emsp;HDFS是分布式文件系统的一种，它采用master/slave架构，一个HDFS集群是由一个Namenode和一定数目的Datanodes组成。它可以处理超大规模的数据，并且提供了良好的容错机制，下图是HDFS的基本结构。 NameNode: 可以看作是分布式文件系统中的管理者，主要负责管理文件系统的命名空间、集群配置信息和存储块的复制等，NameNode会将文件系统的Meta-data存储到内存中，这些信息主要包括了文件信息、每一个文件对应的文件块的信息和每一个文件块在DataNode的信息等。 Datanode: DataNode是文件存储的基本单元，他将Block存储在本地文件系统中，保存了Block的meta-data，同时周期性的将所有存在的Block信息发送给NameNode。slave存储实际的数据块，执行数据块的读写。 Client: 文件切分与NameNode的交互，获取文件位置信息；与DataNode交互，读取或者写入数据；管理HDFS；访问HDFS。 HDFS读取数据流程客户端将要读取的文件路径发送给Namenode，Namenode获取文件的元信息（主要是block的存放位置信息）返回给客户端，客户端根据返回的信息找到相应Datanode逐个获取文件的block并在客户端本地进行数据追加合并从而获得整个文件 HDFS写数据流程客户端要向HDFS写数据，首先要跟Namenode通信以确认可以写文件并获得接收文件block的Datanode，然后，客户端按顺序将文件逐个block传递给相应Datanode，并由接收到block的Datanode负责向其他Datanode复制block的副本。 MapReduce编程模型&emsp;&emsp;MapReduce的诞生也是由于对大规模数据处理的需求。在大型的互联网公司，比如说Google、亚马逊等，在他们的平台上每天都会产生大量的数据，单个的处理器不可能在有限的时间内完成计算。根据多线程和并行计算的启发，我们可以将这些计算分布在成百上千的的机器上，这些机器集群就可以看作硬件资源池，将并行的任务拆分，然后交由每一个空闲机器资源去处理，能够极大地提高计算效率。但是由此而来引发的问题是在这个分布式计算系统中应该如何合理的处理并行计算？如何分发数据？如何处理错误？为了避免对这些问题的考虑，我们希望获得这样一个抽象模型，在这个模型中我们只需要关注我们希望执行的任务，而不必关心并行计算、容错、数据分布、负载均衡等复杂的细节，MapReduce就是这样一个抽象模型。 &emsp;&emsp;MapReduce 是一个编程模型，也是一个处理和生成超大数据集的算法模型的相关实现。用户首先创建一 个 Map 函数处理一个基于 key/value pair 的数据集合， 输出中间的基于 key/value pair 的数据集合； 然后再创建 一个 Reduce 函数用来合并所有的具有相同中间 key 值的中间 value 值。MapReduce 架构的程序能够在大量的普通配置的计算机上实现并行化处理。这个系统在运行时只关心： 如何分割输入数据，在大量计算机组成的集群上的调度，集群中计算机的错误处理，管理集群中计算机之间 必要的通信。采用 MapReduce 架构可以使那些没有并行计算和分布式处理系统开发经验的程序员有效利用分布式系统的丰富资源。 执行过程 用户程序首先调用的 MapReduce 库将输入文件分成 M 个数据片度，每个数据片段的大小一般从 16MB 到 64MB(可以通过可选的参数来控制每个数据片段的大小)。然后用户程序在机群中创建大量 的程序副本。 这些程序副本中的有一个特殊的程序–master。副本中其它的程序都是 worker 程序，由 master 分配 任务。有 M 个 Map 任务和 R 个 Reduce 任务将被分配，master 将一个 Map 任务或 Reduce 任务分配 给一个空闲的 worker。 被分配了 map 任务的 worker 程序读取相关的输入数据片段，从输入的数据片段中解析出 key/value pair，然后把 key/value pair 传递给用户自定义的 Map 函数，由 Map 函数生成并输出的中间 key/value pair，并缓存在内存中。 缓存中的 key/value pair 通过分区函数分成 R 个区域，之后周期性的写入到本地磁盘上。缓存的 key/value pair 在本地磁盘上的存储位置将被回传给 master，由 master 负责把这些存储位置再传送给 Reduce worker 当 Reduce worker 程序接收到 master 程序发来的数据存储位置信息后， 使用 RPC 从 Map worker 所在的磁盘上读取这些缓存数据。当 Reduce worker 读取了所有的中间数据后，通过对 key 进行排序 后使得具有相同 key 值的数据聚合在一起。由于许多不同的 key 值会映射到相同的 Reduce 任务上， 因此必须进行排序。如果中间数据太大无法在内存中完成排序，那么就要在外部进行排序。 Reduce worker 程序遍历排序后的中间数据， 对于每一个唯一的中间 key 值， Reduce worker 程序将这 个 key 值和它相关的中间 value 值的集合传递给用户自定义的 Reduce 函数。Reduce 函数的输出被追加到所属分区的输出文件。 当所有的 Map 和 Reduce 任务都完成之后，master 唤醒用户程序。在这个时候，在用户程序里对 MapReduce 调用才返回。 Hadoop平台搭建&emsp;&emsp;Hadoop有三种安装模式：本地模式安装、伪分布模式安装和完全分布式安装。本文主要介绍Hadoop完全分布式安装，真实环境下都是以这种方式部署（1台Master，2台Slave）。 部署环境 Hadoop2.8.5 VMware14 ubuntu16.04 jdk11.01 第一步：安装ubuntu虚拟机的安装教程网上很多，这里不在赘述，注意这里只需要安装一个虚拟机，虚拟机的网络连接方式为NAT。 第二步：在虚拟机中安装jdk我使用的jdk版本是jdk11.01，建议使用jdk1.8。jdk安装完成后需要配置环境变量，jdk的默认安装路径是 1/usr/lib/jvm/jdk-11.01 编辑用户目录下.bashrc文件，在文件末尾添加 1234export JAVA_HOME=/usr/lib/jvm/jdk-11.01 export JRE_HOME=$&#123;JAVA_HOME&#125;/jre export CLASSPATH=.:$&#123;JAVA_HOME&#125;/lib:$&#123;JRE_HOME&#125;/lib export PATH=$&#123;JAVA_HOME&#125;/bin:$PATH 第三步：克隆虚拟机利用VMware的克隆功能，克隆两个虚拟机 第四步：修改hostname文件现在我们得到三个一模一样的虚拟机，我们选取其中一个虚拟机为Master，其余两个虚拟机分别为Slave。修改Master主机的hostname为Master，两个Slave的hostname分别为Slave1和Slave2，hosts文件的路径为 1/etc/hostname 第五步：配置静态IP分别查看三台主机的IP地址，然后修改hosts文件，将三台主机的hostname以及对于的IP添加到hosts文件中，hosts文件路径为，三台主机都要进行同样的操作。 1/etc/hosts 在我的系统中，三台主机的IP如图所示 第六步：建立Hadoop运行账号在三台主机上都要建立一个hadoop用户组，并在用户组中添加名为hduser的用户,具体操作如下。 12345sudo groupadd hadoop #建立hadoop用户组sudo useradd -s /bin/bash -d /home/hduser -m hduser -g hadoop #添加hduser，指定用户目录sudo passwd hduser #修改hduser用户密码sudo adduser hduser sudo #赋予hduser管理员权限su hduser #切换到hduser用户 第七步：配置ssh免密登录ubuntu默认安装了ssh客户端，但没有安装ssh服务器，配置之前先在三台主机中安装ssh-server 1sudo apt-get install 接着在Master主机中执行如下命令 12ssh-keygen -t rsa -P '' -f ~/.ssh/id_rsa #生成ssh公钥和私钥cat ~/.ssh/id_rsa.pub &gt;&gt; ~/.ssh/authorized_keys #添加公钥到已认证的key中 在两台Slave主机中的用户目录下新建.ssh文件夹，然后将Master中的id_rsa.pub文件复制到两台Slave主机的.ssh文件夹下，执行如下命令 1cat ~/.ssh/id_rsa.pub &gt;&gt; ~/.ssh/authorized_keys #添加公钥到已认证的key中 最后验证是否可以通过Master免密登录两台Slave主机。 第八步:下载并解压Hadoop三台主机都要进行该操作，在用户目录下建立名为hadoop2.8.5的目录，将文件解压到该目录下。 第九步：修改配置文件三台主机都要进行该操作 修改hadoop-env.sh文件,添加JAVA_HOME。（~/hadoop/hadoop2.8.5/etc/hadoop/hadoop-env.sh） 修改core-site.cml文件，添加如下内容。（~/hadoop/hadoop2.8.5/etc/hadoop/core-site.cml) 修改hdfs-site.xml文件，添加如下内容。（~/hadoop/hadoop2.8.5/etc/hadoop/hdfs-site.xml） 修改mapred-site.xml.template文件，添加如下内容。（~/hadoop/hadoop2.8.5/etc/hadoop/mapred-site.xml.template） 修改slaves文件，将两台Slave主机名添加进去即可。（~/hadoop/hadoop2.8.5/etc/hadoop/slaves） 修改/etc/profile文件，添加如下内容 123export JAVA_HOME=/usr/lib/jvm/jdk-11.0.1export HADOOP_INSTALL=/home/hduser/hadoop/hadoop-2.8.5export PATH=$PATH:$&#123;HADOOP_INSTALL&#125;/bin:$&#123;HADOOP_INSTALL&#125;/sbin:$&#123;JAVA_HOME&#125;/bin 然后执行 1source /etc/profile 检查hadoop是否安装成功 第十步：格式化namenode，并启动集群在Master主机中执行以下命令 12hadoop namenode -format #格式化namenodestart-all.ssh #启动集群 在ubuntu地址栏中输入http:Master:50070,可以看到Slave结点的相关信息]]></content>
      <categories>
        <category>Linux</category>
      </categories>
      <tags>
        <tag>Linux</tag>
        <tag>Hadoop集群</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[HITB安全峰会之旅.md]]></title>
    <url>%2FHITB%E5%AE%89%E5%85%A8%E5%B3%B0%E4%BC%9A%E4%B9%8B%E6%97%85.html%2F</url>
    <content type="text"><![CDATA[摘要：2018年10月25日，一次偶然的机会在阿里安全响应中心看到了一则新闻-HITB首次走进中国。当时也并没有太在意，因为本身对HITB没有什么印象，在安全领域我关注的会议并不多，比较了解的是BLACKHAT和DEFCON。正好前几天去合肥参加了科大讯飞的全球开发者大会，回来之后其实挺失望的（从学术界到产业界都在极力鼓吹“AI”所蕴含的价值，但是却始终看不到实质性的产品和突破，而且很大程度忽视了“AI”所带来的负面影响），当看到这则新闻的时候有感而发，在留言处写下了一点个人的感受。没想到的是，第二天居然收到通知说我的留言被小编抽中了，作为奖励我也免费获得了一张HITB安全峰会的门票。突如其来的惊喜让我有点措手不及，我赶紧上网查了一下关于HITB峰会的相关信息，这才对HITB有了一些了解。作为欧洲三大顶级安全会议之一，HITB一直拥有良好的口碑和技术氛围，我觉得这对我来说是一次很好的机会。之后，我也获得了导师的支持，于是2018年10月30日，我踏上了这次难忘的HITB安全峰会之旅。]]></content>
      <categories>
        <category>心得体会</category>
      </categories>
      <tags>
        <tag>网络安全</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[维吉尼亚加解密及唯密文破解]]></title>
    <url>%2F%E7%BB%B4%E5%90%89%E5%B0%BC%E4%BA%9A%E5%8A%A0%E8%A7%A3%E5%AF%86%E5%8F%8A%E5%94%AF%E5%AF%86%E6%96%87%E7%A0%B4%E8%A7%A3.html%2F</url>
    <content type="text"><![CDATA[摘要 古典密码体制主要通过字符间的置换和代换来实现，常见的置换密码包括列置换密码和周期置换密码，而常见的代换密码包括单表代换密码和多表代换密码，本文所讨论的维吉尼亚算法是属于多表代换密码的一种。多表代换密码是以一系列代换表依次对明文消息的字母序列进行代换的加密方法，即明文消息中出现的同一个字母，在加密时不是完全被同一固定的字母代换，而是根据其出现的位置次序用不同的字母代换。如果代换表序列是非周期的无限序列，则相应的密码称为非周期多表代换密码，这类密码对每个明文都采用了不同的代换表进行加密，故称为一次一密密码，它是理论上不可破译的密码体制。但实际应用中经常采用的是周期多表代换密码，它通常使用有限个代换表，代换表被重复使用以完成消息的加密。作为多表代换密码的典型代表，维吉尼亚密码算法蕴含着丰富的古典密码设计思想，本文将深入探讨维吉尼亚算法的加解密过程实现，以及利用统计分析的方法进行唯密文攻击。 文章概览 维吉尼亚算法简介 加密算法实现 编码方式 对明文进行处理 加密过程 解密算法实现 唯密文攻击 确定密钥长度 确定密钥 恢复明文 维吉尼亚算法简介加密算法实现&emsp;&emsp;实现加密算法的大致流程是：首先我们需要确定编码方式，本文采用的编码方式是[a-z]对应[0-25]；接着进行加密算法前需要对明文字符串进行处理，删除非字母字符，将大写字符统一转换为小写字母；最后选定密钥对密文中的逐个字符进行加密（即代换操作），生成最后的密文。 编码方式&emsp;&emsp;本文的字母编码方式由列表s确定，s中每个元素的索引即对应该元素的数字编码。 1s = ['a','b','c','d','e','f','g','h','i','j','k','l','m','n','o','p','q','r','s','t','u','v','w','x','y','z'] 对明文进行处理&emsp;&emsp;对明文进行处理的目的是去除明文中非字母的字符，并将大写字母统一转换为小写字母。转换大小写我们可以使用python字符串内置的lower()函数，稍微有点棘手的是前者，因为在这里需要考虑到一些效率的问题还有如何对后续操作进行优化的问题，比如说: 读取文件中的明文时我们可以采用read()，readline()，readlines()这三个函数，那我们到底采用哪一个呢？（这三个函数的对比可以参考这篇博客) 采用不同读取明文的函数，导致读取结果也不尽相同，有列表形式也有字符串形式，到底哪种形式对后续的操作更有好处 去掉读取后的明文中的非字母字符应采用何种方式？（逐个字符判断或者正则表达式） 本文根据文本的实际情况，采用的处理方式如下所示 12345678910def pretreatment(): """ pretreatment函数的主要作用是对明文进行预处理，去除非字母字符和转换大小写 :return: 经过预处理的明文字符串 """ with open("plain.txt","r") as f: wen = f.read() pattern = re.compile('[\n]|\d|\W') plain_1 = re.sub(pattern,'',wen).lower() return plain_1 加密过程&emsp;&emsp;维吉尼亚算法的加密过程比较简单，基本思想是利用密钥循环对明文字符进行代换操作，进行代换前将相应的明文字符和密钥字符转化为对应的数字编码，然后相加对26取余即得到对应的密文字符。 12345678910111213141516171819def encrypt(key): """ encrypt函数的主要作用是进行加密 :param key: 密钥 :return: 密文字符串 """ wen = pretreatment() num_key = key_to_num(key) ciphertext = '' k = 0 for w in wen: if k == len(num_key): k = 0 cipher = change(w,num_key[k]) cipher = num_to_char(cipher) ciphertext = ciphertext + cipher k += 1 wirte_txt(ciphertext,'crypt.txt') return ciphertext 解密算法实现&emsp;&emsp;解密算法是加密算法的逆过程，进行的代换操作是将密文字符的数字编码减去密钥字符的数字编码，如果相减的结果小于0，则令结果加上26，在转换为对应编码的字符。 123456789101112131415161718192021222324252627282930313233def de_change(ch,num): """ de_change函数的作用是根据密文字符和密钥还原明文字符 :param ch: 密文字符 :param num: 密钥编码 :return: 明文字符 """ ch_num = char_to_num(ch) result = ch_num - num if result &lt; 0: result = 26 + result return resultdef decrypt(key): """ decryption函数的主要作用是将密文解密成明文 :param key: 密钥 :return: 明文 """ with open('crypt.txt','r') as f: ciphertext = f.read() num_key = key_to_num(key) wen = '' k = 0 for c in ciphertext: if k == len(num_key): k = 0 w = de_change(c,num_key[k]) w = num_to_char(w) wen = wen + w k += 1 wirte_txt(wen,'result.txt') return wen 唯密文攻击&emsp;&emsp;某种语言中各个字符出现的频率不一样而表现出一定的统计规律，而这种统计规律可能在密文中重现，所以我们可以通过统计分析的手段进行一些推测和验证过程来实现对密文的分析。在英文字母中各个字母出现的频率如下所示， 1234#编码规则s = ['a','b','c','d','e','f','g','h','i','j','k','l','m','n','o','p','q','r','s','t','u','v','w','x','y','z']#字母出现频率frequency = [0.082,0.015,0.028,0.043,0.127,0.022,0.02,0.061,0.07,0.002,0.008,0.04,0.024,0.06,0.075,0.019,0.001,0.06,0.063,0.091,0.028,0.01,0.023,0.001,0.02,0.001] 对于维吉尼亚密码体制来说，我们可以通过统计分析的方法对其密文进行分析，从而获取明文信息。基于维吉尼亚密码体制的唯密文攻击的破解主要包含三个步骤： 确定密钥长度，常用的方法包括卡西斯基测试法和重合指数法，本文将采用后者进行分析 确定密钥，常用的方法是拟重合指数法 根据密文和密钥恢复明文 确定密钥长度&emsp;&emsp;本文采用重合指数法猜解密钥长度，关于重合指数法的具体解释可以参照《现代密码学教程》或者维基百科，本文主要讲解猜解密钥长度的实现过程。 1234567891011121314151617181920212223def guess_len_key(crypt): """ guess_len_key函数的主要作用是通过密文猜解密钥长度 :param crypt: 密文 :return: 密钥长度以及划为的子串 """ l = 1 d = &#123;&#125; while True: print("****************************假设密钥长度 为%s***********************************" % l) sum_index = 0.0 for i in range(len(crypt)): n = i % l if n not in d: d[n] = '' d[n] += crypt[i] sum_index = sum(coincidence_index(d[j]) for j in range(l)) / l if sum_index &gt;= 0.06 and sum_index &lt;= 0.07: break else: l += 1 d = &#123;&#125; return l,d 该算法的主要思想是将密文划分为l个子串，子串存放在字典d中。分别计算l个子串的重合指数，然后计算l个重合指数的平均数，如果该平均数位于[0.06,0.07]这个区间内，则说明密钥长度为l，返回密钥长度以及划分的l个子串；如果得到的平均数不在[0.06,0.07]这个区间内，则l自增，d初始化，进行下一轮猜解。 确定密钥&emsp;&emsp;确定密钥长度大致过程是：利用之前得到的l个子串，对每个子串都进行移位操作。假设现在对第i个子串进行移位操作（子串的每个字符移动相同的位数，最坏情况下对同一个子串需要进行26次移位操作），移动的位数为k,（k在[0-25]区间内，也就对应了[a-z]）。每进行一次移位操作，就对该子串计算一次拟重合指数，如果该拟重合指数位于[0.06,0.07]这个区间内，则说明此时移动的位数对应的s列表中的字符即为该子串的密钥；否则，继续进行下一次移位操作。 1234567891011121314151617181920212223def crack_key(): """ cracker函数的主要作用是破解密钥 :return: 返回密钥 """ with open("crypt.txt","r") as f: crypt = f.read() len_key,d = guess_len_key(crypt) key = '' print("\n-------------------------------------") print("| 经计算可知，密钥长度为%s |" % len_key) print("-------------------------------------\n") for i in range(len_key): substring = d[i] print("当前字串为：",d[i]) for n in range(26): dex = quasi_index(substring, n) print("假设子串移动&#123;&#125;,拟重合指数为&#123;:.4f&#125;".format(s[n],dex)) if dex &gt;= 0.06 and dex &lt;= 0.07: key += s[n] break print("******************************破解的最终密钥为%s*********************************" % key) 恢复明文&emsp;&emsp;恢复明文的过程与解密过程类似，这里不在详述。 系统运行演示加密 解密 猜解密钥长度 猜解密钥]]></content>
      <categories>
        <category>密码学</category>
      </categories>
      <tags>
        <tag>密码学</tag>
        <tag>维吉尼亚</tag>
        <tag>重合指数</tag>
        <tag>唯密文</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[raspberry入门配置]]></title>
    <url>%2Fraspberry%E5%85%A5%E9%97%A8%E9%85%8D%E7%BD%AE.html%2F</url>
    <content type="text"><![CDATA[摘要 最近准备在树莓派上搭建一个智能家居系统，更新系统的过程中不知道什么原因导致系统崩了，我的心顿时凉了半截。查阅了很多资料，没找到解决方法，只能重装系统（基于stretch版本）了。虽然之前的系统也是自己一步步配置的，但是这次重新配置的过程中还是遇到了很多问题，在这里记录一下，希望能给小伙伴们一些启发。 文章概览 重新安装系统 连接树莓派 安装远程桌面服务 更新软件源 raspi-config 配置无线网络 配置静态IP 配置内网映射 安装zsh python环境搭建 重新安装系统&emsp;&emsp;重新安装系统的过程我们需要用到：系统镜像、DiskGenius、Win32DiskImager，DiskGenius的作用是格式化TF卡，Win32DiskImager的作用是将系统镜像写入TF卡，具体的操作过程可以参考这篇博客。 连接树莓派&emsp;&emsp;对于如何连接树莓派，我在之前的博客中详细的讨论过，这里不再赘述，需要提醒大家的是只需要使用某一种方法连接树莓派即可。 安装远程连接服务&emsp;&emsp;ssh连接是通过命令行对树莓派进行远程操作，而远程桌面是直接通过树莓派的GUI界面进行操作，操作简单，交互性好。xrdp是一个开源的远程桌面服务器，支持windows远程桌面连接，但是需要使用tightvncserver作为其基础服务，具体安装操作如下所示。 123sudo apt-get update #更新sudo apt-get install xrdpsudo apt-get install tightvncserver 安装好以上两个服务以后。可以使用windows自带的远程连接工具连接到树莓派。 更新软件源&emsp;&emsp;在更新软件源的时候，大家注意查看自己的系统版本（推荐大家安装最新的系统版本），具体操作参见清华大学开源软件镜像站。 raspi-config&emsp;&emsp;通过远程桌面连接到树莓派之后，系统会提示进行一些初始化配置，包括拓展内存、设置时区、语言等等，具体操作参见这篇博客。 配置无线网络&emsp;&emsp;配置无线网络有两种方式，一个是在图形化界面直接选择连接的ssid，输入密码即可，系统会保存该无线网络的相关信息到/etc/wpa_supplicant/wpa_supplicant.conf文件；另外一种方式是直接修改该配置文件，将无线网络配置信息添加到该文件中。 123456789country=CNctrl_interface=DIR=/var/run/wpa_supplicant GROUP=netdevupdate_config=1network=&#123; ssid="***" psk="***" priority=1 &#125; 配置静态IP&emsp;&emsp;由于每次树莓派连接路由器的时候，路由器会分配不同的IP地址，所以当我们连接树莓派的时候每次都要通过路由器查看树莓派的IP地址，这样比较麻烦，所以，我们需要给路由器指定静态的IP。修改/etc/dhcpcd.conf文件（一定要注意不是修改/etc/network/interfaces文件），在后面添加以下内容： 1234567891011interface eth0 #有线 static ip_address=192.168.0.10/24static routers=192.168.0.1static domain_name_servers=192.168.0.1 interface wlan0 #无线连接 static ip_address=192.168.0.200/24static routers=192.168.0.1static domain_name_servers=192.168.0.1 配置内网映射&emsp;&emsp;如果想要从外网直接访问树莓派，那我们需要将树莓派的内网IP映射到公网当中，这里我们使用的映射工具是花生壳，具体操作参见官方教程。 安装zsh&emsp;&emsp;树莓派基于linux操作系统，其终端shell默认的是bash，而zsh是比bash更加强大的shell，而且更加美观，具体配置参见这篇博客。 python环境搭建&emsp;&emsp;树莓派中内置了两个版本的python，python2.7和python3.5，系统默认版本为python2.7。在进行系统环境配置和相关依赖安装的过程中，一律使用系统默认版本即python2.7（如果切换至python3.5.会出现各种各样的问题）。在程序开发过程中如果需要使用python3.5，可以切换python环境，具体操作参见这篇博客。]]></content>
      <categories>
        <category>树莓派</category>
      </categories>
      <tags>
        <tag>python</tag>
        <tag>树莓派</tag>
        <tag>内网穿透</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[如何连接树莓派]]></title>
    <url>%2F%E5%A6%82%E4%BD%95%E8%BF%9E%E6%8E%A5%E6%A0%91%E8%8E%93%E6%B4%BE.html%2F</url>
    <content type="text"><![CDATA[摘要 早就听说了树莓派的大名，什么智能家居，智能机器人，无人机等等它都不在话下。我最近刚刚入手了一款树莓派3B+，想利用它来开发一个智能家居的控制系统。由于从来没有接触过相关的硬件，在配置过程中确实遇到了不少坑，在这里简单的记录一下，希望能给读者一些启发。本文主要讲解如何连接树莓派的问题。 文章概览 基本配置 系统安装 连接树莓派 有路由器和网线的情况下 有网线没有路由器的情况下 没有路由器没有网线的情况下 基本配置&emsp;&emsp;想要玩树莓派，仅仅买一块主板是不够的，基本的配置包括：一块主板（大概225RMB）、一个读卡器（大概10RMB）、一根电源线（大概8RMB）、一张大于8G的内存卡（我买的是32G的，38RMB）、散热片（大概5RMB）和一个保护外壳（大概20RMB），推荐大家主板和配件分开买，这样比较划算。 系统安装&emsp;&emsp;树莓派的安装过程比较简单，首先我们需要下载准备安装的系统镜像，在树莓派官网可以下载到相关的镜像文件，树莓派支持的系统很多，包括ubuntu、kail、win10等等。我选择的系统是树莓派官方推荐的RSAPBIAN，基于Debian，稳定兼容性好。然后我们需要下载一款工具：win32diskimager，我们需要利用这款工具将树莓派的系统镜像写入内存卡。 &emsp;&emsp;准备工作完成后，我们通过读卡器将内存卡接入电脑，在利用win32diskmager工具将系统镜像写入内存卡（这个过程很简单，这里不再详细解释）。镜像写入完成之后，我们的系统就成功的安装到内存卡中。但是，有一个问题需要我们注意，镜像写入完成后系统会弹出一个对话框，大概的意思是：无法识别内存卡中的数据，是否要将内存卡格式化，这个时候大家一定选择否或者直接关闭对话框。因为系统写入完成之后，我们的主机只能读出内存卡中系统的boot分区（大概只有40多MB），内存卡中的其他分区我们的系统识别不出来，所以才会弹出这个对话框，一旦我们选择格式化，刚才安装的系统会被删除。如果有小伙伴不小心选择了格式化，那就只有重新安装系统了，大家可以参考这篇博客。 &emsp;&emsp;还有一个需要注意的问题是，树莓派默认没有开启ssh服务，所以之后我们如果需要通过ssh连接树莓派时，我们需要在刚刚写入系统的内存卡的boot文件夹下，建立一个文件名为ssh的空文件（无后缀名），这样在之后的操作中我们就能通过ssh连接树莓派。 连接树莓派&emsp;&emsp;连接树莓派最简单的方法就是通过HDMI数据线连接显示屏，这样我们可以直接通过显示屏对树莓派进行操作。那如果我们没有显示屏，我们该如何连接树莓派呢？ 有路由器和网线的情况下&emsp;&emsp;这种情况下连接树莓派也比较简单，我们只需要将网线的一端接入路由器的lan接口，一端接入树莓派的网络接口（树莓派会自动获取IP地址），同时我们的主机也连接在路由器所建立的局域网内，此时我们可以通过路由器查看树莓派的IP地址。获取树莓派IP地址之后，我们通过主机中的xshell或者putty等远程连接工具就可以连接到树莓派。 有网线没有路由器的情况下&emsp;&emsp;没有了路由器，树莓派就不能获取到IP地址，这种情况稍微复杂一点。解决办法是：将网线的两端连接树莓派和主机，然后我们打开主机的网络和共享中心 点击我们已经连接的网络，查看其属性 点击共享按钮，勾选允许其他网络用户通过此计算机的Internet连接来连接（N） 接下来我们回到网络和共享中心，点击未识别的网络那一栏对于的以太网选项，查看其详细信息，可以看到其IP地址，这里的IP为192.168.137.1 然后我们拔掉网线，关掉树莓派，拔出内存卡，将其通过读卡器连接到主机上。我们进入内存卡的boot文件夹，修改comline.txt文件，将 ip = 192.168.137.100 这句话添加到开头。完成上述步骤之后，我们将内存卡插入树莓派，用网线重新连接树莓派和主机，启动树莓派电源。我们通过远程连接工具连接树莓派，此时树莓派的IP地址为刚才设置的192.168.137.100，不出意外的话，我们也能连接上树莓派。 没有路由器没有网线的情况下&emsp;&emsp;要使我们的主机能够远程连接到树莓派上，我们必须满足一个条件：主机和树莓派位于同一个局域网。没有路由器和网线的情况下，我们可以通过手机热点建立一个局域网环境，让树莓派和主机同时连接手机热点。现在需要解决的问题是如何才能让树莓派连接上手机的热点。开机状态下，树莓派的无线模块一直处于工作状态，我们需要将无线热点的相关配置文件写入系统。同样，我们通过读卡器读取内存卡boot文件夹，在文件夹下新建文件名为wpa_supplicant.conf的文件，在文件内写入手机热点的配置信息并保存。 然后重新插入内存卡，开启树莓派，在手机上查看树莓派的IP地址，用远程连接工具进行连接。]]></content>
      <categories>
        <category>树莓派</category>
      </categories>
      <tags>
        <tag>树莓派</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[解释型语言python]]></title>
    <url>%2F%E8%A7%A3%E9%87%8A%E5%9E%8B%E8%AF%AD%E8%A8%80python.html%2F</url>
    <content type="text"><![CDATA[摘要 计算机不能直接理解高级语言，只能直接理解机器语言，所以必须要把高级语言翻译成机器语言，计算机才能执行高级语言编写的程序。由于翻译方式的不同，习惯上我们大致把高级语言分为两类，即编译型语言和解释型语言。对于这两种类型的编程语言，很多人在理解层面上存在盲点，本文将对这两种类型的编程语言进行探讨，帮助读者更好的理解这一问题。 文章概览 编译型语言和解释型语言 基本解释 优缺点 python python解释器 python代码执行过程 编译型语言和解释型语言基本解释&emsp;&emsp;对于编译型语言，我们以C语言为例，C语言在执行过程中，先要将源程序编译为目标文件（机器代码），该目标文件是与平台相关的，也就是说ARM生成的目标文件，不能被用于MIPS的CPU，也不能用于x86的CPU。目标文件经过连接操作就可以生成可执行文件，以后我们想再次运行这段代码时，不必进行编译操作，只需要直接执行生成的可执行文件即可。 &emsp;&emsp;对于解释型语言呢，我们不需要执行编译过程，程序在执行时直接由解释器逐句地对程序进行解释，转换为机器可以执行的代码。但是对于有些解释型语言来说，也需要进行编译操作，比如Java。Java程序在执行过程中先要将源代码编译成字节码文件，然后再由解释器对字节码文件逐句进行解释，所以说Java是一种先编译后解释的语言。（注：Java为了实现跨平台的特性，专门在从高级语言代码转换至机器码过程的中间加入了一层中间层JVM（java虚拟机），Java首先依赖编译器将代码（.java）编译成JVM能识别的字节码文件（.class），然后由JVM解释并执行该字节码，也可结合JIT（just-in-time compilation即时编译）技术，将解释生成的机器码转换为更高效的本地机器码，且该机器码可被缓存，来提高重复执行的效率。) &emsp;&emsp;常见的编译型语言包括：C/C++、Pascal等，常见的编译型语言包括：Java、JavaScript、VBScript、Perl、Ruby、MATLAB 等。 优缺点 编译型语言可以做到一次编译，多次运行，执行效率比较高；而解释型语言在每次执行时都需要解释器进行解释，执行效率较低（但是我们也不能一概而论，一些解释型语言也可以通过解释器的优化来在对程序做出翻译时对整个程序做出优化，从而在效率上超过编译型语言）。 编译型语言的执行依赖于平台，生成的可执行文件不能运行在其他平台，需重新编译，跨平台的性较差；而解释型语言的执行依赖于解释器，各个平台都有相应的解释器，解释器会将程序解释成基于当前机器指令集的机器码并执行，所以解释型语言可以很好的移植到其他平台，具有很好的跨平台性。 编译型语言，在编译阶段即可发现常见的语法或者链接等错误，此机制可在运行前帮助程序员排查出可能潜在的语法、语义和类型转换错误，编译型语言一般都有明确的变量类型检测，也被称作强类型语言，即编译型语言至少能确保所生成的可执行文件肯定是可运行的，至于执行的逻辑不对则属于程序员业务逻辑错误范畴了。而对于解释型语言，代码中的错误必须直到运行阶段方可发现，由此造成的困惑是：往往一段程序看不出问题但却在运行阶段错误连连且需要一个个排查：变量拼写错误、方法不存在等。但也正是基于解释是在运行期执行转化的特性，一般的解释型语言通常都有自己的shell，可以在不确定某些执行结果时立即“动手执行”试一下，这就比每次都需要编译后才能运行并看到结果省去不少时间。 python&emsp;&emsp;通过上面对编译型语言和解释型语言的分析，我们可以得出结论，python是属于解释型语言的一种。python类似于Java，为了效率上的考虑，也提供了编译方式，编译后生成的也是字节码的文件形式，并由Python的的VM（虚拟机）的去执行。不同点在于，Python的编译并非强制执行的操作，确切来说Python的编译是自动的，通常发生在对某个模块（module）的调用过程中，编译成字节码的可以节省加载模块的时间，以此达到提高效率的目的。可见，某些先进的高级语言在对编译和解释方面的拿捏舍去，都采取了一种：两手抓，两手都要硬的态度。 python解释器&emsp;&emsp;由于整个Python语言从规范到解释器都是开源的，所以理论上，只要水平够高，任何人都可以编写Python解释器来执行Python代码（当然难度很大）。事实上，确实存在多种Python解释器 CPython: 这个解释器是用C语言开发的，所以叫CPython。在命令行下运行python就是启动CPython解释器,它是使用最广的Python解释器. IPython是基于CPython之上的一个交互式解释器，也就是说，IPython只是在交互方式上有所增强，但是执行Python代码的功能和CPython是完全一样的。CPython用&gt;&gt;&gt;作为提示符，而IPython用In [序号]:作为提示符。 绝大部分Python代码都可以在PyPy下运行，但是PyPy和CPython有一些是不同的，这就导致相同的Python代码在两种解释器下执行可能会有不同的结果。PyPy是另一个Python解释器，它的目标是执行速度。PyPy采用JIT技术，对Python代码进行动态编译（注意不是解释），所以可以显著提高Python代码的执行速度。 Jython是运行在Java平台上的Python解释器，可以直接把Python代码编译成Java字节码执行。 python代码执行过程&emsp;&emsp;参考这篇博客和这篇博客]]></content>
      <categories>
        <category>python</category>
      </categories>
      <tags>
        <tag>python</tag>
        <tag>解释型语言</tag>
        <tag>编译式语言</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[决策树算法详解与python实现：ID3和CART]]></title>
    <url>%2F%E5%86%B3%E7%AD%96%E6%A0%91.html%2F</url>
    <content type="text"><![CDATA[摘要 决策树是一种基本的分类与回归方法，本文主要讨论用于分类的决策树，决策树模型呈树形结构，在分类问题中，表示基于特征对实例进行分类的过程，它可以认为是if-then规则的集合，也可以认为是定义在特征空间与类空间上的条件概率分布，其主要优点是模型具有可读性，分类速度快。学习时，利用训练数据，根据损失函数最小化原则建立决策树模型。预测时，对新的数据，利用决策树模型进行分类。决策树学习通常包括三个步骤：特征选择、决策树的生成以及决策树的修剪。本文将主要讲解ID3和CART算法的原理和实现细节。 文章概览 ID3算法 特征选择 决策树的生成 ID3算法的缺陷 C4.5算法对ID3算法的改进 CART算法 特征选择 决策树生成 剪枝 ID3算法&emsp;&emsp;ID3算法是由澳大利亚计算机科学家Ross Quinlan提出的，它是构建决策树中一种非常重要的算法。在设计算法的过程中，它首次采用了信息增益准则来进行特征选择，这很大程度上推动了决策树算法的发展。 特征选择&emsp;&emsp;我们可以将决策树看作是if- then规则的集合，使用决策树模型进行预测的过程就相当于对if - then规则进行判断，那我们可以想到如果if -then规则越多，也就是决策树越复杂，那么预测所需要的时间越长，所以为了不断优化决策树的决策过程，我们需要合理的构建决策树，那么如何来选择if - then的决策规则至关重要。 &emsp;&emsp;在ID3算法中，我们通过信息增益作为决策规则。信息增益 = 信息熵 - 条件熵，信息熵代表随机变量的不确定度，条件熵代表在一定条件下，随机变量的复杂度，所以信息增益表示在一定条件下信息复杂度减少的程度。信息增益越大说明该决策规则的区分度越高，在构建决策树时，我们选取信息增益最大的特征作为决策规则。 123456789101112131415161718def emp_entropy(y_data): ''' emp_entropy函数的主要功能是计算数据集的经验熵 :param y_data: 数据集的类别 :return: 返回数据集的经验熵 ''' count = &#123;&#125; emp = 0.0 m = len(y_data) for y in y_data: if y in count: count[y] += 1 else: count[y] = 1 for i in count.keys(): info = (1.0 * count[i] / m) emp = emp + info * math.log(info,2) return emp emp_entropy函数的主要功能是计算数据集合的经验熵，经验熵的计算公式可以参考《统计学习方法》，同样，下面涉及到条件熵、信息增益的计算公式也可参考本书。代码中字典count的主要作用是统计数据集中不同类别出现的次数，emp即是信息增益。 1234567891011121314151617181920212223def emp_cond_entropy(x_data,y_data,feature): ''' emp_cond_entropy函数的主要作用是计算经验条件熵 :param x_data: 数据集 :param y_data: 数据集类别 :param feature: 数据集特征特征 :return: 数据集的经验条件熵 ''' count_y = &#123;&#125; emp_cond = 0.0 m = len(y_data) fea = x_data[:,feature] for i in range(len(fea)): if fea[i] in count_y: count_y[fea[i]].append(y_data[i]) else: count_y.setdefault(fea[i]) count_y[fea[i]] = [] count_y[fea[i]].append(y_data[i]) for e in count_y.keys(): l = len(count_y[e]) emp_cond = emp_cond + (1.0 * l / m) * emp_entropy(count_y[e]) return emp_cond emp_cond_entropy函数的主要作用是计算经验条件熵，fenture表示数据的某一维特征，对于离散性特征（ID3算法不能处理连续型特征）来讲，特征的取值有多个，这里的count_y就是来统计该特征中不同取值的数据分布情况，列表fea表示的即是该数据集中该特征对应的值，emp_cond表示的是将该特征作为决策规则时的条件熵。 123456789101112131415def choose_feature(x_data,y_data): ''' choose_feature函数的主要作用是从数据集中选择信息增益最大的特征 :param x_data: 数据集 :param y_data: 数据集类别 :return: 信息增益最大的特征 ''' n = np.size(x_data,1) count = [] emp = emp_entropy(y_data) for i in range(n): emp_cond = emp_cond_entropy(x_data,y_data,i) count.append(emp - emp_cond) feature = count.index(min(count)) return feature choose_feature函数的主要作用是从数据集中选择信息增益最大的特征，算法的思路就是对数据集进行遍历，计算每一个特征的信息增益，返回信息增益最大的特征。（由于在计算经验熵的过程中没有添加负号，所以我这里取的是负数的最小值，也就是正数的最大值） 决策树的生成&emsp;&emsp;特征树的生成过程其实是一个递归过程，我们首先选择一个特征，作为根结点，根据根结点的不同取值，将数据集分为几个不同的部分，同时将该特征从数据集中删除。然后再对这几个不同的部分进行同样的操作，直到数据集类别相同或者没有特征为止。 1234567891011121314151617181920212223def create_tree(x_data,y_data,feature_list_data): ''' create_tree函数的主要作用是构建决策树 :param x_data: :param y_data: :param feature_list: :return: 返回决策树 ''' feature_list = feature_list_data[:] if is_all_same(y_data): return y_data[0] if len(x_data) == 0: return node_classfy(y_data) feature = choose_feature(x_data,y_data) node_name = feature_list[feature] tree = &#123;node_name:&#123;&#125;&#125; del feature_list[feature] count_x,count_y = feature_split(x_data,y_data,feature) for i in count_x.keys(): fealist = feature_list[:] count_x_del = del_feature(count_x[i],feature) tree[node_name][i] = create_tree(count_x_del,count_y[i],fealist) return tree feature_list的作用是复制feature_list_data，因为后面要进行删除操作，我们要保证删除操作只能影响函数内部变量，不能对函数的实参造成影响。我们生成的决策树保存在tree字典中，每执行一次递归操作，相当于将当前特征作为一个字典的key，递归操作返回的即是一个子树（即字典）。 ID3算法的缺陷&emsp;&emsp;通过对ID3算法进行分析，我们可以知道，ID3算法主要存在以下缺陷： ID3没有考虑连续型特征，数据集的特征必须是离散型特征 ID3算法采用信息增益大的特征优先建立决策树的结点，但是再计算信息增益的过程中我们发现，在相同条件下，取值比较多的特征比取值少的特征信息增益大 ID3没有对缺失值情况进行处理，现实任务中常会遇到不完整的样本，即样本的某些属性值缺失。 没有考虑过拟合问题 C4.5算法对ID3算法的改进&emsp;&emsp;C4.5算法是对ID3算法存在的缺陷进行改进的一种算法，它通过将连续特征离散化来解决ID3算法不能处理离散型数据的问题（这个会在后面的CART算法中讲到）；通过引入信息增益比来解决信息增益的缺陷；通过增加剪枝操作来解决过拟合的问题。 CART算法&emsp;&emsp;CART算法是一种应用广泛的决策树算法，它的基本流程与C4.5算法类似，它既可以应用于回归任务，也可以应用于分类任务（这里主要讲解分类树），需要注意的是CART算法生成的决策树是二叉树，而ID3和C4.5算法生成的决策树不一定是二叉树。 特征选择&emsp;&emsp;CART算法的决策规则由基尼指数决定，选择基尼指数最小的特征及其切分点作为最优特征和最优切分点。 1234567891011121314151617def Gini_index(y_data): ''' Gini_index函数的主要作用是计算数据集的基尼指数 :param y_data: 数据集类别 :return: 返回基尼指数 ''' m = len(y_data) count = &#123;&#125; num = 0.0 for i in y_data: if i in count: count[i] += 1 else: count[i] = 1 for item in count.keys(): num = num + pow(1.0 * count[item] / m,2) return (1.0-num) 在前面提到过ID3算法只能处理离散型特征，而CART算法既能处理离散型特征，又能处理连续型特征。CART算法处理连续型特征的方法与C4.5算法类似，都是将连续特征离散化，即将连续特征的所有取值进行排序，然后计算相邻取值的平均值作为切分点，在以此计算基尼指数。 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556ef Gini_D_A(x_data,y_data,feature): ''' Gini_D_A函数的主要作用是计算某一离散特征各个取值的基尼指数，选取最优切分点 :param x_data: 数据集合 :param y_data: 数据集类别 :param feature: 特征 :return: 该特征的最优切分点 ''' Gini_data = list(x_data[:,feature]) y_data = list(y_data[:]) m = len(Gini_data) Gini = &#123;&#125; classfy_data = &#123;&#125; for e in range(m): if Gini_data[e] not in classfy_data: classfy_data[Gini_data[e]] = [] classfy_data[Gini_data[e]].append(y_data[e]) for item in classfy_data.keys(): l1 = len(classfy_data[item]) r = y_data[:] for i in classfy_data[item]: r.remove(i) l2 = len(r) num = 1.0 * l1 / m * Gini_index(classfy_data[item]) + 1.0 * l2 / m * Gini_index(r) Gini[item] = num sor = sorted(Gini.items(), key=operator.itemgetter(1)) return sor[0]def Gini_continuous(x_data,y_data,feature): ''' Gini_continous函数的主要作用是计算某一连续特征各个取值的基尼指数，选取最优切分点 :param x_data: 数据集合 :param y_data: 数据集类别 :param feature: 特征 :return: 该特征的最优切分点 ''' Gini_data = list(x_data[:,feature]) m = len(Gini_data) y_data = list(y_data[:]) sort_data = sorted(Gini_data) Gini = &#123;&#125; split_point = [] for i in range(m-1): num = (sort_data[i] + sort_data[i+1]) / 2.0 split_point.append(num) for e in split_point: count_y = &#123;0:[],1:[]&#125; for k in range(m): if Gini_data[k] &lt;= e: count_y[0].append(y_data[k]) else: count_y[1].append(y_data[k]) cal = 1.0 * len(count_y[0]) / m * Gini_index(count_y[0]) + 1.0 * len(count_y[1]) / m * Gini_index(count_y[1]) Gini[e] = cal sor = sorted(Gini.items(), key=operator.itemgetter(1)) return sor[0] 当数据集中同时含有离散型变量和连续型变量时，进行特征选择就稍微有些复杂了，以下便是特征选择的代码,这里需要注意的是对不同类型特征的标识和对计算基尼指数时返回值的统一处理。dis_or_con是一个列表，用来标识特征是连续型还是离散型，0表示离散，1表示连续。 12345678910111213141516171819def choose_feature(x_data,y_data,dis_or_con): ''' choose_feature函数的主要作用是从各个特征的各个切分点中选择基尼指数最小的切分点 :param x_data: 数据集合 :param y_data: 数据类别 :return: 切分点 ''' w = np.size(x_data,axis=1) count = [] count_label = &#123;&#125; for i in range(w): if dis_or_con[i] == 0: a = Gini_D_A(x_data,y_data,i) else: a = Gini_continuous(x_data,y_data,i) count.append(a[1]) count_label[i] = a id = count.index(min(count)) return id,count_label[id][0] 决策树的生成&emsp;&emsp;决策树的生成大致与ID3算法类似 1234567891011121314151617181920def create_tree(x_data,y_data,dis_or_con_data,feature_list_data): feature_list = feature_list_data[:] dis_or_con = dis_or_con_data[:] if dis_or_con == []: return most_y_data(y_data) if is_all_same(y_data): return y_data[0] w,f = choose_feature(x_data,y_data,dis_or_con) count_x, count_y = feature_split(x_data,y_data,w,f,dis_or_con[w]) node_name = feature_list[w] tree = &#123;(node_name,f):&#123;&#125;&#125; del feature_list[w] del dis_or_con[w] for i in count_x.keys(): fealist = feature_list[:] dis_con = dis_or_con[:] count_x_del = del_feature(count_x[i], w) tree[(node_name,f)][i] = create_tree(count_x_del, count_y[i], dis_con,fealist) return tree 剪枝&emsp;&emsp;本文暂未实现剪枝算法，有待后续补充]]></content>
      <categories>
        <category>机器学习算法</category>
      </categories>
      <tags>
        <tag>机器学习</tag>
        <tag>决策树</tag>
        <tag>ID3</tag>
        <tag>CART</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[验证码识别：找回四六级准考证号]]></title>
    <url>%2Fverify-code.html%2F</url>
    <content type="text"><![CDATA[摘要 一晃时间过的真快，距离上次更新博客已经将近10天了，这十天来也没闲着，回家终于把杀千刀的科目三过了，再也不用看到教练那张凶神恶煞的脸。前段时间四六级考试成绩公布了，小伙伴们是不是都第一时间忙着去查自己的成绩，相信有很多小伙伴跟我一样苦逼，幸幸苦苦复习了好长时间，查成绩的时候却忘了自己的准考证号（温馨提示：以后考试之前一定要记得把准考证拍一张存起来）。在网上试过无数种找回办法后，我彻底绝望了。既然别人不靠谱，咱就靠自己，经过两天的努力之后，终于成功的找回了准考证号。这篇博客主要来介绍解决这个问题的一些方法和思路。 文章概览 基本思路 训练模型 获取训练数据 处理数据 生成模型 查询操作 发送请求 使用代理 多线程 使用教程 基本思路&emsp;&emsp;对于查询四六级成绩来说，官方的查询入口有学信网和中国教育考试网，查询成绩需要提交的数据包括准考证号、姓名和验证码。要想查询到成绩，最简单的办法就是手工枚举准考证号，一个一个的尝试。我们知道四六级准考证的组成如下所示（第10位表示类别，四级是1，六级是2）： 也就是说对于在同在一个考点的人来说前十位都是一致的（四级和六级不同），后面五位分别表示考场号和座位号（座位号从01到30），在我们忘记了考场号和座位号的情况下，我们至少要手工枚举几千次才有可能查询到成绩，这个工作难度可想而知。那如果我们不采用手工的方式进行枚举，而采用程序自动进行枚举呢？通过程序枚举准考证号不是什么问题，但是查询参数中包含验证码，现在需要解决地就是如何识别验证码。对于验证码地识别问题，我们可以利用机器学习的相关算法，建立识别模型，再利用识别模型来进行识别验证码。对于学信网和中国教育考试网两个网站，它们采用的验证码不同，学信网的验证码比较复杂，包含汉字等特殊字符，识别难度大，而中国教育考试网的验证码相对来说比较常规，识别难度相对小一点，本文的查询操作都是基于后者而言的。&emsp;&emsp;那么我们解决问题地大致思路就是：首先我们要获取大量的验证码数据，然后选择算法训练识别验证码的模型，最后通过重复识别查询页面的验证码，提交查询数据，分析响应数据来获得最终的结果。 训练模型 获取训练数据&emsp;&emsp;通过抓取请求相应过程中的数据包，我们可以得到获取验证码的地址。 其中ik表示准考证号，我们可以随便填一个，t表示时间戳（这个可以不用管），我们可以不断地向这个地址发送请求，服务器的响应结果即为验证码的地址，我们再向获取到的验证码的地址发送请求，就可以得到验证码。 具体代码如下所示（该项目的所有代码都可以在我的Github中找到）： 12345678910# 获取验证码def save_image_to_file(): myid = "123456789110211" new_id = myid.format(id=myid) img_api_url = image_api.format(id=new_id) img_api_resp = requests.get(img_api_url, headers=img_api_headers,timeout=10) img_url, filename = get_image_url_and_filename(img_api_resp.text) r = requests.get(img_url) with open("images/raw_picture/" + filename, "wb+") as f: f.write(r.content) 处理数据&emsp;&emsp;获取到一定数量的验证码图片后（大概需要100多张，收集的图片越多越好，之后我们会讲到一种快速收集和标注验证码的方法），接下来我们需要对获取到的验证码进行相应的处理。因为对于验证码的识别，我们一般采取监督学习的算法训练模型，所以首先要对获取到的验证码进行标注，即将验证码图片的文件名改为验证码对应的数字和字母组合，这一步必须要人工进行操作。然后，为了提高验证码识别的准确率，训练更好的识别模型，我们需要对验证码图片进行相应的处理，如灰度处理、二值化、降噪。经过这些手段处理后的验证码更能体现出图片本身的特征，同时也减小了训练模型时的计算量，具体代码如下所示。 1234567891011121314# 灰度处理，二值化（降噪部分的代码去掉了，效果不是太理想）def img_denoise(img, threshold): def init_table(threshold=threshold): table = [] for i in range(256): if i &lt; threshold: table.append(0) else: table.append(1) return table img = img.convert("L").point(init_table(), '1') return img &emsp;&emsp;下面我们要对验证码进行分割，因为在识别的时候，我们是识别单个的数字或字母，所以我们要将验证码进行切分，提取出每个字符对应的区域，切割后的每张图片大小一致。 1234567891011121314# 图片分割,参数img_split_start指定起始位置，参数img_split_width指定切割图片宽度def img_split(img,img_split_start,img_split_width): start = img_split_start width = img_split_width top = 0 height = img.size[1] img_list = [] for i in range(4): new_start = start + width * i box = (new_start, top, new_start + width, height) piece = img.crop(box) #piece.save("%s.jpg" % i) img_list.append(piece) return img_list &emsp;&emsp;图片切割完成后，数据处理的最后一步是将切割后的图片转化为numpy array的形式。 123456# 将Image对象转换为array_listdef img_list_to_array_list(img_list): array_list = [] for img in img_list: array_list.append(array(img).flatten()) return array_list 以上这些操作大家可以在我的GitHub的项目文件中通过preprocessing()、make_train_data()和img_to_array()三个函数实现。 生成模型&emsp;&emsp;生成模型主要用到的就是sklearn机器学习库中相关的算法，验证码识别属于分类任务，对于分类任务我们可以采用K近邻、支持向量机、决策树和神经网络等算法，这里我们采用的是支持向量机。 123456789# 训练模型def svm_model(x_data,y_data): SVM = svm.SVC() x_train,x_test,y_train,y_test = train_test_split(x_data,y_data,random_state=14) SVM.fit(x_train,y_train) y_predict = SVM.predict(x_test) average_accuracy = np.mean(y_test==y_predict)*100 print("准确率为：&#123;0:.1f&#125;%".format(average_accuracy)) pickle.dump(SVM, open("model.pkl", "wb+")) 模型训练好之后，将模型对象存储在model.pkl文件中，需要识别验证码时，只需要读取model.pkl文件即可获得识别模型，不需要再次训练。 查询操作发送请求&emsp;&emsp;模型训练好之后，我们就可以进行查询操作了。这一阶段的大致思路是，先获取查询页面的验证码，通过识别模型进行识别，然后再向服务器提交请求参数，包括枚举的准考证号、姓名和验证码。如果服务器返回验证码错误，则重复以上操作。如果服务器返回查询结果为空则说明验证码正确，但是准考证号和姓名不一致，此时可以枚举下一个准考证号，重复操作一直到获得正确结果为止。 &emsp;&emsp;由于一开始我们训练模型时使用的训练数据量很小，所以该识别模型识别的准确率比较低，那么如何提高模型识别的准确率呢。最好的办法就是增大训练数据的数量，训练新的模型。这里提供一个更快更方便获取训练数据的方法，在发送请求的代码中，我们加入两行代码（倒数第三行和倒数第二行），该代码的作用时将识别正确的验证码加入到训练数据的文件夹中，并且会自动进行标注，可以通过该方式一边查询，一边收集大量的训练数据。我的项目中，一开始手工标注的验证码有200张，训练模型后采用这种方式自动收集了1600多张验证码，然后利用所有的训练数据重新建立模型，识别的准确率提高了30%。（但是这样的做法存在一个过拟合的问腿，训练模型对于类似于一开始200张验证码的图片的识别准确率比较高，而对于其他类型的图片识别的准确率比较低。不过这个问题对于我们找回准考证号影响不大，提高准确率最好的就是一开始手工标注更多的验证码） 123456789101112131415161718192021222324252627282930313233343536# 发送请求def send_query_until_true(num): # 生成准考证号 global proxy new_id = myid.format(id=num) # 获取验证码图片地址 img_api_url = image_api.format(id=new_id) while True: try: img_api_resp = requests.get(img_api_url, headers=img_api_headers,timeout=10,proxies=proxy) img_url, filename = get_image_url_and_filename(img_api_resp.text) # 获取验证码图片并猜测 img_resp = requests.get(img_url, timeout=10, proxies=proxy) if img_resp.status_code == 200: images = Image.open(BytesIO(img_resp.content)) code = img_verify_code(images) else: code = "xxxx" except Exception: print("重新获取代理") p = str(get_proxy()) proxy = &#123;'http': 'http://' + p, 'https': 'http://' + p&#125; else: break # CET4成绩查询选项 # data = &#123;"data": "CET4_181_DANGCI,&#123;id&#125;,&#123;name&#125;".format(id=new_id, name=name),"v": code&#125; # CET6成绩查询选项 data = &#123;"data": "CET6_181_DANGCI,&#123;id&#125;,&#123;name&#125;".format(id=new_id, name=name),"v": code&#125; query_resp = requests.post(query_api, data=data, headers=query_api_headers) query_text = query_resp.text log_info(query_text.split("'")[3],new_id) if "验证码错误" in query_text: query_text = send_query_until_true(num) # elif "您查询的结果为空" in query_text: # images.save("images/save_picture/" + code + ".png") return query_text 使用代理&emsp;&emsp;在上面那段代码中，我们在请求过程中使用了代理，是为了防止频繁请求导致ip被封，代理功能可以自动切换代理，保证程序的正常运行。在测试过程中我们发现，该网站不会对ip进行封锁，所以代理可有可无。这里大致说一下代理功能是如何实现的。 &emsp;&emsp;代理功能使用的代理池是Github上的开源项目，它通过从代理平台抓取可用的代理ip存储到本地Redis中，需要使用代理时，即从本地Redis中取出。使用代理功能需要进行相应的配置。 123安装并开启Redis服务器安装依赖 pip3 install -r requirements.txt开启代理服务 python run.py &emsp;&emsp;在上述代码中，我们使用了捕捉异常的语句，因为在使用代理的过程中我们发现代理ip可能存在网络不稳定，传输有延时等问题。总的来说，使用代理的查询速度很慢，不想使用代理的话直接将proxy配置成本地的ip和端口即可。 多线程&emsp;&emsp;在开发过程中，想过用多线程，但是效果不太理想（对并行编程不熟悉），后来想想对于查找准考证号这种问题可以根据实际情况灵活，可能有些人会大致记得自己的考场位于哪个区间之内，所以在项目中，提供了输入查询区间的接口。如果想提高查询速度，可以开启多个终端，每个终端输入不同的查询区间，这样就类似于开启了多进程（一般查询的时候开启10个终端，每个终端的考场区间为10，10分钟内可以查询到结果）。 使用教程&emsp;&emsp;简单介绍一下该项目的文件结构，如图所示。 images：主要用来存放验证码图片，images中包含多个目录，row_picture存放原始验证码，change_picture存 * 放灰度化、二值化处理后的验证码，train_data存放分割后的验证码 proxypool：实现代理功能的相关代码 acquire_picture.py：包含验证码获取、处理相关操作的代码 model.pkl：存放识别模型 recongnition_code.py：项目的执行入口，包含向服务器发送请求、代理等相关代码 setting.py：项目相关的配置文件 train_data_preprocessing.py：整合验证码获取和处理相关操作 train_model.py：训练模型 &emsp;&emsp;该项目使用的大致流程如下（要求python版本不低于3.5，该项目在win10环境测试运行无误）。 123456安装相关依赖PIL、requests、numpyy、sklearn等修改recongnition_code.py文件中的myid（准考证号前10位）、name（自己的名字）修改recongnition_code.py文件中成绩查询选项如果需要使用代理，需要配置代理相关环境在项目文件夹中打开终端输入：python recongnition_code.py 开始区间 结束区间可同时开启多个终端，每个设置不同的区间，加快查找速度]]></content>
      <categories>
        <category>机器学习算法</category>
      </categories>
      <tags>
        <tag>机器学习</tag>
        <tag>验证码识别</tag>
        <tag>爬虫</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[HTTPS详解：SSL/TLS协议]]></title>
    <url>%2FTLS.html%2F</url>
    <content type="text"><![CDATA[摘要: 最近在看关于web安全相关的书籍，说到web安全HTTP和HTTPS之间的联系和区别是一个无法回避的问题。很长时间以来，我也被这个问题所困扰，对于其中涉及的细节问题更是难以触及，但是现在是该去好好考虑这个问题了。在网上查阅了大量资料之后，我发现很多文章在解释这个问题的时候含糊不清，让人难以理解。所以我写下了这篇博客，希望能给大家提供帮助，如有不足之处，欢迎指正！ 文章概览 区别与联系 HTTP和HTTPS 联系 区别 SSL和TLS 联系 区别 SSL/TLS协议 密码套件 记录协议 握手协议 前向安全 OPENSSL 区别与联系 HTTP和HTTPS联系&emsp;&emsp;HTTP（超文本传输协议）是一个客户端终端（用户）和服务器端（网站）请求和应答的标准（TCP）。通过使用网页浏览器、网络爬虫或者其它的工具，客户端发起一个HTTP请求到服务器上指定端口（默认端口为80）。我们称这个客户端为用户代理程序（user agent）。应答的服务器上存储着一些资源，比如HTML文件和图像。我们称这个应答服务器为源服务器（origin server）。在用户代理和源服务器中间可能存在多个“中间层”，比如代理服务器、网关或者隧道（tunnel）。可以从HTTP头、HTTP请求方法、HTTP状态码和统一资源定位符URL四个方面深入理解HTTP协议。&emsp;&emsp;超文本传输协议HTTP协议被用于在Web浏览器和网站服务器之间传递信息。HTTP协议以明文方式发送内容，不提供任何方式的数据加密，易遭受窃听、篡改、劫持等攻击，因此HTTP协议不适合传输一些敏感信息，比如信用卡号、密码等。为了解决HTTP协议的这一缺陷，需要使用另一种协议：安全套接字层超文本传输协议HTTPS。为了数据传输的安全，HTTPS在HTTP的基础上加入了SSL/TLS协议，SSL/TLS依靠证书来验证服务器的身份，并为浏览器和服务器之间的通信加密，一次HTTPS协议实现了数据传输过程中的保密性、完整性和身份认证性。区别&emsp;&emsp;HTTP和HTTPS的主要区别如下包括:HTTPS协议需要到CA申请证书,而HTTP协议则不用；HTTP是超文本传输协议，信息是明文传输，而HTTPS则是加密传输；HTTP和HTTPS使用完全不同的连接方式，所占用的端口也不一样，前者占用80端口，后者占用443端口；HTTPS传输过程比较复杂，对服务端占用的资源比较多，由于握手过程的复杂性和加密传输的特性导致HTTPS传输的效率比较低；HTTP的连接很简单，是无状态的；HTTPS协议是由SSL+HTTP协议构建的可进行加密传输、身份认证的网络协议，比HTTP协议安全。SSL和TLS 联系&emsp;&emsp;SSL(Secure Sockets Layer 安全套接层)为Netscape所研发，用以保障在Internet上数据传输之安全，利用数据加密(Encryption)技术，可确保数据在网络上之传输过程中不会被截取及窃听。SSL协议位于TCP/IP协议与各种应用层协议之间，为数据通讯提供安全支持。SSL协议可分为两层： SSL记录协议（SSL Record Protocol）：它建立在可靠的传输协议（如TCP）之上，为高层协议提供数据封装、压缩、加密等基本功能的支持。 SSL握手协议（SSL Handshake Protocol）：它建立在SSL记录协议之上，用于在实际的数据传输开始前，通讯双方进行身份认证、协商加密算法、交换加密密钥等。&emsp;&emsp;安全传输层协议（TLS）用于在两个通信应用程序之间提供保密性和数据完整性。该协议由两层组成： TLS 记录协议（TLS Record）和 TLS 握手协议（TLS Handshake）。较低的层为 TLS 记录协议，位于某个可靠的传输协议（例如 TCP）上面，与具体的应用无关，所以，一般把TLS协议归为传输层安全协议。TLS 的最大优势就在于：TLS 是独立于应用协议。高层协议可以透明地分布在 TLS 协议上面。然而，TLS 标准并没有规定应用程序如何在 TLS 上增加安全性；它把如何启动 TLS 握手协议以及如何解释交换的认证证书的决定权留给协议的设计者和实施者来判断。&emsp;&emsp;SSL是Netscape开发的专门用户保护Web通讯的，而TLS1.0是IETF(工程任务组)制定的一种新的协议，它建立在SSL 3.0协议规范之上，是SSL 3.0的后续版本。两者差别很小，可以理解为SSL 3.1，它是写入了RFC的。为了兼顾各种说法本文将SSL和TLS统称为SSL/TLS，但是请注意，本文所涉及的关于各种协议的解析是基于TLS1.2版本（这是目前使用最广泛的版本）。 区别&emsp;&emsp;SSL和TLS的主要区别如下： 版本号：TLS记录格式与SSL记录格式相同，但版本号的值不同。 报文鉴别码：SSLv3.0和TLS的MAC算法及MAC计算的范围不同。TLS使用了RFC-2104定义的HMAC算法。SSLv3.0使用了相似的算法，两者差别在于SSLv3.0中，填充字节与密钥之间采用的是连接运算，而HMAC算法采用的是异或运算。但是两者的安全程度是相同的。 伪随机函数：TLS使用了称为PRF的伪随机函数来将密钥扩展成数据块，是更安全的方式。 报警代码：TLS支持几乎所有的SSLv3.0报警代码，而且TLS还补充定义了很多报警代码，如解密失败、记录溢出、未知、拒绝访问等。 加密计算：TLS与SSLv3.0在计算主密值（master secret）时采用的方式不同。 填充：用户数据加密之前需要增加的填充字节。在SSL中，填充后的数据长度要达到密文块长度的最小整数倍。而在TLS中，填充后的数据长度可以是密文块长度的任意整数倍（但填充的最大长度为255字节），这种方式可以防止基于对报文长度进行分析的攻击。 SSL/TLS协议 密码套件（cipher suite）&emsp;&emsp;密码套件（Cipher suite）是传输层安全（TLS）/安全套接字层（SSL）网络协议中的一个概念。在TLS 1.2中，密码套件的名称是以协商安全设置时使用的身份验证、加密、消息认证码（MAC）和密钥交换算法组成。TLS 1.3中的密码套件格式已经修改。在目前的TLS 1.3草案文档中，密码套件仅用于协商加密和HMAC算法。在创建一个TLS连接后，一次也称TLS握手协议的握手发生。在这个握手，一条ClientHello和一条ServerHello消息被发出。首先，客户端按照偏好的顺序发送它支持的密码套件的列表。然后服务器回复它从客户端的列表中选择的密码套件。&emsp;&emsp;例如TLS_ECDHE_RSA_WITH_AES_128_GCM_SHA256,每个密码套件名称定义一个密钥交换算法、一个批量加密算法、一个消息认证码（MAC）算法，以及一个伪随机函数(PRF) 密钥交换算法，例如ECDHE_RSA，用于决定客户端与服务器之间在握手时如何身份验证 批量加密算法，例如AES_128_GCM，用于加密消息流。它还包括密钥大小及显式和隐式初始化向量（密码学随机数）的长度 消息认证码算法，例如SHA256，用于创建消息摘要，消息流每个数据块的加密散列 伪随机函数，例如TLS 1.2的伪随机函数使用MAC算法的散列函数来创建一个主密钥——连接双方共享的一个48字节的私钥。主密钥在创建会话密钥（例如创建MAC）时作为一个熵来源 理解密码套件的作用以及组成部分对我们理解握手协议的过程十分重要，因为使用不同的密码套件在握手协议的实现细节上有很大的不同，特别是密钥交换的过程。本文将主要讲解ECDHE_RSA密钥交换算法下的握手过程（也可以理解为三种不同密码套件的握手过程，对于握手过程而言不同套件的差异主要体现在密钥交换的过程，批量加密算法和消息验证码算法的不同主要体现在加密传输的过程，伪随机算法的不同体现在产生分组加密初始向量的过程）。 记录协议&emsp;&emsp;TLS记录协议位于TLS握手协议的下层，在可靠的传输协议(如TCP/IP)上层。TLS记录协议的一条记录包含长度字段、描述字段和内容字段。TLS记录协议处理数据的加密，即记录协议得到要发送的消息之后，将数据分成易于处理的数据分组，进行数据压缩处理(可选)，计算数据分组的消息认证码MAC，加密数据然后发送数据；接收到的消息首先被解密，然后校验MAC值，解压缩，重组，最后传递给协议的高层客户。记录协议有四种类型的客户：握手协议、警告协议、改变密码格式协议和应用数据协议。通常使用一个对称算法，算法的密钥由握手协议提供的值生成。TLS 记录协议提供的连接安全性具有两个基本特性: 私有――对称加密用以数据加密（DES 、RC4 等）。对称加密所产生的密钥对每个连接都是唯一的，且此密钥基于另一个协议（如握手协议）协商。记录协议也可以不加密使用 可靠――信息传输包括使用密钥的 MAC 进行信息完整性检查。安全哈希功能（ SHA、MD5 等）用于 MAC 计算。记录协议在没有 MAC 的情况下也能操作，但一般只能用于这种模式，即有另一个协议正在使用记录协议传输协商安全参数 握手协议&emsp;&emsp;TLS握手协议处理对等用户的认证，在这一层使用了公共密钥和证书，并协商算法和加密实际数据传输的密钥，该过程在TLS记录协议之上进行。TLS握手协议是TLS协议中最复杂的部分，它定义了10种消息，客户端和服务器利用这10种消息相互认证，协商哈希函数和加密算法并相互提供产生加密密钥的机密数据。TLS记录协议会在加密算法中用到这些加密密钥，从而提供数据保密性和一致性保护。&emsp;&emsp;我们先来分析基于ECDHE_RSA密钥交换算法的握手过程，在这之前先来解释一下ECDHE是什么。ECDHE_RSA = EC（椭圆曲线加密算法)+ DH(Diffie-Hellman密钥交换算法)+ E(临时的temporary)+ RSA(用于签名，防止中间人攻击)，所以ECDHE的意思是结合椭圆曲线的生成临时会话密钥的密钥交换算法。对于这个算法的具体计算过程，这里不详细讨论。&emsp;&emsp;下图是EDCHE_RSA密钥交换算法的大致流程，接下来我会结合wireshark抓取的数据包来分析握手的过程： 在客户端和服务器开始握手之前先进行TCP三次握手，这部分的内容本文不会讨论。三次握手之后，开始握手协议，先在这展示一下抓到的所有握手包。 Client hello：由于客户端对一些加解密算法的支持程度不一样，但是SSL/TLS协议传输过程中必须要求客户端与服务器端使用相同的加解密算法。所以在client hello阶段，客户端要首先告知服务端，自己支持哪些密码套件，客户端将支持的密码套件列表发送给服务端；同时客户端还会产生一个随机数，这个随机数双方都要保存（生成主密钥）；session id字段是用于维持会话，如果客户端与服务端关闭会话之后，客户端又要重新发起会话，session id可用于双方协商是否要进行重新握手过程；extension字段用于添加一些拓展功能；compress表示支持的压缩方法。 server hello：server hello是根据客户端发送过来的密码套件和压缩方法选择双方都支持的类型，同时服务器端也会生成一个随机数，双方都要保存。 certificate：该过程中服务器用私钥签名证书，发送给客户端以认证身份 server key exchange：对于ECDHE_RSA密钥交换算法来说这一过程是必须的，在此过程服务端将生成一对公钥和私钥，私钥保留（用于服务器端生成预主密钥），并将公钥发送给客户端（用于客户端生成预主密钥），同时将前一阶段所有的会话内容利用私钥进行前面发给客户端，用于验证服务端身份，防止中间人攻击。而对于RSA_RSA密钥交换算法，没有这一过程，同样有这个过程的还有DHE密钥交换算法，有没有这一过程都是却决于密钥交换算法自身。在这个数据包中，还给出了服务端生成公私钥所用的算法sec256r1 server hello done：表示server hello结束，这是个空消息 client key exchange：客户端也生成一对公钥和私钥，私钥保留（用于客户端生成预主密钥），公钥发给服务端（用于服务端生成预主密钥） change cipher spec：客户端根据交互过程中获得的信息，以及应用服务端规定的密码套件，已经生成了相应的密钥。通过这条消息，客户端告诉服务器端：从现在起，我将使用双方约定的密码规范进行通信 encrypted handshake message：客户端利用生成的密钥加密一段finishde数据传送给服务端，此数据是为了在正式传输应用之前对刚刚握手建立起来的加解密通道进行验证 new session ticket：服务端告知客户端将生成新的session ticket用于保持会话（session ticket与前面提到的session id作用类似，但两者实现方式不同） change cipher spec：同样服务端也要发送这段信息，作用与客户端一致 encrypted handshake message：作用与客户端一致至此，握手协议结束，双方开始建立加密通道。 &emsp;&emsp;值得注意的是在这个过程中客户端和服务器端都各自产生一对公钥和私钥还有一个随机数，这些都是作为生成预主密钥的元素。预主密钥分别在客户端和服务器端生成，算法的特性能够保证二者生成的预主密钥相同。那么由预主密钥如何生成会话密钥呢，这就要用到前面提到的伪随机函数，通过预主密钥我们将生成客户端验证密钥、服务器端验证密钥、客户端加密密钥、服务器端加密密钥以及客户端分组加密的初始向量和服务器端的分组加密初始向量，具体生成过程可以参考这篇博客。对基于DH算法和RSA算法的握手过程可以参见下图，也可以参考这篇博客。 前向安全&emsp;&emsp;在这里有必要提一下关于前向安全的定义：前向安全或前向保密，有时也被称为完美前向安全（Perfect Forward Secrecy，缩写：PFS），是密码学中通讯协议的安全属性，指的是长期使用的主密钥泄漏不会导致过去的会话密钥泄漏。前向安全能够保护过去进行的通讯不受密码或密钥在未来暴露的威胁。如果系统具有前向安全性，就可以保证在主密钥泄露时历史通讯的安全，即使系统遭到主动攻击也是如此。在传输层安全协议（TLS）中，提供了基于迪菲-赫尔曼密钥交换（DHE）的前向安全通讯，分别为（DHE-RSA）和DHE-DSA），还有基于椭圆曲线迪菲-赫尔曼密钥交换（ECDHE）的前向安全通讯，包括（ECDHE-RSA与ECDHE-ECDSA）。理论上，从SSLv3开始，就已经可以使用支持前向安全的密码算法进行通讯。之前我们提到ECDHE算法在sever key exchage阶段会生成一个临时的公私钥对，公钥发送给用户，私钥用于对数据进行RSA签名来验证服务器的身份，如果服务器的私钥泄露，这些会话不会受到影响，无法解密。对于有些算法而言，它在握手过程中不会有这个生成公私钥对的过程，它将使用服务器的私钥进行签名。如果服务器的私钥泄露，这些会话都将被暴露，这就是所谓的前向安全。 openssl]]></content>
      <categories>
        <category>web安全</category>
      </categories>
      <tags>
        <tag>HTTPS和HTTP</tag>
        <tag>SSL安全套接层</tag>
        <tag>TLS安全传输层协议</tag>
        <tag>openssl</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[K近邻算法详解]]></title>
    <url>%2FKNN.html%2F</url>
    <content type="text"><![CDATA[摘要: K近邻（简称KNN）是一种基于统计的数据挖掘算法，它是在一组历史数据记录中寻找一个或者若干个与当前记录最相似的历史记录的特征值来预测当前记录的未知的特征值，因此具有直观、无需先验统计知识等特点，同时K近邻算法适用于分类和回归两种不同的应用场景，本文主要介绍K近邻算法在回归任务场景下的应用。 文章概览 K近邻概述 K近邻三要素 距离度量 K值选择 分类决策规则 K近邻的实现 线性查找 空间分割 kd树的生成 kd树的搜索 scikit_learn中的K近邻算法 总结 K近邻概述 &emsp;&emsp;K近邻算法简单直观，下面举一个简单的例子帮助大家理解。在一个城市当，居住着许多不同民族的居民，相同民族的人们大多聚集在一起，形成一个小型的部落。现在你想知道其中一个部落是属于哪个民族的，并且你已经掌握很多关于部落和民族的信息，你会怎么做？其实我们可以通过观察这个部落的人们的生活习惯、节日风俗、衣着服饰等特点，在与我们掌握其他部落的特点进行对比，找出与该部落在这些方面最接近的几个部落（已知这几个部落分别属于哪个民族），如果这几个部落的多数属于哪个民族，那么在很大程度上我们可以猜测该部落可能也属于这个民族，从而得到我们想要的答案。&emsp;&emsp;对于K近邻稍微正式一点的描述是：给定一个训练数据集，对新的输入实例，在训练数据集中找到与该实例最近邻的K个实例，这K个实例的多数属于某个类，就把该输入实例分为这个类。从这段描述中我们可以看出，K近邻算法的学习过程只是简单的存储已知的训练数据，当遇到新的查询实例时，再从存储器中取出一系列相似的实例，用来分类新的查询实例。我们把K近邻算法的这种分类特点称为消极学习方法,具有同样特点的学习算法还有局部加权回归，它的优点在于不是在整个实例空间上一次性的估计目标函数，而是针对每个待分类的新实例做出局部的和相异的估计。而与之对应的分类算法，我们称之为积极学习方法，例如：支持向量机、神经网络等等，它的特点是在新的查询实例到来之前，通过训练实例总结归纳出相似判断的目标函数。&emsp;&emsp;K近邻同样可以应用于回归任务。K近邻做分类预测时，一般是选择多数表决法，即训练集里和预测的样本特征最近的K个样本，预测为里面有最多类别数的类别。而K近邻做回归时，一般是选择平均法，即最近的K个样本的样本输出的平均值作为回归预测值。由于两者区别不大，虽然本文主要是讲解K近邻的分类方法，但思想对K近邻的回归方法也适用。 K近邻三要素 距离度量&emsp;&emsp;回到刚才那个例子，我们假设从生活习惯、节日风俗、衣着服饰、宗教信仰等多个方面来考察部落之间的相似程度。所谓的相似程度用另外一种说法来表达即差异，差异越小，相似程度越大。在机器学习中，我们使用距离来度量差异。一般情况下我们采用欧式距离来度量差异（其他的距离度量方式如曼哈顿距离等同样适用）。欧式距离： 设特征空间\({\chi}\)是\(n\)维实数向量空间\(R^n,x_i,x_j\)属于\({\chi}\)，\(x_i=(x^1_i,x^2_i,…,x^n_i),x_j=(x^1_j,x^2_j,…,x^n_j),x_i,x_j\)的欧式距离定义为$$\sqrt{\sum_{l=1}^{n}|x_i^l-x_j^l|^2}$$&emsp;&emsp;刚才说到我们将通过生活习惯、节日风俗、衣着服饰、宗教信仰等多个方面来考察部落之间的相似程度，但是现在我们需要考虑这样一种情况，我们观察发现这些部落虽然在有些方面存在很大的差异，但是这些差异却不能成为区分不同民族的依据，比如说，A和B两个部落都属于C民族，但是A部落信仰D教，B部落信仰E教。也就是说，应用k-近邻的一个实践问题是，实例间的距离是根据实例的所有属性计算的，但是这些属性当中存在着对分类无关的属性，这些无关的属性可能在实例空间中相距很远，这样一来近邻间的距离会被大量的不相关属性所支配。这种由于存在很多不相关属性所导致的难题，有时被称为维度灾难。解决该问题的一个方法是，当计算两个实例间的距离时对每个属性加权，从不断的测试中获得启发，给对分类影响大的属性赋予更高的权值。 K值选择&emsp;&emsp;K值的选择会对K近邻法的结果产生巨大的影响。如果选择较小的K值，学习的近似误差会减小，只有与输入实例较近的训练实例才会对预测结果起作用，这样做存在的问题是预测结果对近邻点过于敏感，如果近邻点恰巧是噪声，预测结果就会出错。如果选择较大的K值，其优点是可以减少学习得估计误差，缺点是与输入实例较远的训练实例也会对预测起作用。K值选择的原则往往是经过大量独立测试数据、多个模型来验证最佳选择。 分类决策规则&emsp;&emsp;K近邻中的分类决策往往是多数表决，即由输入实例的K个近邻的训练实例中的多数类决定输入实例的类。这样的决策规则存在一个问题，假设我们现在已知A、B两个部落属于同一个民族，C、D、E三个部落属于同一个民族。为了测试我们的模型，我们将A部落作为实例，输入到模型中进行测试，K值设为4。经过计算我们发现，得到的K个近邻实例分别为B、C、D、E，并且A、B部落之间的特征距离很小，而A与C、D、E三个部落之间的特征距离很大。但是由于分类决策规则是依据多数进行表决的，所以我们最终会将A判断为与C、D、E部落相同的民族。由此可以看出，多数表决的决策规则是不合理的。解决这一问题的方法是对距离进行加权，B部落与A部落的差异比较小，所以在K个实例当中B部落应该对最终的决策产生更大的影响，而距离越远影响力越小。 K近邻的实现 线性查找&emsp;&emsp;K近邻的核心思想是寻找与输入实例距离最近的K个实例，那么一个最朴素的想法是计算输入实例和所有训练实例之间的距离，然后从中挑选出距离最近的K个实例，这就是线性查找的思想，具体实现如下（大家可以在我的github中找到本文所有的源代码）：123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081#!/usr/bin/env python#_*_ coding:utf-8 _*_#通过直接对k-近邻算法的描述来构建鸢尾花数据集的模型，并利用该模型对鸢尾花类型进行预测import numpy as npimport randomimport operatorfrom init_data import load_datadef data_split(): """ data_split函数的主要作用是将原始数据分为训练数据和测试数据，其中训练数据和测试数据的比例为2：1 """ x_data,y_data = load_data() x_training = [] x_test= [] y_training = [] y_test = [] for i in range(len(x_data)): if random.random() &gt; 0.67: x_training.append(x_data[i]) y_training.append(y_data[i]) else: x_test.append(x_data[i]) y_test.append(y_data[i]) x_training = np.array(x_training) y_training = np.array(y_training) x_test = np.array(x_test) y_test = np.array(y_test) return (x_training,x_test,y_training,y_test)def euclidean_distance(x_training,row): """ euclidean_distance函数的主要功能是计算每一条测试数据和训练数据集的欧式距离 :param x_training: 训练数据集 :param row: 一条测试数据 :return: 表示欧式距离的矩阵 """ dis = np.sum((row - x_training)**2,axis=1) dis = np.sqrt(dis) return disdef predict(dis,y_training,k): """ predict函数的主要作用是通过计算所得的欧式距离的集合，从中选取k个距离最小的数据点，统计这k个数据点中各个类别所出现的次数，出现次数最多的类别即为预测值 :param dis: 表示欧式距离的矩阵 :param y_training: 训练数据的类别 :param k: 选取k个距离最近的数据 :return: 预测值 """ dis_sort = np.argsort(dis)#对欧式距离集合进行排序，返回的dis_sort表示的是排序（从小到大）后的数据在原数组中的索引 statistics = &#123;&#125;#定义字典，用于统计k个数据点中各个类别的鸢尾花出现的次数 for i in range(k): rank = dis_sort[i] if y_training[rank] in statistics: statistics[y_training[rank]] = statistics[y_training[rank]] + 1 else: statistics[y_training[rank]] = 1 sort_statis = sorted(statistics.items(), key=operator.itemgetter(1), reverse=True)#对statistics字典按照value进行排序（从大到小） y_predict = sort_statis[0][0] return y_predictif __name__ == "__main__": x_training, x_test, y_training, y_test = data_split() num = 0 i = 0 for row in x_test: dis = euclidean_distance(x_training,row) y_predict = predict(dis,y_training,5) if y_predict == y_test[i]: num = num + 1 i = i + 1 print('The accuracy is &#123;0:1f&#125;%'.format(num/i)) 空间分割&emsp;&emsp;基于线性查找的思想，存在一个严重的问题是，如果训练集合很大，计算非常耗时，这种方法在实际中难以应用。为了提高K近邻的搜索效率，我们考虑将搜索空间进行分割，通过这种方法来提高搜索效率，减少计算距离的次数。具体的方法有很多，这里主要介绍kd树。&emsp;&emsp;关于kd树的算法和结构定义大家可以参考《统计学习方法》和这篇博文,本文主要关注kd树python实现的一些细节问题。 kd树的生成 每次对子空间的划分时，怎样确定在哪个维度上进行划分：在《统计学习方法》中采用的是轮流的方式，即如果这次选择了在第i维上进行数据划分，那下一次就在第j(j≠i)维上进行划分，例如：j = (i mod k) + 1。但是这样忽略了不同属性数据之间的分散程度，有的属性值比较分散，有的属性值比较集中，如果我们以数据分布比较分散的属性作为数据分割的依据，可以更大程度的分割数据，这样更有利于提高搜索的效率。方差可以衡量数据集合的分散程度，所以一般情况下我们采用最大方差分割法对数据集合进行分割。 以下是kd树生成算法的python描述，代码标注了详细的解释，可以在我的github中找到完整的代码。123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960def splitdata(data): """ splitdata函数的作用是对输入数据集合进行分割，具体规则：求出方差值最大的那一维特征，然后将整个数据集合根据这一维特征进行排序，中位数为分割点 :param data: 数据集合 :return: 分割数据的属性，数据分割点，分割后两个部分的数据集合 """ n,m = np.shape(data) #获取numpy矩阵维度 right_data = [] left_data = [] num = 0 row_mean = np.sum(data,axis=0) / n #求矩阵每一列的平均值 row_variance = np.sum((data - row_mean)**2,axis=0) #求矩阵每一列的方差 max_row_variance = np.where(row_variance == np.max(row_variance))[0][0] #方差值最大的那一列的索引 sort_data = np.argsort(data,axis=0) #data矩阵按照列排序，返回排序后的对应的索引 split_row = sort_data[:,max_row_variance] #方差值最大的那一列排序后的索引值 split_index = int(n/2) #中位数 for line in split_row: #将data中的数据分成两个部分，索引排在中位数之前的放进left_data,反之放进right_data if num &gt; split_index: if right_data == []: right_data = data[line,:] right_data = np.array([right_data]) else: right_data = np.concatenate((right_data,[data[line,:]]),axis=0) elif num &lt; split_index: if left_data == []: left_data = data[line,:] left_data = np.array([left_data]) else: left_data = np.concatenate((left_data,[data[line,:]]),axis=0) num = num + 1 #用于计数 split_data = data[split_row[split_index]] #取对应原始数据中的分割点值 print("分割结点为：",split_data,"--------- 分割维度为：",max_row_variance) return(max_row_variance,split_data,right_data,left_data) #返回值分别为分割数据的属性，数据分割点，分割后两个部分的数据class KNode(object): """ 定义节点类 """ def __init__(self,row = None,point = None,right = None,left = None,parent = None): self.row = row #分割数据集合的特征 self.point = point #数据分割点 self.right = right #右子树 self.left = left #左子树def create_tree(dataset,knode): """ create_tree函数的主要作用是通过递归的方式来建立kd树 :param dataset: 数据集合 :param knode: 根结点 :return: 返回kd树 """ length = len(dataset) if length == 0: return row,point,right_data,left_data = splitdata(dataset) knode = KNode(row,point) knode.right = create_tree(right_data,knode.right) knode.left = create_tree(left_data,knode.left) return knode kd树的搜索 kd树的搜索分为两个过程，首先找出包含输入实例的叶子结点，然后在从叶子结点回溯寻找K个近邻实例。在寻找叶子结点的过程中，我们会建立三个列表，一个列表用于存储搜索路径，一个列表用于存储K个近邻点，另外一个列表用于存储K个近邻点所对应的与输入实例的距离，在搜索叶子结点的过程中就计算K近邻点有利于简化回溯过程的搜索。在该过程中，每到达一个结点，我们先将该结点加入搜索路径，然后计算该结点与输入实例之间的距离，如果K近邻点列表中不足K个结点，直接将该结点加入K近邻点列表，同时将计算的距离加入对应的距离列表；如果K近邻列表中已经有K个结点，则选择距离列表中距离最大值与该结点计算的距离进行比较。如果该结点的距离小，则删除最大距离对应的结点，加入该结点，反之无需改变。 kd树的回溯过程稍微麻烦一点,大家可以参照我给出的代码注释进行理解。在看代码的时候一定要学会去调试，可以帮助我们理解。123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263def find_KNN(point,kdtree,k): """ k近邻查找 :param point: 测试数据点 :param kdtree: 建立好的kd树 :param k: k值 :return: k个近邻点 """ current = kdtree #当前节点 nodelist = [] #搜索路径 nodek = [] #存储k个近邻点与测试数据点之间的距离 nodek_point = [] #存储k个近邻点对应的值 min_dis = euclidean_distance(point,kdtree.point) print("---------------------------------------------------------------------------------------") while current: #找到测试点所对应的叶子结点，同时将搜索路径中的结点进行k近邻判断 nodelist.append(current) #将当前结点加入搜索路径 dis = euclidean_distance(point,current.point) if len(nodek) &lt; k: #nodek中不足k个结点时，直接将当前结点加入nodek_point nodek.append(dis) nodek_point.append(current.point) print(current.point,"加入k近邻列表") else: #nodek中有k个结点时，删除距离最大的哪个结点，再将该结点加入nodek_point max_dis = max(nodek) if dis &lt; max_dis: index = nodek.index(max_dis) print(current.point, "加入k近邻列表;",nodek_point[index],"离开k近邻列表") del(nodek[index]) del(nodek_point[index]) nodek.append(dis) nodek_point.append(current.point) ind = current.row #该结点进行分割时的特征 if point[ind] &gt;= current.point[ind]: current = current.right else: current = current.left while nodelist: #回溯寻找k近邻 back_point = nodelist.pop() ind = back_point.row max_dis = max(nodek) if len(nodek) &lt; k or abs(point[ind] - back_point.point[ind])&lt;max_dis: #如果nodek_point中存储的节点数少于k个，或者测试数据点和当前结点在分割特征维度上的差值的绝对值小于k近邻中的最大距离 if point[ind] &lt;= back_point.point[ind]: #注意理解这一段判断的代码，因为在之前寻找叶子结点的过程中，我们决定搜索路径的判断方法是大于即搜索右子树，小于即搜索左子树，这里的判断恰恰相反，是为了遍历之前没有搜索的结点 current = back_point.right else: current = back_point.left if current: nodelist.append(current) dis = euclidean_distance(point,current.point) if max_dis &gt; dis and len(nodek) == k: index = nodek.index((max_dis)) print(current.point, "加入k近邻列表;", nodek_point[index], "离开k近邻列表") del(nodek[index]) del (nodek_point[index]) nodek.append(dis) nodek_point.append(current.point) elif len(nodek) &lt; k: nodek.append(dis) nodek_point.append(current.point) print(current.point, "加入k近邻列表") return nodek_point scikit_learn中的K近邻算法scikit_learn中的K近邻算法的具体解释大家可以参照scikit_learn官网的文档，这里给出一段利用scikit_learn解决鸢尾花数据集的代码。这篇文章中给出的代码都是基于UCI鸢尾花数据集实现的，大家可以比较一下这三种实现方式的预测准确率。1234567891011121314151617#!/usr/bin/env python#_*_ coding:utf-8 _*_#通过sklearn库中所提供的关于k-近邻算法相关的包来实现对鸢尾花数据集的建模与预测from init_data import load_dataimport numpy as npfrom sklearn.neighbors import KNeighborsClassifierfrom sklearn.cross_validation import train_test_splitx_data,y_data = load_data() #获取数据x_training,x_test,y_training,y_test = train_test_split(x_data,y_data,random_state=10) #将数据分为训练集和测试集estimotor = KNeighborsClassifier() #构造k-近邻分类器estimotor.fit(x_training,y_training) #训练模型y_predicted = estimotor.predict(x_test) #用训练的模型进行预测accuracy = np.mean(y_test == y_predicted)*100 #计算预测结果的准确率print('The accuracy is &#123;0:1f&#125;%'.format(accuracy)) 总结K近邻算法的优点在于 算法简单直观，易于实现 K近邻在进行类别决策时只于少量的相邻样本有关，可以避免样本数量不平衡问题 K近邻最直接的利用了样本之间的关系，减少了类别特征选择不当对分类结果造成的不利影响，可以最大程度减少分类过程中的误差项 同时K近邻算法存在的问题也很突出 当样本数量大、特征多的时候计算量非常大 样本不平衡的时候，对稀有类别的预测准确率降低 预测速度慢 &emsp;&emsp;花了两天的时间找资料、写代码，又花了一天的时间写文章，终于结束了！有一点不太满意的地方是对kd树的回溯没有进行详尽的描述，原因是真的找不出一种好的描述方法，文字描述看起来累，用图表的话工作量太大，大家可以看看我推荐的博文结合代码，理解起来也不会太难。好了，到这里结束了，好好加油，坚持！]]></content>
      <categories>
        <category>机器学习算法</category>
      </categories>
      <tags>
        <tag>KNN</tag>
        <tag>KD树</tag>
        <tag>机器学习</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[搭建博客：HEXO+GITHUB+CODING]]></title>
    <url>%2Fhex.html%2F</url>
    <content type="text"><![CDATA[摘要: 进入计算机行业已经好几年了，这么多年的摸爬滚打，我终于意识到了一个血的教训:好记性不如烂键盘!当我们遇到问题并解决问题之后，我们应该及时的把我们处理问题的过程记录下来，一来可以防止我们在此遇到同样的问题时又要重复造轮子，二来可以为遇到同样问题的小伙伴提供经验，所以对于我们来说有一个属于自己的博客尤为重要。本文记录的是搭建hexo个人博客平台过程中遇到的一些问题和心得，希望能对小伙伴们有所启发。 文章概览 HEXO简介 静态博客与动态博客 基本流程 运行机制 HEXO NEXT主题美化 HEXO部署到GITHUB 提交搜索引擎 提交谷歌搜索引擎 提交百度搜索引擎 总结 HEXO简介Hexo是一个基于node.js开发的快速、简洁且高效的静态博客框架。Hexo 使用 Markdown（或其他渲染引擎）解析文章，在几秒内，即可利用靓丽的主题生成静态网页。hexo具有以下这些特性： hexo基于node.js，非常小巧，安装部署简单 hexo开源，主题丰富，插件丰富，自定义能力强 hexo支持markdown语法，易于博客写作 hexo是纯静态博客，不需要数据库支持 静态博客系统与动态博客系统一个网站最基础的部分就是网页，如果想从HTML页面写起，显然成本太高，好在大牛们已经做好了博客生成器来解决网页编写的问题。一般来说，博客生成器分为动态和静态两种。其中，动态博客生成器典型代表有：WordPress、FarBox、Ghost等，静态的博客生成器典型代表有：Hexo、Jekyll、Octopress、Hugo等。关于动态和静态的区别主要有以下几点： 资源占用上，静态博客相对于动态博客占用服务器资源少，可以托管在github pages上，而动态博客往往需要一台相对独立的服务器 数据管理和更新操作上，由于动态博客有独立的数据库和后台管理系统，对资源的管理和发布相对比较容易；而静态博客往往需要一些第三方平台的支持，如评论系统以及图床，数据管理更新比较繁琐。 安全性上，静态博客比动态博客安全性更好 基本流程&emsp;&emsp;这里我大致叙述一下搭建hexo博客系统的大致流程。我们首先要搭建hexo博客系统的开发环境，这里我主要讲解windows环境下的安装配置，其他系统的安装配置可以参考官方文档。1234567安装git安装node.js执行命令 npm install -g hexo-cli（安装hexo命令行工具）执行命令 hexo init [文件夹名]进入刚才初始化的文件夹，执行命令 npm install 即创建了一个原始的hexo博客系统执行命令 hexo g 生成站点文件执行命令 hexo d 把博客部署到站点 运行机制&emsp;&emsp;首先我们来分析一下hexo文件夹的结构 _config.yml:站点的配置文件 db.json:缓存文件 node_modules:安装的插件以及hexo所需要的一些node.js模块 package.json：应用程序信息，配置hexo运行需要的js包 public：生成的站点文件 scaffolds：模板文件夹，新建文章时，会默认包含对应模板内容 source：资源文件夹是存放用户资源的地方。所有的源文件都会被保存在_post文件夹中 themes：hexo站点所使用的主题 &emsp;&emsp;为了搞清楚hexo的运行机制，我们有必要了解一下hexo的模板引擎（hexo使用的模板引擎是ejs编写的），模板引擎的作用就是将界面与数据分离最简单的原理是将模板内容中指定的地方替换成数据，实现业务代码与逻辑代码分离。我们可以注意到，在hexo中，source文件夹和themes文件夹是同级的，我们可以将source文件夹理解为数据库，而主题文件夹相当于界面，当执行hexo g命令时，就相当于将数据嵌入到界面中，生成静态文件public。&emsp;&emsp;具体来说在hexo中，从markdown文件到生成html的过程中大致经历了两次渲染的过程： 通过解析markdown文件，并结合站点配置文件和source目录下的相关文件，生成相应的数据对象 将生成的数据对象嵌入到themes主题中的渲染引擎生成站点文件 HEXO NEXT主题美化&emsp;&emsp;基于hexo博客系统的主题有很多，你可以在这里找到你喜欢的主题。我的博客采用的是next主题，我个人觉得next主题看上去简洁大方，用起来很舒服。这里我不会详细的介绍next主题的配置过程，我会分享一些我在配置过程中遇到的一些问题。 next主题的官方网站详细阐述了主题的基本配置过程，我也是参照它一步步进行配置的 在配置主题之前推荐大家安装一款node.js的开发工具，有利于提高效率。我安装的是webstorm，它也支持markdown文件的编写，强烈推荐（这个公司提供的开发工具都很强大，pycharm也是这个公司的产品之一）。 在配置主题的过程中要注意区分两个配置文件，一个是主题的配置文件_config.yml，一个是站点的配置文件_config.yml。因为有些配置操作实在主题的配置文件中进行的，有的实在站点的配置文件进行的，一定不能弄混了。 推荐一篇next主题美化的博文 对主题进行配置时，我的建议是每修改一项之后都在本地运行一下（先运行hexo g命令，在运行hexo s命令，在浏览器中查看），看看有没有出错，这样我们可以及时找到出错的地方。 hexo的配置文件是yaml格式的，它通过缩进来表示层级关系，修改配置文件时要主要缩进问题 HEXO部署到GITHUB&emsp;&emsp;在对主题修改完成之后，下一步的工作就是将hexo部署到github pages。在部署之前，需要做好一些准备工作，具体的操作过程可以参考这篇博客。 注册github账号，添加ssh key 在github中新建仓库，仓库名为：username.github.io 修改站点配置文件的deploy选项 执行命令hexo deploy &emsp;&emsp;在这些操作过程中我们需要注意一些问题： 在deploy的过程中可能会出现速度过慢的问题，这是由于GFW对github的限制造成的，可以通过代理或者修改hosts文件来提高访问速度 有时候会因为一些莫名其妙的问题导致deploy失败，无法解决这一问题时，我们可以通过复制生成的public文件，通过git提交到远程仓库，为了使整个过程更加自动化，我们可以在根目录下写一个脚本文件deploy.sh 123456hexo generate cp -R public/* chaoge123456.github.io cd chaoge123456.github.io git add . git commit -m “update” git push origin master 每次提交更新时只需要在根目录下执行命令：./deploy.sh即可 提交搜索引擎&emsp;&emsp;部署完成之后，现在我们可以通过浏览器访问到我们的博客，但是还有一件非常重要的事我们需要去完成。虽然我们可以通过浏览器访问到我们的博客，但是我们无法通过搜索引擎搜索到我们的博客，所以我们需要将我们的博客地址提交给搜索引擎。这时我们需要注意，github屏蔽了百度搜索引擎的爬虫，这也就意味着通过百度是无法搜索到我们在github上的博客（googl不存在这样的问题）。所以为了在国内也能访问到我们的博客，我们需要将我们的博客托管到国内的类似于github的平台——coding（coding的博客地址和github的博客地址不一样，所以接下来我们需要做的是将github的博客地址提交给google，将coding的地址提交给百度）。部署到coding的流程跟github类似，为了将站点同时更新到coding和github，我们需要在站点配置文件下的deploy选项的repo同时添加github和coding的远程仓库123456deploy: type: git repo: github: git@github.com:chaoge123456/chaoge123456.github.io.git coding: git@git.coding.net:chao3236gmailco/chao3236gmailco.git branch: master 提交谷歌搜索引擎Google搜索引擎提交入口 提交百度搜索引擎百度搜索引擎入口&emsp;&emsp;将站点地址提交给搜索引擎的步骤也比较简单，具体操作可以参考这篇博文,这个过程中需要注意的问题是： 将验证文件提交给站点时，有人会认为，直接将验证文件放入public文件夹中然后执行hexo g和hexo d就可以将验证文件提交的远程仓库。这样做确实可以将验证文件提交的远程仓库，但是需要注意的是此时的验证文件经过了hexo渲染，和原来的验证文件已经不一致，这样的验证文件时无效的 正确的做法是通过git clone获得远程仓库，在将验证文件加入刚刚获得的远程文件，然后通过向远程仓库提交该文件 验证完毕后，要向搜索引擎提交站点地图，方便爬虫爬取站点 总结&emsp;&emsp;HEXO+GITHUB+CODING博客搭建大概就是这些流程，希望大家看了我的博客会有所收获。这是我博客上的第一篇博文，确实不容易，希望以后能好好坚持下去吧。好了，夜已深了，晚安世界！]]></content>
      <categories>
        <category>HEXO相关</category>
      </categories>
      <tags>
        <tag>HEXO部署和美化</tag>
        <tag>github pages</tag>
        <tag>建站教程</tag>
      </tags>
  </entry>
</search>
